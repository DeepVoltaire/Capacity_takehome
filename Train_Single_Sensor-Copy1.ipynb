{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6031383c-6318-4d2e-86f9-6ace15e99fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import tifffile\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 250)\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import shape\n",
    "import glob\n",
    "from shapely.geometry import box\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "import json\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "red_to_green = LinearSegmentedColormap.from_list(\"RedGreen\", [\"red\", \"green\"])\n",
    "\n",
    "from src.train import train\n",
    "from src.hyperparams import HyperParams, open_from_yaml\n",
    "from src.data import *\n",
    "from src.validate import return_metrics_all_folds, visualize_single_model, cross_validation, visualize_s2_concat_bands_path\n",
    "from src.visualize import visualize_s2_concat_bands_path, visualize_s1_path, scale_img, visualize_ls_concat_bands_path, visualize_cb_mux_path, visualize_cb_wpm_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7410b0-57cb-427b-b184-76ea8bf0c176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_cloud_nodata = 75\n",
    "# s2_valid_data_events = []\n",
    "# for event_id in events_sorted_by_area[:]:\n",
    "#     # rand_int = np.random.randint(len(events_sorted_by_area))\n",
    "#     # rand_int = k\n",
    "#     # event_id = events_sorted_by_area[rand_int]\n",
    "#     # s2_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel2/*L2A*.tif\") if not \"ndvi\" in k and not \"ndwi\" in k])\n",
    "#     s1_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel1/*.tif\") if not \"_nrpb\" in k and not \"_rfdi\" in k and not \"_rvi\" in k and not \"_vv_vh_ratio\" in k])\n",
    "#     if len(s1_paths) == 0:\n",
    "#         continue\n",
    "#     # s2_16d_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel2/*16D*.tif\") if not \"ndvi\" in k and not \"ndwi\" in k])\n",
    "#     # ls_paths = sorted([k for k in glob.glob(f\"data/{event_id}/landsat/*_L2SP_*.tif\") if not \"_nbr\" in k and not \"_ndwi\" in k and not \"_ndvi\" in k])\n",
    "#     # cb_wpm_paths = sorted([k for k in glob.glob(f\"data/{event_id}/cbers4a/*_WPM_*.tif\") if not \"_ndvi\" in k])\n",
    "#     # cb_mux_paths = sorted([k for k in glob.glob(f\"data/{event_id}/cbers4a/*_MUX_*.tif\") if not \"_ndvi\" in k])\n",
    "#     # print(f\"{event_id}: {len(s2_paths):2} S2 L2A, {len(s2_16d_paths):2} S2 16D,{len(s1_paths):2} S1, {len(ls_paths):2} LS L2SP, {len(cb_wpm_paths):2} CB WPM, {len(cb_mux_paths):2} CB MUX\")\n",
    "#     # paths = s2_paths + s1_paths + ls_paths + cb_wpm_paths + cb_mux_paths + s2_16d_paths\n",
    "#     # if len(paths) == 0:\n",
    "#     #     continue\n",
    "#     # continue\n",
    "#     mask_path = glob.glob(f\"data/{event_id}/*_mask.tif\")[0]\n",
    "#     mask_date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "#     mask_date_str = f\"{mask_date[:4]}-{mask_date[4:6]}-{mask_date[6:8]}\"\n",
    "#     mask_date = datetime.strptime(mask_date, \"%Y%m%d\")\n",
    "#     # print(mask_date)\n",
    "    \n",
    "#     date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "#     # Sort by the date right before the .tif\n",
    "#     paths = sorted(s1_paths, key=lambda p: datetime.strptime(p.split('_')[-1].split('.')[0], \"%Y%m%d\"))\n",
    "#     # break\n",
    "#     paths_after, paths_before = [], []\n",
    "#     for path in paths:\n",
    "#         date = datetime.strptime(path.split('_')[-1].split('.')[0], \"%Y%m%d\")\n",
    "#         if date > mask_date:\n",
    "#             paths_after.append(path)\n",
    "#         else:\n",
    "#             paths_before.append(path)\n",
    "#     print(f\"{event_id}: {len(paths):2} S1 paths, {len(paths_before):2} before, {len(paths_after):2} after ({mask_date})\")\n",
    "#     if len(paths_before) > 0 and len(paths_after) > 0:\n",
    "#         s2_valid_data_events.append(event_id)\n",
    "# len(s2_valid_data_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f2051-5469-44da-b857-2438fb8bdd4a",
   "metadata": {},
   "source": [
    "# S2 single sensor, single before/after image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a3ae77-1192-43da-bf7f-30ee9bef3476",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create cross validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084c5c82-c50c-4292-9d11-ecdbf75ec1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_file(\"data/mapbiomas_alerts.geojson\")\n",
    "events_sorted_by_area = df.sort_values(\"areaHa\", ascending=False)[\"alertCode\"].tolist()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e98212-b27f-4a21-b5db-5b22a0f8fcd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paths_before_, paths_after_ = [], []\n",
    "clouds_nodata_before_, clouds_nodata_after_ = [], []\n",
    "s2_valid_data_events = []\n",
    "for event_id in events_sorted_by_area[:]:\n",
    "    s2_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel2/*L2A*.tif\") if not \"ndvi\" in k and not \"ndwi\" in k])\n",
    "    if len(s2_paths) == 0:\n",
    "        continue\n",
    "    mask_path = glob.glob(f\"data/{event_id}/*_mask.tif\")[0]\n",
    "    mask_date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "    mask_date_str = f\"{mask_date[:4]}-{mask_date[4:6]}-{mask_date[6:8]}\"\n",
    "    mask_date = datetime.strptime(mask_date, \"%Y%m%d\")\n",
    "    # print(mask_date)\n",
    "    \n",
    "    date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "    # Sort by the date right before the .tif\n",
    "    paths = sorted(s2_paths, key=lambda p: datetime.strptime(p.split('_')[-1].split('.')[0], \"%Y%m%d\"))\n",
    "    # break\n",
    "    paths_after, paths_before = [], []\n",
    "    clouds_nodata_after, clouds_nodata_before = [], []\n",
    "    for path in paths:\n",
    "        # what_to_do_string = \"SKIP\"\n",
    "        img_orig = tifffile.imread(path)\n",
    "        # img_orig = img_orig.astype(np.float32)\n",
    "        # img_orig = img_orig - 1000\n",
    "        # img_orig[img_orig < 0] = 0\n",
    "        clouds_perc = 100 * (img_orig[:, :, 2] > 3400).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "        nodata_perc = 100 * (img_orig[:, :, 2] <= 1000).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "        date = path.split(\"/\")[-1].split('.')[0].split('_')[-1]\n",
    "        date_str = f\"{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "            \n",
    "        date = datetime.strptime(path.split('_')[-1].split('.')[0], \"%Y%m%d\")\n",
    "        if date > mask_date:\n",
    "            paths_after.append(path)\n",
    "            clouds_nodata_after.append(clouds_perc + nodata_perc)\n",
    "        else:\n",
    "            paths_before.append(path)\n",
    "            clouds_nodata_before.append(clouds_perc + nodata_perc)\n",
    "    print(f\"{event_id}: {len(paths):2} S2 L2A paths, {len(paths_before):2} before, {len(paths_after):2} after, ({mask_date})\")\n",
    "    if len(clouds_nodata_before) > 0 and len(clouds_nodata_after) > 0:\n",
    "        print(f\" --> min nodata/clouds before: {min(clouds_nodata_before):5.1f}%, after: {min(clouds_nodata_after):5.1f}%\")\n",
    "    if len(paths_before) > 0 and len(paths_after) > 0:\n",
    "        s2_valid_data_events.append(event_id)\n",
    "        paths_after_.append(paths_after)\n",
    "        clouds_nodata_after_.append(clouds_nodata_after)\n",
    "        paths_before_.append(paths_before)\n",
    "        clouds_nodata_before_.append(clouds_nodata_before)\n",
    "len(s2_valid_data_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372e464b-74d2-41c3-ae73-b4e150ca2896",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"event_id\": s2_valid_data_events, \n",
    "              \"paths_before\": paths_before_, \"clouds_nodata_before\": clouds_nodata_before_,\n",
    "                  \"paths_after\": paths_after_, \"clouds_nodata_after\": clouds_nodata_after_})\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c635b52-cbd9-488f-a7fa-9695178f2cc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def best_before_path(row):\n",
    "    # Take the earliest date with clouds < 10%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"], row[\"paths_before\"]):\n",
    "        if clouds_nodata < 10:\n",
    "            return path\n",
    "    # Take the earliest date with clouds < 30%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"], row[\"paths_before\"]):\n",
    "        if clouds_nodata < 30:\n",
    "            return path\n",
    "    # Take the earliest date with clouds < 100%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"], row[\"paths_before\"]):\n",
    "        if clouds_nodata < 100:\n",
    "            return path\n",
    "    # Else return the oldest path\n",
    "    return row[\"paths_before\"][0]\n",
    "\n",
    "def best_after_path(row):\n",
    "    # Take the latest date with clouds < 10%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"], row[\"paths_after\"]):\n",
    "        if clouds_nodata < 10:\n",
    "            return path\n",
    "    # Take the latest date with clouds < 30%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"], row[\"paths_after\"]):\n",
    "        if clouds_nodata < 30:\n",
    "            return path\n",
    "    # Take the latest date with clouds < 100%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"], row[\"paths_after\"]):\n",
    "        if clouds_nodata < 100:\n",
    "            return path\n",
    "    # Else return the newest path\n",
    "    return row[\"paths_after\"][-1]\n",
    "    \n",
    "df[\"path_best_before\"] = df[[\"paths_before\", \"clouds_nodata_before\"]].apply(lambda row: best_before_path(row), axis=1)\n",
    "df[\"path_best_after\"] = df[[\"paths_after\", \"clouds_nodata_after\"]].apply(lambda row: best_after_path(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a09e2d1-2074-4a98-87b2-425a9c78e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_paths = []\n",
    "deforest_pxs = []\n",
    "for event_id in df[\"event_id\"].tolist():\n",
    "    label_path = glob.glob(f\"data/{event_id}/*_mask.tif\")[0]\n",
    "    label_paths.append(label_path)\n",
    "    label = tifffile.imread(mask_path)\n",
    "    deforest_pxs.append((label == 255).sum())\n",
    "df[\"deforest_pxs\"] = deforest_pxs\n",
    "df[\"label_path\"] = label_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe615dfe-b37b-46b2-b50c-bfb83d7a1765",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c93318-6ac9-4034-9261-a51919cf556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 22222\n",
    "while True:\n",
    "    np.random.seed(seed)\n",
    "    folds = [0, 1, 2, 3, 4] * (len(df) // 5 + 1) #np.random.choice([0, 1, 2, 3, 4], size=len(df))\n",
    "    np.random.shuffle(folds)\n",
    "    df[\"fold\"] = folds[:len(df)]\n",
    "    # print(df[\"fold\"].value_counts())\n",
    "    std_counts = df.groupby(\"fold\")[\"deforest_pxs\"].sum().std().item()\n",
    "    # print(f'Seed {seed}: {df.groupby(\"fold\")[\"deforest_pxs\"].sum()}, Std = {std_counts:.0f}')\n",
    "    if std_counts < 1500:\n",
    "        break\n",
    "    seed += 1\n",
    "os.makedirs(\"catalogues\", exist_ok=True)\n",
    "df.to_pickle(\"catalogues/2025_08_18_s2_single.pkl\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4195f91-a82b-4519-bba4-99617f91e3cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8460766b-4d40-4d34-a0d7-641bd4092854",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38cf06e-6838-455b-8729-c2c353d03b04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_nb = \"S2_l2a_16d_1-3\"\n",
    "# path = \"catalogues/2025_08_19_s2_single.pkl\"\n",
    "path = \"catalogues/2025_08_19_s2_l2a_16d.pkl\"\n",
    "hps_dict = {\n",
    "    ############\n",
    "    # Data\n",
    "    ############\n",
    "    \"df_path\": path,\n",
    "\n",
    "    ############\n",
    "    # Training\n",
    "    ############\n",
    "\n",
    "    ## Experiment Setup\n",
    "    \"name\": f\"Exp_{exp_nb}\",\n",
    "\n",
    "    ## Model\n",
    "    \"num_classes\": 1,\n",
    "    \"input_channel\": 13,\n",
    "    \"backbone\": \"timm_efficientnet_b1\",\n",
    "    \"pretrained\": \"noisy-student\",\n",
    "    \"model\": \"unetplusplus\",\n",
    "\n",
    "    # Training Setup\n",
    "    \"print_freq\": 500,\n",
    "    \"use_fp16\": 0,\n",
    "    \"patience\": 8,\n",
    "    \"epoch_start_scheduler\": 10,\n",
    "\n",
    "    # Optimizer\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 0.0,\n",
    "\n",
    "    ## Data Augmentation on CPU\n",
    "    \"train_crop_size\": 256 + 32 + 32,\n",
    "    \"train_batch_size\": 16, #32\n",
    "    \"da_brightness_magnitude\": 0.0,\n",
    "    \"da_contrast_magnitude\": 0.0,\n",
    "\n",
    "    # Data Augmentation on GPU\n",
    "    \"gpu_da_params\": [0.25],\n",
    "    \n",
    "    ### Loss, Metric\n",
    "    \"alpha\": 0.25,\n",
    "           }\n",
    "\n",
    "for fold_nb in range(0, 5):\n",
    "    hps = HyperParams(**hps_dict)\n",
    "    hps.fold_nb = fold_nb\n",
    "    num_batches = 250 // (hps.train_batch_size * torch.cuda.device_count())\n",
    "    # num_batches = 10\n",
    "    hps.num_batches = num_batches\n",
    "    train_dataset, train_loader, val_dataset, val_loader = get_dataloaders(hps)\n",
    "    # continue\n",
    "\n",
    "    best_metric, best_metric_epoch = train(hps, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a7a6c7-5621-41b6-a33a-e641d547c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_nb = \"S2_l2a_16d_1-1\"\n",
    "# path = \"catalogues/2025_08_19_s2_single.pkl\"\n",
    "path = \"catalogues/2025_08_19_s2_l2a_16d.pkl\"\n",
    "hps_dict = {\n",
    "    ############\n",
    "    # Data\n",
    "    ############\n",
    "    \"df_path\": path,\n",
    "\n",
    "    ############\n",
    "    # Training\n",
    "    ############\n",
    "\n",
    "    ## Experiment Setup\n",
    "    \"name\": f\"Exp_{exp_nb}\",\n",
    "\n",
    "    ## Model\n",
    "    \"num_classes\": 1,\n",
    "    \"input_channel\": 13,\n",
    "    \"backbone\": \"timm_efficientnet_b1\",\n",
    "    \"pretrained\": 1,\n",
    "    \"model\": \"unetplusplus\",\n",
    "\n",
    "    # Training Setup\n",
    "#     \"resume\": \"trained_models/Exp7-4/fold_0/2022-02-05_23-13-25/best_metric_18_0.9768.pt\",\n",
    "    \"print_freq\": 500,\n",
    "    \"use_fp16\": 0,\n",
    "    \"patience\": 8, #5,\n",
    "    \"epoch_start_scheduler\": 10,\n",
    "\n",
    "    # Optimizer\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 0.0,\n",
    "\n",
    "    ## Data Augmentation on CPU\n",
    "    \"train_crop_size\": 256 + 32 + 32,\n",
    "    \"train_batch_size\": 16, #32\n",
    "    \"cutmix_alpha\": 0,\n",
    "    \"da_brightness_magnitude\": 0.0,\n",
    "    \"da_contrast_magnitude\": 0.0,\n",
    "\n",
    "    # Data Augmentation on GPU\n",
    "    \"gpu_da_params\": [0.25],\n",
    "    \n",
    "    ### Loss, Metric\n",
    "    \"alpha\": 0.25,\n",
    "#     \"loss\": \"lovasz\",\n",
    "           }\n",
    "\n",
    "for fold_nb in [4, 3, 2, 1, 0]:\n",
    "    hps = HyperParams(**hps_dict)\n",
    "    hps.fold_nb = fold_nb\n",
    "    num_batches = 250 // (hps.train_batch_size * torch.cuda.device_count())\n",
    "    # num_batches = 10\n",
    "    hps.num_batches = num_batches\n",
    "    train_dataset, train_loader, val_dataset, val_loader = get_dataloaders(hps)\n",
    "    # continue\n",
    "\n",
    "    best_metric, best_metric_epoch = train(hps, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cb4d52-5132-4b04-95dd-345025a4b81a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in range(100):\n",
    "    print(np.unique(train_dataset[k][\"mask\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43875d7c-b44b-4627-9123-13c4a9405452",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adf4ffa-68a1-4b42-a86b-42abeae16ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hps_list = []\n",
    "hps = HyperParams(**open_from_yaml(\"trained_models/ExpS2_single_0-3\"))\n",
    "hps_list.append(hps)\n",
    "df = pd.read_pickle(hps.df_path)\n",
    "\n",
    "pred_name = \"_\".join([hps_.name for hps_ in hps_list])\n",
    "print(pred_name)\n",
    "# threshold = 0.5 #0.8 #0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad7252f-308e-4114-bfb8-9cc1b03b4471",
   "metadata": {},
   "outputs": [],
   "source": [
    "also_save_preds = True\n",
    "threshold = 0.1\n",
    "fold_nbs = [0]#, 1, 2, 3, 4] #[4] \n",
    "\n",
    "subset = \"val_all_folds\"\n",
    "export = return_metrics_all_folds(hps_list, threshold=threshold, on_gpu=True, output_spatial_size=(400, 400), also_save_preds=also_save_preds,\n",
    "                                  save_path=f\"/data/C2S_CNN/predictions/{pred_name}_{subset}.bc\", fold_nbs=fold_nbs)\n",
    "# export_path = f\"data/metrics_{pred_name}_{subset}.npy\"\n",
    "# np.save(export_path, export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b642cf4-c311-4c74-b76d-a111276c72df",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, losses, tps, fps, fns = export\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a2f5f1-4cf5-4b84-aa26-98ef29994ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths_before, img_paths_after, GT_paths = [], [], []\n",
    "hps.only_val = True\n",
    "for fold_nb in fold_nbs:    \n",
    "    hps.fold_nb = fold_nb\n",
    "    val_dataset, val_loader = get_dataloaders(hps=hps)\n",
    "    img_paths_before.extend(val_dataset.img_paths_before)\n",
    "    img_paths_after.extend(val_dataset.img_paths_after)\n",
    "    GT_paths.extend(val_dataset.mask_paths)\n",
    "\n",
    "print(len(img_paths_before), len(img_paths_after), len(GT_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ae75a3-085c-4c76-8f58-eab3659c617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966d2171-5f29-43cf-8249-c0538d7f4135",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_criteria = \"FP+FN\" #\"FP+FN\" #\"FP+FN ratio\" #\"(FP+FN)/TP ratio\" #\"FP+FN\" #\"random\" #\"FP+FN\" #\"random\" \n",
    "specific_location = 0 #\"Senanga\" #\"Matadi\" #\"South America - Argentina\" #0 #\"Santa Cruz do Sul\" #\"Timbuktu\" #0 #\"Eritrea\" #0\n",
    "\n",
    "looked_at_paths, looked_at_idx = visualize_single_model(\n",
    "    df, tps, fps, fns, [probs], threshold, img_paths_before, img_paths_after, GT_paths, export_how_many=50, skip_how_many=0, alpha=0.4, \n",
    "    selection_criteria=selection_criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa53266-8ef5-4105-8743-5b615dc360e1",
   "metadata": {},
   "source": [
    "# S1 single sensor, single before/after image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910ba1cd-4c2d-4328-ba05-e8f574a0133a",
   "metadata": {},
   "source": [
    "## Create cross validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa5b11-5b52-4802-a501-3dea8b501b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_file(\"data/mapbiomas_alerts.geojson\")\n",
    "events_sorted_by_area = df.sort_values(\"areaHa\", ascending=False)[\"alertCode\"].tolist()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb62d90-d70d-4586-ac99-a8fce6ee27ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paths_before_, paths_after_ = [], []\n",
    "nodata_before_, nodata_after_ = [], []\n",
    "s1_valid_data_events = []\n",
    "for event_id in events_sorted_by_area[:]:\n",
    "    s1_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel1/*.tif\") if not \"_nrpb\" in k and not \"_rfdi\" in k and not \"_rvi\" in k and not \"_vv_vh_ratio\" in k])\n",
    "    if len(s1_paths) == 0:\n",
    "        continue\n",
    "    mask_path = glob.glob(f\"data/{event_id}/*_mask.tif\")[0]\n",
    "    mask_date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "    mask_date_str = f\"{mask_date[:4]}-{mask_date[4:6]}-{mask_date[6:8]}\"\n",
    "    mask_date = datetime.strptime(mask_date, \"%Y%m%d\")\n",
    "    # print(mask_date)\n",
    "    \n",
    "    date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "    # Sort by the date right before the .tif\n",
    "    paths = sorted(s1_paths, key=lambda p: datetime.strptime(p.split('_')[-1].split('.')[0], \"%Y%m%d\"))\n",
    "    # break\n",
    "    paths_after, paths_before = [], []\n",
    "    nodata_after, nodata_before = [], []\n",
    "    for path in paths:\n",
    "        # what_to_do_string = \"SKIP\"\n",
    "        img_orig = tifffile.imread(path)\n",
    "        nodata_perc = 100 * (img_orig[:, :, 0] == 0).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "        date = path.split(\"/\")[-1].split('.')[0].split('_')[-1]\n",
    "        date_str = f\"{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "            \n",
    "        date = datetime.strptime(path.split('_')[-1].split('.')[0], \"%Y%m%d\")\n",
    "        if date > mask_date:\n",
    "            paths_after.append(path)\n",
    "            nodata_after.append(nodata_perc)\n",
    "        else:\n",
    "            paths_before.append(path)\n",
    "            nodata_before.append(nodata_perc)\n",
    "    print(f\"{event_id}: {len(paths):2} S1 paths, {len(paths_before):2} before, {len(paths_after):2} after, ({mask_date})\")\n",
    "    if len(nodata_before) > 0 and len(nodata_after) > 0:\n",
    "        print(f\" --> min nodata before: {min(nodata_before):5.1f}%, after: {min(nodata_after):5.1f}%\")\n",
    "    if len(paths_before) > 0 and len(paths_after) > 0:\n",
    "        s1_valid_data_events.append(event_id)\n",
    "        paths_after_.append(paths_after)\n",
    "        nodata_after_.append(nodata_after)\n",
    "        paths_before_.append(paths_before)\n",
    "        nodata_before_.append(nodata_before)\n",
    "len(s1_valid_data_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caa8b12-767e-4dc2-81e8-9d8fcda99338",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"event_id\": s1_valid_data_events, \n",
    "              \"paths_before\": paths_before_, \"clouds_nodata_before\": nodata_before_,\n",
    "                  \"paths_after\": paths_after_, \"clouds_nodata_after\": nodata_after_})\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a219863-2dcd-4f67-89f8-d36f3c09a89f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def best_before_path(row):\n",
    "    # Take the earliest date with clouds < 10%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"], row[\"paths_before\"]):\n",
    "        if clouds_nodata < 10:\n",
    "            return path, clouds_nodata\n",
    "    # Take the earliest date with clouds < 30%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"], row[\"paths_before\"]):\n",
    "        if clouds_nodata < 30:\n",
    "            return path, clouds_nodata\n",
    "    # Take the earliest date with clouds < 100%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"], row[\"paths_before\"]):\n",
    "        if clouds_nodata < 100:\n",
    "            return path, clouds_nodata\n",
    "    # Else return the oldest path\n",
    "    return row[\"paths_before\"][0], row[\"clouds_nodata_before\"][0]\n",
    "\n",
    "def best_after_path(row):\n",
    "    # Take the latest date with clouds < 10%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"], row[\"paths_after\"]):\n",
    "        if clouds_nodata < 10:\n",
    "            return path, clouds_nodata\n",
    "    # Take the latest date with clouds < 30%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"], row[\"paths_after\"]):\n",
    "        if clouds_nodata < 30:\n",
    "            return path, clouds_nodata\n",
    "    # Take the latest date with clouds < 100%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"], row[\"paths_after\"]):\n",
    "        if clouds_nodata < 100:\n",
    "            return path, clouds_nodata\n",
    "    # Else return the newest path\n",
    "    return row[\"paths_after\"][-1], row[\"clouds_nodata_after\"][-1]\n",
    "    \n",
    "df[\"path_best_before\"] = df[[\"paths_before\", \"clouds_nodata_before\"]].apply(lambda row: best_before_path(row)[0], axis=1)\n",
    "df[\"path_best_after\"] = df[[\"paths_after\", \"clouds_nodata_after\"]].apply(lambda row: best_after_path(row)[0], axis=1)\n",
    "\n",
    "df[\"clouds_nodata_best_before\"] = df[[\"paths_before\", \"clouds_nodata_before\"]].apply(lambda row: best_before_path(row)[1], axis=1)\n",
    "df[\"clouds_nodata_best_after\"] = df[[\"paths_after\", \"clouds_nodata_after\"]].apply(lambda row: best_after_path(row)[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a4fa86-6d7f-4dde-90b7-eb80a5684103",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_paths = []\n",
    "deforest_pxs = []\n",
    "for event_id in df[\"event_id\"].tolist():\n",
    "    label_path = glob.glob(f\"data/{event_id}/*_mask.tif\")[0]\n",
    "    label_paths.append(label_path)\n",
    "    label = tifffile.imread(label_path)\n",
    "    deforest_pxs.append((label == 255).sum())\n",
    "df[\"deforest_pxs\"] = deforest_pxs\n",
    "df[\"label_path\"] = label_paths\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51eea01-448d-4241-8d5e-9e890d868e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c6a7bd-d94d-4b21-a293-2dcbc690bc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 222221\n",
    "while True:\n",
    "    np.random.seed(seed)\n",
    "    folds = [0, 1, 2, 3, 4] * (len(df) // 5 + 1) #np.random.choice([0, 1, 2, 3, 4], size=len(df))\n",
    "    np.random.shuffle(folds)\n",
    "    df[\"fold\"] = folds[:len(df)]\n",
    "    # print(df[\"fold\"].value_counts())\n",
    "    std_counts = df.groupby(\"fold\")[\"deforest_pxs\"].sum().std().item()\n",
    "    # print(f'Seed {seed}: {df.groupby(\"fold\")[\"deforest_pxs\"].sum()}, Std = {std_counts:.0f}')\n",
    "    if std_counts < 2000:\n",
    "        break\n",
    "    seed += 1\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c94dc7-aacd-4b56-a5a4-156529433f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    with rasterio.open(row[\"path_best_before\"]) as src:\n",
    "        output_path = row[\"path_best_before\"].replace(\".tif\", \"_uncompressed.tif\")\n",
    "        if not os.path.exists(output_path):\n",
    "            profile = src.profile\n",
    "            profile[\"compress\"] = None\n",
    "            with rasterio.open(output_path, \"w\", **profile) as dst:\n",
    "                dst.write(src.read())\n",
    "\n",
    "    with rasterio.open(row[\"path_best_after\"]) as src:\n",
    "        output_path = row[\"path_best_after\"].replace(\".tif\", \"_uncompressed.tif\")\n",
    "        if not os.path.exists(output_path):\n",
    "            profile = src.profile\n",
    "            profile[\"compress\"] = None\n",
    "            with rasterio.open(output_path, \"w\", **profile) as dst:\n",
    "                dst.write(src.read())\n",
    "\n",
    "df[\"path_best_before\"] = df[\"path_best_before\"].apply(lambda x: x.replace(\".tif\", \"_uncompressed.tif\"))\n",
    "df[\"path_best_after\"] = df[\"path_best_after\"].apply(lambda x: x.replace(\".tif\", \"_uncompressed.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da49f8-72c1-465b-8ea5-23388c8a6671",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"catalogues\", exist_ok=True)\n",
    "df.to_pickle(\"catalogues/2025_08_18_s1_single.pkl\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499636dc-3fff-490b-85f6-208c56be3b49",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff24fbb5-1089-4e0b-9fea-c89288c52621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"trained_models/ExpS1_single_1-0/fold_0/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dcdbd8-54a7-404a-bc69-aacd6737e2d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_nb = \"S1_single_1-0\"\n",
    "path = \"catalogues/2025_08_18_s1_single.pkl\"\n",
    "hps_dict = {\n",
    "    ############\n",
    "    # Data\n",
    "    ############\n",
    "    \"df_path\": path,\n",
    "\n",
    "    ############\n",
    "    # Training\n",
    "    ############\n",
    "\n",
    "    ## Experiment Setup\n",
    "    \"name\": f\"Exp{exp_nb}\",\n",
    "\n",
    "    ## Model\n",
    "    \"num_classes\": 1,\n",
    "    \"input_channel\": 2,\n",
    "    \"backbone\": \"timm_efficientnet_b1\",\n",
    "    \"pretrained\": 1,\n",
    "    \"model\": \"unetplusplus\",\n",
    "\n",
    "    # Training Setup\n",
    "#     \"resume\": \"trained_models/Exp7-4/fold_0/2022-02-05_23-13-25/best_metric_18_0.9768.pt\",\n",
    "    \"print_freq\": 500,\n",
    "    \"use_fp16\": 0,\n",
    "    \"patience\": 5, #5,\n",
    "    \"epoch_start_scheduler\": 5,\n",
    "\n",
    "    # Optimizer\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 0.0,\n",
    "\n",
    "    ## Data Augmentation on CPU\n",
    "    \"train_crop_size\": 256 + 32,\n",
    "    \"train_batch_size\": 16, #32\n",
    "    \"cutmix_alpha\": 0,\n",
    "    \"da_brightness_magnitude\": 0.0,\n",
    "    \"da_contrast_magnitude\": 0.0,\n",
    "\n",
    "    # Data Augmentation on GPU\n",
    "    \"gpu_da_params\": [0.25],\n",
    "    \n",
    "    ### Loss, Metric\n",
    "    \"alpha\": 0.25,\n",
    "#     \"loss\": \"lovasz\",\n",
    "           }\n",
    "\n",
    "for fold_nb in range(0, 5):\n",
    "    hps = HyperParams(**hps_dict)\n",
    "    hps.fold_nb = fold_nb\n",
    "    num_batches = 100 // (hps.train_batch_size * torch.cuda.device_count())\n",
    "    # num_batches = 10\n",
    "    hps.num_batches = num_batches\n",
    "    train_dataset, train_loader, val_dataset, val_loader = get_dataloaders(hps)\n",
    "    # continue\n",
    "\n",
    "    best_metric, best_metric_epoch = train(hps, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06ac2d3-627f-4f42-9e30-a5f6305a7a84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_nb = \"S1_single_1-1\"\n",
    "path = \"catalogues/2025_08_18_s1_single.pkl\"\n",
    "hps_dict = {\n",
    "    ############\n",
    "    # Data\n",
    "    ############\n",
    "    \"df_path\": path,\n",
    "\n",
    "    ############\n",
    "    # Training\n",
    "    ############\n",
    "\n",
    "    ## Experiment Setup\n",
    "    \"name\": f\"Exp{exp_nb}\",\n",
    "\n",
    "    ## Model\n",
    "    \"num_classes\": 1,\n",
    "    \"input_channel\": 2,\n",
    "    \"backbone\": \"timm_efficientnet_b1\",\n",
    "    \"pretrained\": 1,\n",
    "    \"model\": \"unetplusplus\",\n",
    "\n",
    "    # Training Setup\n",
    "#     \"resume\": \"trained_models/Exp7-4/fold_0/2022-02-05_23-13-25/best_metric_18_0.9768.pt\",\n",
    "    \"print_freq\": 500,\n",
    "    \"use_fp16\": 0,\n",
    "    \"patience\": 7, #5,\n",
    "    \"epoch_start_scheduler\": 7,\n",
    "\n",
    "    # Optimizer\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 0.0,\n",
    "\n",
    "    ## Data Augmentation on CPU\n",
    "    \"train_crop_size\": 256 + 32 + 32,\n",
    "    \"train_batch_size\": 16, #32\n",
    "    \"cutmix_alpha\": 0,\n",
    "    \"da_brightness_magnitude\": 0.0,\n",
    "    \"da_contrast_magnitude\": 0.0,\n",
    "\n",
    "    # Data Augmentation on GPU\n",
    "    \"gpu_da_params\": [0.25],\n",
    "    \n",
    "    ### Loss, Metric\n",
    "    \"alpha\": 0.25,\n",
    "#     \"loss\": \"lovasz\",\n",
    "           }\n",
    "\n",
    "for fold_nb in range(0, 5):\n",
    "    hps = HyperParams(**hps_dict)\n",
    "    hps.fold_nb = fold_nb\n",
    "    num_batches = 150 // (hps.train_batch_size * torch.cuda.device_count())\n",
    "    # num_batches = 10\n",
    "    hps.num_batches = num_batches\n",
    "    train_dataset, train_loader, val_dataset, val_loader = get_dataloaders(hps)\n",
    "    # continue\n",
    "\n",
    "    best_metric, best_metric_epoch = train(hps, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d20964-b691-4fef-a6a7-57f6cc2e2a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"trained_models/ExpS1_single_1-2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d08919a-f3f2-49f0-9a86-b821bd4123da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_nb = \"S1_single_1-2\"\n",
    "path = \"catalogues/2025_08_18_s1_single.pkl\"\n",
    "hps_dict = {\n",
    "    ############\n",
    "    # Data\n",
    "    ############\n",
    "    \"df_path\": path,\n",
    "\n",
    "    ############\n",
    "    # Training\n",
    "    ############\n",
    "\n",
    "    ## Experiment Setup\n",
    "    \"name\": f\"Exp{exp_nb}\",\n",
    "\n",
    "    ## Model\n",
    "    \"num_classes\": 1,\n",
    "    \"input_channel\": 2,\n",
    "    \"backbone\": \"timm_efficientnet_b1\",\n",
    "    \"pretrained\": 1,\n",
    "    \"model\": \"unetplusplus\",\n",
    "\n",
    "    # Training Setup\n",
    "#     \"resume\": \"trained_models/Exp7-4/fold_0/2022-02-05_23-13-25/best_metric_18_0.9768.pt\",\n",
    "    \"print_freq\": 500,\n",
    "    \"use_fp16\": 0,\n",
    "    \"patience\": 5, #5,\n",
    "    \"epoch_start_scheduler\": 10,\n",
    "\n",
    "    # Optimizer\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 0.0,\n",
    "\n",
    "    ## Data Augmentation on CPU\n",
    "    \"train_crop_size\": 256 + 32 + 32,\n",
    "    \"train_batch_size\": 16, #32\n",
    "    \"cutmix_alpha\": 0,\n",
    "    \"da_brightness_magnitude\": 0.0,\n",
    "    \"da_contrast_magnitude\": 0.0,\n",
    "\n",
    "    # Data Augmentation on GPU\n",
    "    \"gpu_da_params\": [0.25],\n",
    "    \n",
    "    ### Loss, Metric\n",
    "    \"alpha\": 0.5,\n",
    "#     \"loss\": \"lovasz\",\n",
    "           }\n",
    "\n",
    "for fold_nb in range(0, 5):\n",
    "    hps = HyperParams(**hps_dict)\n",
    "    hps.fold_nb = fold_nb\n",
    "    num_batches = 100 // (hps.train_batch_size * torch.cuda.device_count())\n",
    "    # num_batches = 10\n",
    "    hps.num_batches = num_batches\n",
    "    train_dataset, train_loader, val_dataset, val_loader = get_dataloaders(hps)\n",
    "    # continue\n",
    "\n",
    "    best_metric, best_metric_epoch = train(hps, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715840f6-c2b7-4661-a4ee-b939f4a96843",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in range(100):\n",
    "    print(np.unique(train_dataset[k][\"mask\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de232a73-2718-4019-a620-3308827db69e",
   "metadata": {},
   "source": [
    "## Create cross validation dataset with using the last after image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4679646-ac4a-4723-b0d6-0069e0175239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_file(\"data/mapbiomas_alerts.geojson\")\n",
    "events_sorted_by_area = df.sort_values(\"areaHa\", ascending=False)[\"alertCode\"].tolist()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e494c6-2357-4171-b565-1fb7dd2ff067",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paths_before_, paths_after_ = [], []\n",
    "nodata_before_, nodata_after_ = [], []\n",
    "s1_valid_data_events = []\n",
    "for event_id in events_sorted_by_area[:]:\n",
    "    s1_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel1/*.tif\") if not \"_nrpb\" in k and not \"_rfdi\" in k and not \"_rvi\" in k and not \"_vv_vh_ratio\" in k and not \"uncompressed\" in k])\n",
    "    if len(s1_paths) == 0:\n",
    "        continue\n",
    "    mask_path = glob.glob(f\"data/{event_id}/*_mask.tif\")[0]\n",
    "    mask_date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "    mask_date_str = f\"{mask_date[:4]}-{mask_date[4:6]}-{mask_date[6:8]}\"\n",
    "    mask_date = datetime.strptime(mask_date, \"%Y%m%d\")\n",
    "    # print(mask_date)\n",
    "    \n",
    "    date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "    # Sort by the date right before the .tif\n",
    "    paths = sorted(s1_paths, key=lambda p: datetime.strptime(p.split('_')[-1].split('.')[0], \"%Y%m%d\"))\n",
    "    # break\n",
    "    paths_after, paths_before = [], []\n",
    "    nodata_after, nodata_before = [], []\n",
    "    for path in paths:\n",
    "        # what_to_do_string = \"SKIP\"\n",
    "        img_orig = tifffile.imread(path)\n",
    "        nodata_perc = 100 * (img_orig[:, :, 0] == 0).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "        date = path.split(\"/\")[-1].split('.')[0].split('_')[-1]\n",
    "        date_str = f\"{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "            \n",
    "        date = datetime.strptime(path.split('_')[-1].split('.')[0], \"%Y%m%d\")\n",
    "        if date > mask_date:\n",
    "            paths_after.append(path)\n",
    "            nodata_after.append(nodata_perc)\n",
    "        else:\n",
    "            paths_before.append(path)\n",
    "            nodata_before.append(nodata_perc)\n",
    "    print(f\"{event_id}: {len(paths):2} S1 paths, {len(paths_before):2} before, {len(paths_after):2} after, ({mask_date})\")\n",
    "    if len(nodata_before) > 0 and len(nodata_after) > 0:\n",
    "        print(f\" --> min nodata before: {min(nodata_before):5.1f}%, after: {min(nodata_after):5.1f}%\")\n",
    "    if len(paths_before) > 0 and len(paths_after) > 0:\n",
    "        s1_valid_data_events.append(event_id)\n",
    "        paths_after_.append(paths_after)\n",
    "        nodata_after_.append(nodata_after)\n",
    "        paths_before_.append(paths_before)\n",
    "        nodata_before_.append(nodata_before)\n",
    "len(s1_valid_data_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbe5a69-e8e5-414d-9699-43abe6f633a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"event_id\": s1_valid_data_events, \n",
    "              \"paths_before\": paths_before_, \"clouds_nodata_before\": nodata_before_,\n",
    "                  \"paths_after\": paths_after_, \"clouds_nodata_after\": nodata_after_})\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264d213e-7f19-4dd8-b084-eed90d8df367",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def best_before_path(row):\n",
    "    # Take the earliest date with clouds < 10%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"], row[\"paths_before\"]):\n",
    "        if clouds_nodata < 10:\n",
    "            return path, clouds_nodata\n",
    "    # Take the earliest date with clouds < 30%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"], row[\"paths_before\"]):\n",
    "        if clouds_nodata < 30:\n",
    "            return path, clouds_nodata\n",
    "    # Take the earliest date with clouds < 100%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"], row[\"paths_before\"]):\n",
    "        if clouds_nodata < 100:\n",
    "            return path, clouds_nodata\n",
    "    # Else return the oldest path\n",
    "    return row[\"paths_before\"][0], row[\"clouds_nodata_before\"][0]\n",
    "\n",
    "def best_after_path(row):\n",
    "    # Take the latest date with clouds < 10%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"], row[\"paths_after\"][::-1]):\n",
    "        if clouds_nodata < 10:\n",
    "            return path, clouds_nodata\n",
    "    # Take the latest date with clouds < 30%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"], row[\"paths_after\"][::-1]):\n",
    "        if clouds_nodata < 30:\n",
    "            return path, clouds_nodata\n",
    "    # Take the latest date with clouds < 100%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"], row[\"paths_after\"][::-1]):\n",
    "        if clouds_nodata < 100:\n",
    "            return path, clouds_nodata\n",
    "    # Else return the newest path\n",
    "    return row[\"paths_after\"][-1], row[\"clouds_nodata_after\"][-1]\n",
    "    \n",
    "df[\"path_best_before\"] = df[[\"paths_before\", \"clouds_nodata_before\"]].apply(lambda row: best_before_path(row)[0], axis=1)\n",
    "df[\"path_best_after\"] = df[[\"paths_after\", \"clouds_nodata_after\"]].apply(lambda row: best_after_path(row)[0], axis=1)\n",
    "\n",
    "df[\"clouds_nodata_best_before\"] = df[[\"paths_before\", \"clouds_nodata_before\"]].apply(lambda row: best_before_path(row)[1], axis=1)\n",
    "df[\"clouds_nodata_best_after\"] = df[[\"paths_after\", \"clouds_nodata_after\"]].apply(lambda row: best_after_path(row)[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4955bf5a-a9b0-4e47-a9a9-dff237b083cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_paths = []\n",
    "deforest_pxs = []\n",
    "for event_id in df[\"event_id\"].tolist():\n",
    "    label_path = glob.glob(f\"data/{event_id}/*_mask.tif\")[0]\n",
    "    label_paths.append(label_path)\n",
    "    label = tifffile.imread(label_path)\n",
    "    deforest_pxs.append((label == 255).sum())\n",
    "df[\"deforest_pxs\"] = deforest_pxs\n",
    "df[\"label_path\"] = label_paths\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc879ca-6075-4aac-983b-8e2c5c903749",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6e3899-02d2-4642-95a9-0d1672de1933",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 222221\n",
    "while True:\n",
    "    np.random.seed(seed)\n",
    "    folds = [0, 1, 2, 3, 4] * (len(df) // 5 + 1) #np.random.choice([0, 1, 2, 3, 4], size=len(df))\n",
    "    np.random.shuffle(folds)\n",
    "    df[\"fold\"] = folds[:len(df)]\n",
    "    # print(df[\"fold\"].value_counts())\n",
    "    std_counts = df.groupby(\"fold\")[\"deforest_pxs\"].sum().std().item()\n",
    "    # print(f'Seed {seed}: {df.groupby(\"fold\")[\"deforest_pxs\"].sum()}, Std = {std_counts:.0f}')\n",
    "    if std_counts < 2000:\n",
    "        break\n",
    "    seed += 1\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef8110a-5e7f-4af3-b831-11bdb515a833",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    with rasterio.open(row[\"path_best_before\"]) as src:\n",
    "        output_path = row[\"path_best_before\"].replace(\".tif\", \"_uncompressed.tif\")\n",
    "        if not os.path.exists(output_path):\n",
    "            profile = src.profile\n",
    "            profile[\"compress\"] = None\n",
    "            with rasterio.open(output_path, \"w\", **profile) as dst:\n",
    "                dst.write(src.read())\n",
    "\n",
    "    with rasterio.open(row[\"path_best_after\"]) as src:\n",
    "        output_path = row[\"path_best_after\"].replace(\".tif\", \"_uncompressed.tif\")\n",
    "        if not os.path.exists(output_path):\n",
    "            profile = src.profile\n",
    "            profile[\"compress\"] = None\n",
    "            with rasterio.open(output_path, \"w\", **profile) as dst:\n",
    "                dst.write(src.read())\n",
    "\n",
    "df[\"path_best_before\"] = df[\"path_best_before\"].apply(lambda x: x.replace(\".tif\", \"_uncompressed.tif\"))\n",
    "df[\"path_best_after\"] = df[\"path_best_after\"].apply(lambda x: x.replace(\".tif\", \"_uncompressed.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2152737-2291-4cef-bfcd-6f946d1002c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"catalogues\", exist_ok=True)\n",
    "df.to_pickle(\"catalogues/2025_08_18_s1_single_last_after_img.pkl\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4db03f5-6b74-490c-98f3-13ff523d444f",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b20d4c-653b-4f5c-8f8e-18f6cc062772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# shutil.rmtree(\"trained_models/ExpS1_single_1-0/fold_0/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf62af91-1759-4ba6-a806-a98fb3056b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_nb = \"S1_single_2-0\"\n",
    "path = \"catalogues/2025_08_18_s1_single_last_after_img.pkl\"\n",
    "hps_dict = {\n",
    "    ############\n",
    "    # Data\n",
    "    ############\n",
    "    \"df_path\": path,\n",
    "\n",
    "    ############\n",
    "    # Training\n",
    "    ############\n",
    "\n",
    "    ## Experiment Setup\n",
    "    \"name\": f\"Exp{exp_nb}\",\n",
    "\n",
    "    ## Model\n",
    "    \"num_classes\": 1,\n",
    "    \"input_channel\": 2,\n",
    "    \"backbone\": \"timm_efficientnet_b1\",\n",
    "    \"pretrained\": 1,\n",
    "    \"model\": \"unetplusplus\",\n",
    "\n",
    "    # Training Setup\n",
    "#     \"resume\": \"trained_models/Exp7-4/fold_0/2022-02-05_23-13-25/best_metric_18_0.9768.pt\",\n",
    "    \"print_freq\": 500,\n",
    "    \"use_fp16\": 0,\n",
    "    \"patience\": 5, #5,\n",
    "    \"epoch_start_scheduler\": 5,\n",
    "\n",
    "    # Optimizer\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 0.0,\n",
    "\n",
    "    ## Data Augmentation on CPU\n",
    "    \"train_crop_size\": 256 + 32,\n",
    "    \"train_batch_size\": 16, #32\n",
    "    \"cutmix_alpha\": 0,\n",
    "    \"da_brightness_magnitude\": 0.0,\n",
    "    \"da_contrast_magnitude\": 0.0,\n",
    "\n",
    "    # Data Augmentation on GPU\n",
    "    \"gpu_da_params\": [0.25],\n",
    "    \n",
    "    ### Loss, Metric\n",
    "    \"alpha\": 0.25,\n",
    "#     \"loss\": \"lovasz\",\n",
    "           }\n",
    "\n",
    "for fold_nb in range(0, 5):\n",
    "    hps = HyperParams(**hps_dict)\n",
    "    hps.fold_nb = fold_nb\n",
    "    num_batches = 100 // (hps.train_batch_size * torch.cuda.device_count())\n",
    "    # num_batches = 10\n",
    "    hps.num_batches = num_batches\n",
    "    train_dataset, train_loader, val_dataset, val_loader = get_dataloaders(hps)\n",
    "    # continue\n",
    "\n",
    "    best_metric, best_metric_epoch = train(hps, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3b10e1-552f-4661-95c5-2eb0daebe408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
