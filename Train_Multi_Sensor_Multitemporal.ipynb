{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5416e8ab-b636-47aa-b77d-87b4e2a4871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Minimal PyTorch skeleton for variable-length, multi-sensor, multitemporal change segmentation\n",
    "(Per-sensor encoders → temporal transformer aggregation → U-Net style decoder)\n",
    "\n",
    "Key ideas:\n",
    "- Accepts a list of frames per sample; each frame has (image tensor, sensor id, Δdays to event, optional per-pixel valid mask).\n",
    "- Per-sensor encoders produce two scales of features (H/2, H/4).  \n",
    "- We add **time** (Δdays) and **sensor** embeddings to each frame's features.\n",
    "- A **temporal Transformer** aggregates across time **per spatial location** at each scale, with masking for missing/invalid pixels.\n",
    "- A light U-Net decoder upsamples to full resolution to output a binary change mask.\n",
    "\n",
    "This is a scaffold you can adapt/extend (e.g., deeper backbones, more scales, SAR-specific preprocessing, etc.).\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -----------------------------\n",
    "# Utility: small building blocks\n",
    "# -----------------------------\n",
    "\n",
    "class ConvBNAct(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, k: int = 3, s: int = 1, p: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        if p is None:\n",
    "            p = k // 2\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, k, s, p, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, ch: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBNAct(ch, ch)\n",
    "        self.conv2 = ConvBNAct(ch, ch)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.conv2(self.conv1(x))\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.conv = ConvBNAct(in_ch, out_ch, k=3, s=2, p=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "# Per-sensor lightweight encoders\n",
    "# ---------------------------------\n",
    "\n",
    "class SensorEncoder(nn.Module):\n",
    "    \"\"\"A tiny 2-stage encoder that returns two scales: (H/2), (H/4).\n",
    "\n",
    "    Replace with timm/SegFormer backbones if desired. Keep channel dims same across sensors\n",
    "    so fusion is easy.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch: int, base_ch: int = 48):\n",
    "        super().__init__()\n",
    "        # Stage 1 → H/2\n",
    "        self.stem = ConvBNAct(in_ch, base_ch, k=3, s=2, p=1)\n",
    "        self.res1 = ResidualBlock(base_ch)\n",
    "        # Stage 2 → H/4\n",
    "        self.down2 = Downsample(base_ch, base_ch * 2)\n",
    "        self.res2 = ResidualBlock(base_ch * 2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        f1 = self.res1(self.stem(x))          # (B, C1, H/2, W/2)\n",
    "        f2 = self.res2(self.down2(f1))        # (B, C2, H/4, W/4)\n",
    "        return f1, f2\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "# Time & sensor embeddings (FiLM-ish)\n",
    "# ---------------------------------\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"Encode Δdays (float) → feature bias + scale.\n",
    "    We output a vector of size C and simply **add** it to feature maps (could FiLM with scale, too).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(16, d_model),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(d_model, d_model),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _fourier_features(x: torch.Tensor, B: int = 8) -> torch.Tensor:\n",
    "        # x: (N,) in days. Map to sin/cos features.\n",
    "        device = x.device\n",
    "        freqs = 2 ** torch.arange(B, device=device).float() * 2 * math.pi / 365.0\n",
    "        xf = x[:, None] * freqs[None, :]\n",
    "        return torch.cat([torch.sin(xf), torch.cos(xf)], dim=-1)  # (N, 2B)\n",
    "\n",
    "    def forward(self, delta_days: torch.Tensor) -> torch.Tensor:\n",
    "        ff = self._fourier_features(delta_days)  # (N, 16)\n",
    "        return self.proj(ff)                     # (N, d_model)\n",
    "\n",
    "\n",
    "class SensorEmbedding(nn.Module):\n",
    "    def __init__(self, sensor_vocab: List[str], d_model: int):\n",
    "        super().__init__()\n",
    "        self.sensor_to_idx: Dict[str, int] = {s: i for i, s in enumerate(sensor_vocab)}\n",
    "        self.emb = nn.Embedding(len(sensor_vocab), d_model)\n",
    "\n",
    "    def forward(self, sensors: List[str]) -> torch.Tensor:\n",
    "        idx = torch.tensor([self.sensor_to_idx[s] for s in sensors], dtype=torch.long, device=self.emb.weight.device)\n",
    "        return self.emb(idx)  # (N, d_model)\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "# Temporal aggregation: Transformer per spatial location\n",
    "# ---------------------------------\n",
    "\n",
    "class TemporalAggregator(nn.Module):\n",
    "    \"\"\"Aggregate a variable-length list of frame features (C, H, W) using a Transformer along time.\n",
    "\n",
    "    We process one **sample** at a time to keep the padding/masking simple. This is fine as a template; \n",
    "    for speed, you can pack multiple samples by padding T to the batch max and using attn masks.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int, nhead: int = 8, num_layers: int = 2, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model=channels, nhead=nhead, dropout=dropout, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        feats: List[torch.Tensor],          # List[T] of (C, H, W)\n",
    "        valid_masks: Optional[List[torch.Tensor]] = None,  # List[T] of (1, H, W) or None\n",
    "    ) -> torch.Tensor:\n",
    "        assert len(feats) > 0, \"No frames provided\"\n",
    "        C, H, W = feats[0].shape\n",
    "        T = len(feats)\n",
    "\n",
    "        # Stack to (T, H*W, C)\n",
    "        x = torch.stack(feats, dim=0)                  # (T, C, H, W)\n",
    "        x = x.permute(0, 2, 3, 1).contiguous().view(T, H * W, C)  # (T, HW, C)\n",
    "\n",
    "        # Build padding mask over time if per-pixel valid masks provided\n",
    "        # Padding mask: (N, S) where N=batch(HW), S=seq_len(T). True = PAD\n",
    "        if valid_masks is not None and valid_masks[0] is not None:\n",
    "            m = torch.stack(valid_masks, dim=0)  # (T, 1, H, W)\n",
    "            m = (m > 0.5).float()\n",
    "            m = m.permute(0, 2, 3, 1).contiguous().view(T, H * W, 1)  # (T, HW, 1)\n",
    "            # if all zeros at a pixel across time, we'll treat all as valid (avoid NaNs)\n",
    "            pix_has_valid = (m.sum(dim=0, keepdim=False) > 0)  # (HW, 1)\n",
    "            # Create time-wise mask: invalid → pad=True\n",
    "            pad_mask = (~(m.bool())).permute(1, 0, 2).squeeze(-1)  # (HW, T)\n",
    "            # For pixels with no valid frames at all, mark all as not padded so the transformer sees something\n",
    "            pad_mask = torch.where(pix_has_valid.permute(1, 0), pad_mask, torch.zeros_like(pad_mask))\n",
    "        else:\n",
    "            pad_mask = None\n",
    "\n",
    "        # Transformer expects (batch=HW, seq=T, d=C) when batch_first=True\n",
    "        x = x.permute(1, 0, 2).contiguous()  # (HW, T, C)\n",
    "        x = self.encoder(x, src_key_padding_mask=pad_mask)  # (HW, T, C)\n",
    "\n",
    "        # Aggregate over time: masked mean\n",
    "        if pad_mask is not None:\n",
    "            valid = (~pad_mask).float()  # (HW, T)\n",
    "            denom = valid.sum(dim=1, keepdim=True).clamp_min(1.0)\n",
    "            agg = (x * valid.unsqueeze(-1)).sum(dim=1) / denom  # (HW, C)\n",
    "        else:\n",
    "            agg = x.mean(dim=1)  # (HW, C)\n",
    "\n",
    "        agg = agg.view(H, W, C).permute(2, 0, 1).contiguous()  # (C, H, W)\n",
    "        return agg\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "# Decoder (2-scale U-Net style)\n",
    "# ---------------------------------\n",
    "\n",
    "class UNetDecoder2Scale(nn.Module):\n",
    "    def __init__(self, ch_low: int, ch_high: int, out_ch: int = 1):\n",
    "        super().__init__()\n",
    "        self.up1 = nn.ConvTranspose2d(ch_high, ch_low, kernel_size=2, stride=2)\n",
    "        self.fuse1 = nn.Sequential(\n",
    "            ConvBNAct(ch_low + ch_low, ch_low),\n",
    "            ResidualBlock(ch_low),\n",
    "        )\n",
    "        self.up2 = nn.ConvTranspose2d(ch_low, ch_low // 2, kernel_size=2, stride=2)\n",
    "        self.head = nn.Sequential(\n",
    "            ConvBNAct(ch_low // 2, ch_low // 4),\n",
    "            nn.Conv2d(ch_low // 4, out_ch, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, f_low: torch.Tensor, f_high: torch.Tensor) -> torch.Tensor:\n",
    "        # f_low: (B, C1, H/2, W/2); f_high: (B, C2, H/4, W/4)\n",
    "        x = self.up1(f_high)  # → (B, C1, H/2, W/2)\n",
    "        x = torch.cat([x, f_low], dim=1)\n",
    "        x = self.fuse1(x)\n",
    "        x = self.up2(x)       # → (B, C1//2, H, W)\n",
    "        logits = self.head(x) # → (B, 1, H, W)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "# Full model\n",
    "# ---------------------------------\n",
    "\n",
    "@dataclass\n",
    "class Frame:\n",
    "    image: torch.Tensor          # (C, H, W)\n",
    "    sensor: str                  # e.g., 'S2', 'S1', 'Landsat', 'BR'\n",
    "    delta_days: float            # days relative to event date (negative=pre, positive=post)\n",
    "    valid_mask: Optional[torch.Tensor] = None  # (1, H, W) 1=valid, 0=invalid (e.g., clouds)\n",
    "\n",
    "\n",
    "class MultiSensorTemporalChangeSeg(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sensor_specs: Dict[str, int],   # sensor_id → in_channels\n",
    "        base_ch: int = 48,\n",
    "        t_nhead: int = 8,\n",
    "        t_layers: int = 2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sensors = list(sensor_specs.keys())\n",
    "        # Build per-sensor encoders with the same channel dims\n",
    "        self.encoders = nn.ModuleDict({s: SensorEncoder(in_ch=cin, base_ch=base_ch) for s, cin in sensor_specs.items()})\n",
    "\n",
    "        ch_low = base_ch\n",
    "        ch_high = base_ch * 2\n",
    "\n",
    "        # Embeddings to add to features\n",
    "        self.time_emb_low = TimeEmbedding(ch_low)\n",
    "        self.time_emb_high = TimeEmbedding(ch_high)\n",
    "        self.sensor_emb_low = SensorEmbedding(self.sensors, ch_low)\n",
    "        self.sensor_emb_high = SensorEmbedding(self.sensors, ch_high)\n",
    "\n",
    "        # Temporal aggregators (per scale)\n",
    "        self.tagg_low = TemporalAggregator(channels=ch_low, nhead=t_nhead, num_layers=t_layers)\n",
    "        self.tagg_high = TemporalAggregator(channels=ch_high, nhead=t_nhead, num_layers=t_layers)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = UNetDecoder2Scale(ch_low=ch_low, ch_high=ch_high, out_ch=1)\n",
    "\n",
    "    def _apply_emb(self, feat: torch.Tensor, t_vec: torch.Tensor, s_vec: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Add time and sensor embeddings to a feature map (C, H, W).\"\"\"\n",
    "        C, H, W = feat.shape\n",
    "        t = t_vec.view(C, 1, 1)\n",
    "        s = s_vec.view(C, 1, 1)\n",
    "        return feat + t + s\n",
    "\n",
    "    def forward(self, batch: List[List[Frame]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch: list of samples; each sample is a list of Frame objects (variable length)\n",
    "        Returns:\n",
    "            logits: (B, 1, H, W) binary change logits\n",
    "        Notes:\n",
    "            - All frames within a sample must share the same (H, W). Different samples in the same batch\n",
    "              must also share (H, W) for this simple implementation.\n",
    "        \"\"\"\n",
    "        B = len(batch)\n",
    "        assert B > 0, \"Empty batch\"\n",
    "        # Determine spatial size from first frame of first sample\n",
    "        H, W = batch[0][0].image.shape[-2:]\n",
    "\n",
    "        f_low_agg_list = []\n",
    "        f_high_agg_list = []\n",
    "\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        for sample in batch:\n",
    "            # Per-sample lists of per-frame features (C, H', W')\n",
    "            low_feats: List[torch.Tensor] = []\n",
    "            high_feats: List[torch.Tensor] = []\n",
    "            low_masks: List[Optional[torch.Tensor]] = []\n",
    "            high_masks: List[Optional[torch.Tensor]] = []\n",
    "            time_vecs_low: List[torch.Tensor] = []\n",
    "            time_vecs_high: List[torch.Tensor] = []\n",
    "            sens_vecs_low: List[torch.Tensor] = []\n",
    "            sens_vecs_high: List[torch.Tensor] = []\n",
    "\n",
    "            if len(sample) == 0:\n",
    "                raise ValueError(\"A sample in the batch has zero frames.\")\n",
    "\n",
    "            for fr in sample:\n",
    "                assert fr.image.shape[-2:] == (H, W), \"All frames in a batch must share H, W\"\n",
    "                x = fr.image.unsqueeze(0).to(device)  # (1, C, H, W)\n",
    "                enc = self.encoders[fr.sensor]\n",
    "                f1, f2 = enc(x)  # shapes: (1, C1, H/2, W/2), (1, C2, H/4, W/4)\n",
    "                f1, f2 = f1.squeeze(0), f2.squeeze(0)\n",
    "\n",
    "                # Embeddings\n",
    "                dd = torch.tensor([fr.delta_days], device=device, dtype=torch.float32)\n",
    "                t1 = self.time_emb_low(dd)[0]   # (C1,)\n",
    "                t2 = self.time_emb_high(dd)[0]  # (C2,)\n",
    "                s1 = self.sensor_emb_low([fr.sensor])[0]  # (C1,)\n",
    "                s2 = self.sensor_emb_high([fr.sensor])[0] # (C2,)\n",
    "\n",
    "                f1 = self._apply_emb(f1, t1, s1)\n",
    "                f2 = self._apply_emb(f2, t2, s2)\n",
    "\n",
    "                low_feats.append(f1)\n",
    "                high_feats.append(f2)\n",
    "\n",
    "                if fr.valid_mask is not None:\n",
    "                    # Downsample valid mask to the two scales (nearest keeps binary nature)\n",
    "                    vm = fr.valid_mask.to(device)\n",
    "                    vm1 = F.interpolate(vm.unsqueeze(0), scale_factor=0.5, mode=\"nearest\").squeeze(0)   # (1, H/2, W/2)\n",
    "                    vm2 = F.interpolate(vm.unsqueeze(0), scale_factor=0.25, mode=\"nearest\").squeeze(0)  # (1, H/4, W/4)\n",
    "                else:\n",
    "                    vm1 = None\n",
    "                    vm2 = None\n",
    "                low_masks.append(vm1)\n",
    "                high_masks.append(vm2)\n",
    "\n",
    "            # Temporal aggregation per scale\n",
    "            f1_agg = self.tagg_low(low_feats, low_masks)    # (C1, H/2, W/2)\n",
    "            f2_agg = self.tagg_high(high_feats, high_masks) # (C2, H/4, W/4)\n",
    "\n",
    "            f_low_agg_list.append(f1_agg.unsqueeze(0))\n",
    "            f_high_agg_list.append(f2_agg.unsqueeze(0))\n",
    "\n",
    "        f_low_agg = torch.cat(f_low_agg_list, dim=0)   # (B, C1, H/2, W/2)\n",
    "        f_high_agg = torch.cat(f_high_agg_list, dim=0) # (B, C2, H/4, W/4)\n",
    "\n",
    "        logits = self.decoder(f_low_agg, f_high_agg)   # (B, 1, H, W)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "# Example usage & simple sanity test\n",
    "# ---------------------------------\n",
    "\n",
    "def _dummy_batch(\n",
    "    B: int = 2,\n",
    "    T_list: List[int] = (4, 7),\n",
    "    H: int = 256,\n",
    "    W: int = 256,\n",
    "    device: str = \"cpu\",\n",
    ") -> List[List[Frame]]:\n",
    "    torch.manual_seed(0)\n",
    "    sensors = [\"S2\", \"S1\", \"Landsat\", \"BR\"]\n",
    "    batch: List[List[Frame]] = []\n",
    "    for b in range(B):\n",
    "        frames: List[Frame] = []\n",
    "        T = T_list[b % len(T_list)]\n",
    "        for t in range(T):\n",
    "            sensor = sensors[t % len(sensors)]\n",
    "            C = {\"S2\": 4, \"S1\": 2, \"Landsat\": 6, \"BR\": 4}[sensor]\n",
    "            img = torch.randn(C, H, W, device=device)\n",
    "            delta_days = float(-20 + 5 * t)  # toy\n",
    "            # 20% chance to have a cloud mask with 80% valid pixels\n",
    "            if torch.rand(1).item() < 0.5:\n",
    "                vm = (torch.rand(1, H, W, device=device) > 0.2).float()\n",
    "            else:\n",
    "                vm = None\n",
    "            frames.append(Frame(image=img, sensor=sensor, delta_days=delta_days, valid_mask=vm))\n",
    "        batch.append(frames)\n",
    "    return batch\n",
    "\n",
    "\n",
    "def _sanity_run():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    sensor_specs = {\"S2\": 4, \"S1\": 2, \"Landsat\": 6, \"BR\": 4}\n",
    "    model = MultiSensorTemporalChangeSeg(sensor_specs, base_ch=32, t_nhead=8, t_layers=2).to(device)\n",
    "    batch = _dummy_batch(B=2, T_list=[5, 3], H=128, W=128, device=device)\n",
    "    logits = model(batch)  # (B, 1, H, W)\n",
    "    print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _sanity_run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
