{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47382a04-3378-458b-875c-4ce62a6e4b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tifffile\n",
      "  Downloading tifffile-2025.6.11-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from tifffile) (2.3.1)\n",
      "Downloading tifffile-2025.6.11-py3-none-any.whl (230 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.8/230.8 kB\u001b[0m \u001b[31m283.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tifffile\n",
      "Successfully installed tifffile-2025.6.11\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6031383c-6318-4d2e-86f9-6ace15e99fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tifffile\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 250)\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import shape\n",
    "import glob\n",
    "from shapely.geometry import box\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "import json\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "red_to_green = LinearSegmentedColormap.from_list(\"RedGreen\", [\"red\", \"green\"])\n",
    "\n",
    "from src.train import train\n",
    "from src.hyperparams import HyperParams, open_from_yaml\n",
    "from src.data import *\n",
    "# from src.validate import return_metrics_all_folds, visualize_single_model, cross_validation, visualize_s2_concat_bands_path\n",
    "from utils import visualize_s2_concat_bands_path, visualize_s1_path, scale_img, visualize_ls_concat_bands_path, visualize_cb_mux_path, visualize_cb_wpm_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e7410b0-57cb-427b-b184-76ea8bf0c176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_cloud_nodata = 75\n",
    "# s2_valid_data_events = []\n",
    "# for event_id in events_sorted_by_area[:]:\n",
    "#     # rand_int = np.random.randint(len(events_sorted_by_area))\n",
    "#     # rand_int = k\n",
    "#     # event_id = events_sorted_by_area[rand_int]\n",
    "#     # s2_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel2/*L2A*.tif\") if not \"ndvi\" in k and not \"ndwi\" in k])\n",
    "#     s1_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel1/*.tif\") if not \"_nrpb\" in k and not \"_rfdi\" in k and not \"_rvi\" in k and not \"_vv_vh_ratio\" in k])\n",
    "#     if len(s1_paths) == 0:\n",
    "#         continue\n",
    "#     # s2_16d_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel2/*16D*.tif\") if not \"ndvi\" in k and not \"ndwi\" in k])\n",
    "#     # ls_paths = sorted([k for k in glob.glob(f\"data/{event_id}/landsat/*_L2SP_*.tif\") if not \"_nbr\" in k and not \"_ndwi\" in k and not \"_ndvi\" in k])\n",
    "#     # cb_wpm_paths = sorted([k for k in glob.glob(f\"data/{event_id}/cbers4a/*_WPM_*.tif\") if not \"_ndvi\" in k])\n",
    "#     # cb_mux_paths = sorted([k for k in glob.glob(f\"data/{event_id}/cbers4a/*_MUX_*.tif\") if not \"_ndvi\" in k])\n",
    "#     # print(f\"{event_id}: {len(s2_paths):2} S2 L2A, {len(s2_16d_paths):2} S2 16D,{len(s1_paths):2} S1, {len(ls_paths):2} LS L2SP, {len(cb_wpm_paths):2} CB WPM, {len(cb_mux_paths):2} CB MUX\")\n",
    "#     # paths = s2_paths + s1_paths + ls_paths + cb_wpm_paths + cb_mux_paths + s2_16d_paths\n",
    "#     # if len(paths) == 0:\n",
    "#     #     continue\n",
    "#     # continue\n",
    "#     mask_path = glob.glob(f\"data/{event_id}/*_mask.tif\")[0]\n",
    "#     mask_date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "#     mask_date_str = f\"{mask_date[:4]}-{mask_date[4:6]}-{mask_date[6:8]}\"\n",
    "#     mask_date = datetime.strptime(mask_date, \"%Y%m%d\")\n",
    "#     # print(mask_date)\n",
    "    \n",
    "#     date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "#     # Sort by the date right before the .tif\n",
    "#     paths = sorted(s1_paths, key=lambda p: datetime.strptime(p.split('_')[-1].split('.')[0], \"%Y%m%d\"))\n",
    "#     # break\n",
    "#     paths_after, paths_before = [], []\n",
    "#     for path in paths:\n",
    "#         date = datetime.strptime(path.split('_')[-1].split('.')[0], \"%Y%m%d\")\n",
    "#         if date > mask_date:\n",
    "#             paths_after.append(path)\n",
    "#         else:\n",
    "#             paths_before.append(path)\n",
    "#     print(f\"{event_id}: {len(paths):2} S1 paths, {len(paths_before):2} before, {len(paths_after):2} after ({mask_date})\")\n",
    "#     if len(paths_before) > 0 and len(paths_after) > 0:\n",
    "#         s2_valid_data_events.append(event_id)\n",
    "# len(s2_valid_data_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f2051-5469-44da-b857-2438fb8bdd4a",
   "metadata": {},
   "source": [
    "# S2 single sensor, single before/after image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a3ae77-1192-43da-bf7f-30ee9bef3476",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create cross validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "084c5c82-c50c-4292-9d11-ecdbf75ec1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = gpd.read_file(\"data/mapbiomas_alerts.geojson\")\n",
    "events_sorted_by_area = df.sort_values(\"areaHa\", ascending=False)[\"alertCode\"].tolist()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17e98212-b27f-4a21-b5db-5b22a0f8fcd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1389549: 16 S2 L2A paths, 10 before,  6 after, (2025-05-01 00:00:00)\n",
      " --> min nodata/clouds before:   8.5%, after:   0.0%\n",
      "1389572: 15 S2 L2A paths, 10 before,  5 after, (2025-05-19 00:00:00)\n",
      " --> min nodata/clouds before: 100.0%, after: 100.0%\n",
      "1389158:  1 S2 L2A paths,  1 before,  0 after, (2025-05-08 00:00:00)\n",
      "1389503:  8 S2 L2A paths,  5 before,  3 after, (2025-05-16 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1389968: 13 S2 L2A paths, 10 before,  3 after, (2025-05-15 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1389833:  2 S2 L2A paths,  2 before,  0 after, (2025-05-01 00:00:00)\n",
      "1387972: 18 S2 L2A paths, 12 before,  6 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1388042: 13 S2 L2A paths, 11 before,  2 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1389517: 17 S2 L2A paths,  9 before,  8 after, (2025-05-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1387954:  9 S2 L2A paths,  5 before,  4 after, (2025-03-01 00:00:00)\n",
      " --> min nodata/clouds before: 100.0%, after: 100.0%\n",
      "1388065: 13 S2 L2A paths, 11 before,  2 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before:   2.7%, after: 100.0%\n",
      "1389891: 21 S2 L2A paths, 12 before,  9 after, (2025-05-09 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1388607: 10 S2 L2A paths,  7 before,  3 after, (2025-05-01 00:00:00)\n",
      " --> min nodata/clouds before:  85.4%, after:  85.5%\n",
      "1389526: 11 S2 L2A paths,  6 before,  5 after, (2025-05-08 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.1%\n",
      "1388066: 11 S2 L2A paths,  7 before,  4 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before: 100.0%, after:  80.3%\n",
      "1388048: 16 S2 L2A paths, 10 before,  6 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.2%, after:   1.4%\n",
      "1388078: 14 S2 L2A paths, 12 before,  2 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after: 100.0%\n",
      "1388022:  6 S2 L2A paths,  3 before,  3 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before: 100.0%, after:   0.3%\n",
      "1389165: 12 S2 L2A paths,  6 before,  6 after, (2025-05-21 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1388023:  5 S2 L2A paths,  0 before,  5 after, (2025-02-01 00:00:00)\n",
      "1389786: 17 S2 L2A paths, 13 before,  4 after, (2025-05-21 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1388039: 12 S2 L2A paths,  9 before,  3 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1387927: 13 S2 L2A paths,  9 before,  4 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after: 100.0%\n",
      "1389494: 22 S2 L2A paths, 14 before,  8 after, (2025-05-26 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1390023: 11 S2 L2A paths,  8 before,  3 after, (2025-05-27 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1387942:  2 S2 L2A paths,  2 before,  0 after, (2025-01-01 00:00:00)\n",
      "1387997: 13 S2 L2A paths, 10 before,  3 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after: 100.0%\n",
      "1388043: 10 S2 L2A paths,  6 before,  4 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before: 100.0%, after: 100.0%\n",
      "1388015:  7 S2 L2A paths,  4 before,  3 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.3%, after:   0.0%\n",
      "1387993: 18 S2 L2A paths, 10 before,  8 after, (2025-03-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after: 100.0%\n",
      "1388031:  4 S2 L2A paths,  4 before,  0 after, (2025-01-01 00:00:00)\n",
      "1388045: 13 S2 L2A paths, 11 before,  2 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after: 100.0%\n",
      "1387989:  9 S2 L2A paths,  5 before,  4 after, (2025-03-01 00:00:00)\n",
      " --> min nodata/clouds before:   1.0%, after: 100.0%\n",
      "1387981: 17 S2 L2A paths, 12 before,  5 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before: 100.0%, after: 100.0%\n",
      "1388071: 17 S2 L2A paths, 11 before,  6 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1387944: 12 S2 L2A paths,  9 before,  3 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1387970:  4 S2 L2A paths,  3 before,  1 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after: 100.0%\n",
      "1388038:  3 S2 L2A paths,  1 before,  2 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.8%, after:   0.9%\n",
      "1388074: 13 S2 L2A paths, 11 before,  2 after, (2025-03-01 00:00:00)\n",
      " --> min nodata/clouds before:  11.4%, after: 100.0%\n",
      "1388007: 11 S2 L2A paths,  6 before,  5 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1388001: 13 S2 L2A paths, 11 before,  2 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.2%\n",
      "1387923:  3 S2 L2A paths,  2 before,  1 after, (2025-03-01 00:00:00)\n",
      " --> min nodata/clouds before: 100.0%, after: 100.0%\n",
      "1388059: 26 S2 L2A paths, 22 before,  4 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1388037: 11 S2 L2A paths,  9 before,  2 after, (2025-03-01 00:00:00)\n",
      " --> min nodata/clouds before: 100.0%, after: 100.0%\n",
      "1388051: 12 S2 L2A paths,  8 before,  4 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1387986: 12 S2 L2A paths, 11 before,  1 after, (2025-03-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after: 100.0%\n",
      "1387926: 13 S2 L2A paths, 11 before,  2 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after: 100.0%\n",
      "1388046: 15 S2 L2A paths, 10 before,  5 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.4%\n",
      "1388076: 12 S2 L2A paths, 10 before,  2 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after: 100.0%\n",
      "1388077: 11 S2 L2A paths,  7 before,  4 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1387951: 12 S2 L2A paths, 12 before,  0 after, (2025-01-01 00:00:00)\n",
      "1388016: 13 S2 L2A paths, 11 before,  2 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.1%\n",
      "1388067: 15 S2 L2A paths, 11 before,  4 after, (2025-03-01 00:00:00)\n",
      " --> min nodata/clouds before:  99.9%, after: 100.0%\n",
      "1388061: 17 S2 L2A paths, 11 before,  6 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1387971:  1 S2 L2A paths,  1 before,  0 after, (2025-01-01 00:00:00)\n",
      "1388035:  5 S2 L2A paths,  0 before,  5 after, (2025-02-01 00:00:00)\n",
      "1387928:  1 S2 L2A paths,  0 before,  1 after, (2025-02-01 00:00:00)\n",
      "1388079: 14 S2 L2A paths, 11 before,  3 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   4.7%, after: 100.0%\n",
      "1388028:  8 S2 L2A paths,  7 before,  1 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1387950:  2 S2 L2A paths,  0 before,  2 after, (2025-02-01 00:00:00)\n",
      "1389780: 14 S2 L2A paths,  9 before,  5 after, (2025-05-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.1%, after:   0.0%\n",
      "1388049: 14 S2 L2A paths, 11 before,  3 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after: 100.0%\n",
      "1389443: 13 S2 L2A paths,  7 before,  6 after, (2025-05-28 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1388080: 14 S2 L2A paths, 10 before,  4 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1387925:  9 S2 L2A paths,  6 before,  3 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.9%, after:   0.5%\n",
      "1388026:  6 S2 L2A paths,  2 before,  4 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before: 100.0%, after:   0.0%\n",
      "1388057: 14 S2 L2A paths, 12 before,  2 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after: 100.0%\n",
      "1388047: 23 S2 L2A paths, 17 before,  6 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.4%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_before_, paths_after_ = [], []\n",
    "clouds_nodata_before_, clouds_nodata_after_ = [], []\n",
    "s2_valid_data_events = []\n",
    "for event_id in events_sorted_by_area[:]:\n",
    "    s2_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel2/*L2A*.tif\") if not \"ndvi\" in k and not \"ndwi\" in k])\n",
    "    if len(s2_paths) == 0:\n",
    "        continue\n",
    "    mask_path = glob.glob(f\"data/{event_id}/*_mask.tif\")[0]\n",
    "    mask_date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "    mask_date_str = f\"{mask_date[:4]}-{mask_date[4:6]}-{mask_date[6:8]}\"\n",
    "    mask_date = datetime.strptime(mask_date, \"%Y%m%d\")\n",
    "    # print(mask_date)\n",
    "    \n",
    "    date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "    # Sort by the date right before the .tif\n",
    "    paths = sorted(s2_paths, key=lambda p: datetime.strptime(p.split('_')[-1].split('.')[0], \"%Y%m%d\"))\n",
    "    # break\n",
    "    paths_after, paths_before = [], []\n",
    "    clouds_nodata_after, clouds_nodata_before = [], []\n",
    "    for path in paths:\n",
    "        # what_to_do_string = \"SKIP\"\n",
    "        img_orig = tifffile.imread(path)\n",
    "        # img_orig = img_orig.astype(np.float32)\n",
    "        # img_orig = img_orig - 1000\n",
    "        # img_orig[img_orig < 0] = 0\n",
    "        clouds_perc = 100 * (img_orig[:, :, 2] > 3400).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "        nodata_perc = 100 * (img_orig[:, :, 2] <= 1000).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "        date = path.split(\"/\")[-1].split('.')[0].split('_')[-1]\n",
    "        date_str = f\"{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "            \n",
    "        date = datetime.strptime(path.split('_')[-1].split('.')[0], \"%Y%m%d\")\n",
    "        if date > mask_date:\n",
    "            paths_after.append(path)\n",
    "            clouds_nodata_after.append(clouds_perc + nodata_perc)\n",
    "        else:\n",
    "            paths_before.append(path)\n",
    "            clouds_nodata_before.append(clouds_perc + nodata_perc)\n",
    "    print(f\"{event_id}: {len(paths):2} S2 L2A paths, {len(paths_before):2} before, {len(paths_after):2} after, ({mask_date})\")\n",
    "    if len(clouds_nodata_before) > 0 and len(clouds_nodata_after) > 0:\n",
    "        print(f\" --> min nodata/clouds before: {min(clouds_nodata_before):5.1f}%, after: {min(clouds_nodata_after):5.1f}%\")\n",
    "    if len(paths_before) > 0 and len(paths_after) > 0:\n",
    "        s2_valid_data_events.append(event_id)\n",
    "        paths_after_.append(paths_after)\n",
    "        clouds_nodata_after_.append(clouds_nodata_after)\n",
    "        paths_before_.append(paths_before)\n",
    "        clouds_nodata_before_.append(clouds_nodata_before)\n",
    "len(s2_valid_data_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372e464b-74d2-41c3-ae73-b4e150ca2896",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"event_id\": s2_valid_data_events, \n",
    "              \"paths_before\": paths_before_, \"clouds_nodata_before\": clouds_nodata_before_,\n",
    "                  \"paths_after\": paths_after_, \"clouds_nodata_after\": clouds_nodata_after_})\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c635b52-cbd9-488f-a7fa-9695178f2cc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def best_before_path(row):\n",
    "    # Take the earliest date with clouds < 10%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"], row[\"paths_before\"]):\n",
    "        if clouds_nodata < 10:\n",
    "            return path\n",
    "    # Take the earliest date with clouds < 30%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"], row[\"paths_before\"]):\n",
    "        if clouds_nodata < 30:\n",
    "            return path\n",
    "    # Take the earliest date with clouds < 100%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"], row[\"paths_before\"]):\n",
    "        if clouds_nodata < 100:\n",
    "            return path\n",
    "    # Else return the oldest path\n",
    "    return row[\"paths_before\"][0]\n",
    "\n",
    "def best_after_path(row):\n",
    "    # Take the latest date with clouds < 10%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"], row[\"paths_after\"]):\n",
    "        if clouds_nodata < 10:\n",
    "            return path\n",
    "    # Take the latest date with clouds < 30%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"], row[\"paths_after\"]):\n",
    "        if clouds_nodata < 30:\n",
    "            return path\n",
    "    # Take the latest date with clouds < 100%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"], row[\"paths_after\"]):\n",
    "        if clouds_nodata < 100:\n",
    "            return path\n",
    "    # Else return the newest path\n",
    "    return row[\"paths_after\"][-1]\n",
    "    \n",
    "df[\"path_best_before\"] = df[[\"paths_before\", \"clouds_nodata_before\"]].apply(lambda row: best_before_path(row), axis=1)\n",
    "df[\"path_best_after\"] = df[[\"paths_after\", \"clouds_nodata_after\"]].apply(lambda row: best_after_path(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a09e2d1-2074-4a98-87b2-425a9c78e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_paths = []\n",
    "deforest_pxs = []\n",
    "for event_id in df[\"event_id\"].tolist():\n",
    "    label_path = glob.glob(f\"data/{event_id}/*_mask.tif\")[0]\n",
    "    label_paths.append(label_path)\n",
    "    label = tifffile.imread(mask_path)\n",
    "    deforest_pxs.append((label == 255).sum())\n",
    "df[\"deforest_pxs\"] = deforest_pxs\n",
    "df[\"label_path\"] = label_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe615dfe-b37b-46b2-b50c-bfb83d7a1765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['event_id', 'paths_before', 'clouds_nodata_before', 'paths_after',\n",
       "       'clouds_nodata_after', 'path_best_before', 'path_best_after',\n",
       "       'deforest_pxs', 'label_path'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7c93318-6ac9-4034-9261-a51919cf556b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 10)\n"
     ]
    }
   ],
   "source": [
    "seed = 22222\n",
    "while True:\n",
    "    np.random.seed(seed)\n",
    "    folds = [0, 1, 2, 3, 4] * (len(df) // 5 + 1) #np.random.choice([0, 1, 2, 3, 4], size=len(df))\n",
    "    np.random.shuffle(folds)\n",
    "    df[\"fold\"] = folds[:len(df)]\n",
    "    # print(df[\"fold\"].value_counts())\n",
    "    std_counts = df.groupby(\"fold\")[\"deforest_pxs\"].sum().std().item()\n",
    "    # print(f'Seed {seed}: {df.groupby(\"fold\")[\"deforest_pxs\"].sum()}, Std = {std_counts:.0f}')\n",
    "    if std_counts < 1500:\n",
    "        break\n",
    "    seed += 1\n",
    "os.makedirs(\"catalogues\", exist_ok=True)\n",
    "df.to_pickle(\"catalogues/2025_08_18_s2_single.pkl\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4195f91-a82b-4519-bba4-99617f91e3cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8460766b-4d40-4d34-a0d7-641bd4092854",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a7a6c7-5621-41b6-a33a-e641d547c262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18-Aug-25 13:44:27 - Training with 1 GPUS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Augmentations: [RandomCrop(p=1.0, border_mode=0, fill=0.0, fill_mask=0.0, height=400, pad_if_needed=False, pad_position='center', width=400)]\n",
      "Train Image Data Augmentations: []\n",
      "####################################################################################################\n",
      "####################################################################################################\n",
      "Fold 0 --> Train: 46, Val: 12\n",
      "####################################################################################################\n",
      "We want to train 6 batches in each epoch, but our current dataset is only 2 batches long. Doubling dataset size\n",
      "We want to train 6 batches in each epoch, but our current dataset is only 5 batches long. Doubling dataset size\n",
      "Last 5 sample weights: [1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 158.00 MiB. GPU 0 has a total capacity of 11.61 GiB of which 38.31 MiB is free. Including non-PyTorch memory, this process has 11.55 GiB memory in use. Of the allocated memory 11.21 GiB is allocated by PyTorch, and 170.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     55\u001b[39m train_dataset, train_loader, val_dataset, val_loader = get_dataloaders(hps)\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# continue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m best_metric, best_metric_epoch = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Capacity_takehome/src/train.py:148\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(hps, train_loader, val_loader)\u001b[39m\n\u001b[32m    146\u001b[39m     scaler.update()\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     preds = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m     loss = loss_func(preds, targets)\n\u001b[32m    150\u001b[39m     loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Capacity_takehome/src/model.py:250\u001b[39m, in \u001b[36mSiameseUNetShared.forward\u001b[39m\u001b[34m(self, x0, x1)\u001b[39m\n\u001b[32m    248\u001b[39m fb = \u001b[38;5;28mself\u001b[39m.tfb(b0, b1)\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mf1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf4\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Capacity_takehome/src/model.py:125\u001b[39m, in \u001b[36mUNetDecoder.forward\u001b[39m\u001b[34m(self, b, f_skips)\u001b[39m\n\u001b[32m    123\u001b[39m x = \u001b[38;5;28mself\u001b[39m.up1(b, f4)\n\u001b[32m    124\u001b[39m x = \u001b[38;5;28mself\u001b[39m.up2(x, f3)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m x = \u001b[38;5;28mself\u001b[39m.up4(x, f1)\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.outc(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Capacity_takehome/src/model.py:66\u001b[39m, in \u001b[36mUp.forward\u001b[39m\u001b[34m(self, x, skip)\u001b[39m\n\u001b[32m     64\u001b[39m x = F.pad(x, [diffX // \u001b[32m2\u001b[39m, diffX - diffX // \u001b[32m2\u001b[39m, diffY // \u001b[32m2\u001b[39m, diffY - diffY // \u001b[32m2\u001b[39m])\n\u001b[32m     65\u001b[39m x = torch.cat([skip, x], dim=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Capacity_takehome/src/model.py:36\u001b[39m, in \u001b[36mDoubleConv.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:193\u001b[39m, in \u001b[36m_BatchNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    186\u001b[39m     bn_training = (\u001b[38;5;28mself\u001b[39m.running_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.running_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    188\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_mean\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/functional.py:2822\u001b[39m, in \u001b[36mbatch_norm\u001b[39m\u001b[34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[39m\n\u001b[32m   2819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[32m   2820\u001b[39m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m.size())\n\u001b[32m-> \u001b[39m\u001b[32m2822\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2823\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2830\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2832\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 158.00 MiB. GPU 0 has a total capacity of 11.61 GiB of which 38.31 MiB is free. Including non-PyTorch memory, this process has 11.55 GiB memory in use. Of the allocated memory 11.21 GiB is allocated by PyTorch, and 170.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "exp_nb = \"S2_single_0-1\"\n",
    "path = \"catalogues/2025_08_18_s2_single.pkl\"\n",
    "hps_dict = {\n",
    "    ############\n",
    "    # Data\n",
    "    ############\n",
    "    \"df_path\": path,\n",
    "\n",
    "    ############\n",
    "    # Training\n",
    "    ############\n",
    "\n",
    "    ## Experiment Setup\n",
    "    \"name\": f\"Exp{exp_nb}\",\n",
    "\n",
    "    ## Model\n",
    "    \"num_classes\": 1,\n",
    "    \"input_channel\": 13,\n",
    "    \"backbone\": \"timm_efficientnet_b1\",\n",
    "    \"pretrained\": 1,\n",
    "    \"model\": \"unetplusplus\",\n",
    "\n",
    "    # Training Setup\n",
    "#     \"resume\": \"trained_models/Exp7-4/fold_0/2022-02-05_23-13-25/best_metric_18_0.9768.pt\",\n",
    "    \"print_freq\": 10,\n",
    "    \"use_fp16\": 0,\n",
    "    \"patience\": 5, #5,\n",
    "    \"epoch_start_scheduler\": 5,\n",
    "\n",
    "    # Optimizer\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 0.0,\n",
    "\n",
    "    ## Data Augmentation on CPU\n",
    "    \"train_crop_size\": 256,\n",
    "    \"train_batch_size\": 16, #32\n",
    "    \"cutmix_alpha\": 0,\n",
    "    \"da_brightness_magnitude\": 0.0,\n",
    "    \"da_contrast_magnitude\": 0.0,\n",
    "\n",
    "    # Data Augmentation on GPU\n",
    "    \"gpu_da_params\": [0.25],\n",
    "    \n",
    "    ### Loss, Metric\n",
    "    \"alpha\": 0.5,\n",
    "#     \"loss\": \"lovasz\",\n",
    "           }\n",
    "\n",
    "for fold_nb in range(5):\n",
    "    hps = HyperParams(**hps_dict)\n",
    "    hps.fold_nb = fold_nb\n",
    "    num_batches = 100 // (hps.train_batch_size * torch.cuda.device_count())\n",
    "    # num_batches = 10\n",
    "    hps.num_batches = num_batches\n",
    "    train_dataset, train_loader, val_dataset, val_loader = get_dataloaders(hps)\n",
    "    # continue\n",
    "\n",
    "    best_metric, best_metric_epoch = train(hps, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cb4d52-5132-4b04-95dd-345025a4b81a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in range(100):\n",
    "    print(np.unique(train_dataset[k][\"mask\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa53266-8ef5-4105-8743-5b615dc360e1",
   "metadata": {},
   "source": [
    "# S1 single sensor, single before/after image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910ba1cd-4c2d-4328-ba05-e8f574a0133a",
   "metadata": {},
   "source": [
    "## Create cross validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa5b11-5b52-4802-a501-3dea8b501b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_file(\"data/mapbiomas_alerts.geojson\")\n",
    "events_sorted_by_area = df.sort_values(\"areaHa\", ascending=False)[\"alertCode\"].tolist()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb62d90-d70d-4586-ac99-a8fce6ee27ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paths_before_, paths_after_ = [], []\n",
    "nodata_before_, nodata_after_ = [], []\n",
    "s1_valid_data_events = []\n",
    "for event_id in events_sorted_by_area[:]:\n",
    "    s1_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel1/*.tif\") if not \"_nrpb\" in k and not \"_rfdi\" in k and not \"_rvi\" in k and not \"_vv_vh_ratio\" in k])\n",
    "    if len(s1_paths) == 0:\n",
    "        continue\n",
    "    mask_path = glob.glob(f\"data/{event_id}/*_mask.tif\")[0]\n",
    "    mask_date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "    mask_date_str = f\"{mask_date[:4]}-{mask_date[4:6]}-{mask_date[6:8]}\"\n",
    "    mask_date = datetime.strptime(mask_date, \"%Y%m%d\")\n",
    "    # print(mask_date)\n",
    "    \n",
    "    date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "    # Sort by the date right before the .tif\n",
    "    paths = sorted(s1_paths, key=lambda p: datetime.strptime(p.split('_')[-1].split('.')[0], \"%Y%m%d\"))\n",
    "    # break\n",
    "    paths_after, paths_before = [], []\n",
    "    nodata_after, nodata_before = [], []\n",
    "    for path in paths:\n",
    "        # what_to_do_string = \"SKIP\"\n",
    "        img_orig = tifffile.imread(path)\n",
    "        nodata_perc = 100 * (img_orig[:, :, 0] == 0).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "        date = path.split(\"/\")[-1].split('.')[0].split('_')[-1]\n",
    "        date_str = f\"{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "            \n",
    "        date = datetime.strptime(path.split('_')[-1].split('.')[0], \"%Y%m%d\")\n",
    "        if date > mask_date:\n",
    "            paths_after.append(path)\n",
    "            nodata_after.append(nodata_perc)\n",
    "        else:\n",
    "            paths_before.append(path)\n",
    "            nodata_before.append(nodata_perc)\n",
    "    print(f\"{event_id}: {len(paths):2} S1 paths, {len(paths_before):2} before, {len(paths_after):2} after, ({mask_date})\")\n",
    "    if len(nodata_before) > 0 and len(nodata_after) > 0:\n",
    "        print(f\" --> min nodata before: {min(nodata_before):5.1f}%, after: {min(nodata_after):5.1f}%\")\n",
    "    if len(paths_before) > 0 and len(paths_after) > 0:\n",
    "        s1_valid_data_events.append(event_id)\n",
    "        paths_after_.append(paths_after)\n",
    "        nodata_after_.append(nodata_after)\n",
    "        paths_before_.append(paths_before)\n",
    "        nodata_before_.append(nodata_before)\n",
    "len(s1_valid_data_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caa8b12-767e-4dc2-81e8-9d8fcda99338",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"event_id\": s1_valid_data_events, \n",
    "              \"paths_before\": paths_before_, \"nodata_before\": nodata_before_,\n",
    "                  \"paths_after\": paths_after_, \"nodata_after\": nodata_after_})\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a219863-2dcd-4f67-89f8-d36f3c09a89f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def best_before_path(row):\n",
    "    # Take the earliest date with clouds < 10%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"], row[\"paths_before\"]):\n",
    "        if clouds_nodata < 10:\n",
    "            return path\n",
    "    # Take the earliest date with clouds < 30%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"], row[\"paths_before\"]):\n",
    "        if clouds_nodata < 30:\n",
    "            return path\n",
    "    # Take the earliest date with clouds < 100%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"], row[\"paths_before\"]):\n",
    "        if clouds_nodata < 100:\n",
    "            return path\n",
    "    # Else return the oldest path\n",
    "    return row[\"paths_before\"][0]\n",
    "\n",
    "def best_after_path(row):\n",
    "    # Take the latest date with clouds < 10%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"], row[\"paths_after\"]):\n",
    "        if clouds_nodata < 10:\n",
    "            return path\n",
    "    # Take the latest date with clouds < 30%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"], row[\"paths_after\"]):\n",
    "        if clouds_nodata < 30:\n",
    "            return path\n",
    "    # Take the latest date with clouds < 100%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"], row[\"paths_after\"]):\n",
    "        if clouds_nodata < 100:\n",
    "            return path\n",
    "    # Else return the newest path\n",
    "    return row[\"paths_after\"][-1]\n",
    "    \n",
    "df[\"path_best_before\"] = df[[\"paths_before\", \"clouds_nodata_before\"]].apply(lambda row: best_before_path(row), axis=1)\n",
    "df[\"path_best_after\"] = df[[\"paths_after\", \"clouds_nodata_after\"]].apply(lambda row: best_after_path(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a4fa86-6d7f-4dde-90b7-eb80a5684103",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_paths = []\n",
    "deforest_pxs = []\n",
    "for event_id in df[\"event_id\"].tolist():\n",
    "    label_path = glob.glob(f\"data/{event_id}/*_mask.tif\")[0]\n",
    "    label_paths.append(label_path)\n",
    "    label = tifffile.imread(mask_path)\n",
    "    deforest_pxs.append((label == 255).sum())\n",
    "df[\"deforest_pxs\"] = deforest_pxs\n",
    "df[\"label_path\"] = label_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51eea01-448d-4241-8d5e-9e890d868e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c6a7bd-d94d-4b21-a293-2dcbc690bc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 22222\n",
    "while True:\n",
    "    np.random.seed(seed)\n",
    "    folds = [0, 1, 2, 3, 4] * (len(df) // 5 + 1) #np.random.choice([0, 1, 2, 3, 4], size=len(df))\n",
    "    np.random.shuffle(folds)\n",
    "    df[\"fold\"] = folds[:len(df)]\n",
    "    # print(df[\"fold\"].value_counts())\n",
    "    std_counts = df.groupby(\"fold\")[\"deforest_pxs\"].sum().std().item()\n",
    "    # print(f'Seed {seed}: {df.groupby(\"fold\")[\"deforest_pxs\"].sum()}, Std = {std_counts:.0f}')\n",
    "    if std_counts < 1500:\n",
    "        break\n",
    "    seed += 1\n",
    "os.makedirs(\"catalogues\", exist_ok=True)\n",
    "df.to_pickle(\"catalogues/2025_08_18_s2_single.pkl\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c94dc7-aacd-4b56-a5a4-156529433f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499636dc-3fff-490b-85f6-208c56be3b49",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dcdbd8-54a7-404a-bc69-aacd6737e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_nb = \"S2_single_0-2\"\n",
    "path = \"catalogues/2025_08_18_s2_single.pkl\"\n",
    "hps_dict = {\n",
    "    ############\n",
    "    # Data\n",
    "    ############\n",
    "    \"df_path\": path,\n",
    "\n",
    "    ############\n",
    "    # Training\n",
    "    ############\n",
    "\n",
    "    ## Experiment Setup\n",
    "    \"name\": f\"Exp{exp_nb}\",\n",
    "\n",
    "    ## Model\n",
    "    \"num_classes\": 1,\n",
    "    \"input_channel\": 13,\n",
    "    \"backbone\": \"timm_efficientnet_b1\",\n",
    "    \"pretrained\": 1,\n",
    "    \"model\": \"unetplusplus\",\n",
    "\n",
    "    # Training Setup\n",
    "#     \"resume\": \"trained_models/Exp7-4/fold_0/2022-02-05_23-13-25/best_metric_18_0.9768.pt\",\n",
    "    \"print_freq\": 1,\n",
    "    \"use_fp16\": 0,\n",
    "    \"patience\": 2, #5,\n",
    "    \"epoch_start_scheduler\": 5,\n",
    "\n",
    "    # Optimizer\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 0.0,\n",
    "\n",
    "    ## Data Augmentation on CPU\n",
    "    \"train_crop_size\": 400,\n",
    "    \"train_batch_size\": 4, #32\n",
    "    \"cutmix_alpha\": 0,\n",
    "    \"da_brightness_magnitude\": 0.0,\n",
    "    \"da_contrast_magnitude\": 0.0,\n",
    "\n",
    "    # Data Augmentation on GPU\n",
    "    \"gpu_da_params\": [0.25],\n",
    "    \n",
    "    ### Loss, Metric\n",
    "    \"alpha\": 0.5,\n",
    "#     \"loss\": \"lovasz\",\n",
    "           }\n",
    "\n",
    "for fold_nb in range(5):\n",
    "    hps = HyperParams(**hps_dict)\n",
    "    hps.fold_nb = fold_nb\n",
    "    # num_batches = 50 // (hps.train_batch_size * torch.cuda.device_count())\n",
    "    num_batches = 10\n",
    "    hps.num_batches = num_batches\n",
    "    train_dataset, train_loader, val_dataset, val_loader = get_dataloaders(hps)\n",
    "    # continue\n",
    "\n",
    "    best_metric, best_metric_epoch = train(hps, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715840f6-c2b7-4661-a4ee-b939f4a96843",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in range(100):\n",
    "    print(np.unique(train_dataset[k][\"mask\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3b10e1-552f-4661-95c5-2eb0daebe408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
