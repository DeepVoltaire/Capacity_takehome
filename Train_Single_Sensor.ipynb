{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6031383c-6318-4d2e-86f9-6ace15e99fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tifffile\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 250)\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import shape\n",
    "import glob\n",
    "from shapely.geometry import box\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "import json\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "red_to_green = LinearSegmentedColormap.from_list(\"RedGreen\", [\"red\", \"green\"])\n",
    "\n",
    "from src.train import train\n",
    "from src.hyperparams import HyperParams, open_from_yaml\n",
    "from src.data import *\n",
    "# from src.validate import return_metrics_all_folds, visualize_single_model, cross_validation, visualize_s2_concat_bands_path\n",
    "from src.visualize import visualize_s2_concat_bands_path, visualize_s1_path, scale_img, visualize_ls_concat_bands_path, visualize_cb_mux_path, visualize_cb_wpm_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7410b0-57cb-427b-b184-76ea8bf0c176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_cloud_nodata = 75\n",
    "# s2_valid_data_events = []\n",
    "# for event_id in events_sorted_by_area[:]:\n",
    "#     # rand_int = np.random.randint(len(events_sorted_by_area))\n",
    "#     # rand_int = k\n",
    "#     # event_id = events_sorted_by_area[rand_int]\n",
    "#     # s2_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel2/*L2A*.tif\") if not \"ndvi\" in k and not \"ndwi\" in k])\n",
    "#     s1_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel1/*.tif\") if not \"_nrpb\" in k and not \"_rfdi\" in k and not \"_rvi\" in k and not \"_vv_vh_ratio\" in k])\n",
    "#     if len(s1_paths) == 0:\n",
    "#         continue\n",
    "#     # s2_16d_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel2/*16D*.tif\") if not \"ndvi\" in k and not \"ndwi\" in k])\n",
    "#     # ls_paths = sorted([k for k in glob.glob(f\"data/{event_id}/landsat/*_L2SP_*.tif\") if not \"_nbr\" in k and not \"_ndwi\" in k and not \"_ndvi\" in k])\n",
    "#     # cb_wpm_paths = sorted([k for k in glob.glob(f\"data/{event_id}/cbers4a/*_WPM_*.tif\") if not \"_ndvi\" in k])\n",
    "#     # cb_mux_paths = sorted([k for k in glob.glob(f\"data/{event_id}/cbers4a/*_MUX_*.tif\") if not \"_ndvi\" in k])\n",
    "#     # print(f\"{event_id}: {len(s2_paths):2} S2 L2A, {len(s2_16d_paths):2} S2 16D,{len(s1_paths):2} S1, {len(ls_paths):2} LS L2SP, {len(cb_wpm_paths):2} CB WPM, {len(cb_mux_paths):2} CB MUX\")\n",
    "#     # paths = s2_paths + s1_paths + ls_paths + cb_wpm_paths + cb_mux_paths + s2_16d_paths\n",
    "#     # if len(paths) == 0:\n",
    "#     #     continue\n",
    "#     # continue\n",
    "#     mask_path = glob.glob(f\"data/{event_id}/*_mask.tif\")[0]\n",
    "#     mask_date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "#     mask_date_str = f\"{mask_date[:4]}-{mask_date[4:6]}-{mask_date[6:8]}\"\n",
    "#     mask_date = datetime.strptime(mask_date, \"%Y%m%d\")\n",
    "#     # print(mask_date)\n",
    "    \n",
    "#     date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "#     # Sort by the date right before the .tif\n",
    "#     paths = sorted(s1_paths, key=lambda p: datetime.strptime(p.split('_')[-1].split('.')[0], \"%Y%m%d\"))\n",
    "#     # break\n",
    "#     paths_after, paths_before = [], []\n",
    "#     for path in paths:\n",
    "#         date = datetime.strptime(path.split('_')[-1].split('.')[0], \"%Y%m%d\")\n",
    "#         if date > mask_date:\n",
    "#             paths_after.append(path)\n",
    "#         else:\n",
    "#             paths_before.append(path)\n",
    "#     print(f\"{event_id}: {len(paths):2} S1 paths, {len(paths_before):2} before, {len(paths_after):2} after ({mask_date})\")\n",
    "#     if len(paths_before) > 0 and len(paths_after) > 0:\n",
    "#         s2_valid_data_events.append(event_id)\n",
    "# len(s2_valid_data_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f2051-5469-44da-b857-2438fb8bdd4a",
   "metadata": {},
   "source": [
    "# S2 single sensor, single before/after image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a3ae77-1192-43da-bf7f-30ee9bef3476",
   "metadata": {},
   "source": [
    "## Create cross validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084c5c82-c50c-4292-9d11-ecdbf75ec1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_file(\"data/mapbiomas_alerts.geojson\")\n",
    "events_sorted_by_area = df.sort_values(\"areaHa\", ascending=False)[\"alertCode\"].tolist()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e98212-b27f-4a21-b5db-5b22a0f8fcd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paths_before_, paths_after_ = [], []\n",
    "clouds_nodata_before_, clouds_nodata_after_ = [], []\n",
    "label_paths, deforest_pxs = [], []\n",
    "s2_valid_data_events = []\n",
    "seed = 1121\n",
    "seed_count = 0\n",
    "for event_id in events_sorted_by_area[:]:\n",
    "    s2_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel2/*L2A*.tif\") if not \"ndvi\" in k and not \"ndwi\" in k and not \"compressed\" in k])\n",
    "    if len(s2_paths) == 0:\n",
    "        continue\n",
    "    mask_path = glob.glob(f\"data/{event_id}/*_mask.tif\")[0]\n",
    "    mask_date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "    mask_date_str = f\"{mask_date[:4]}-{mask_date[4:6]}-{mask_date[6:8]}\"\n",
    "    mask_date = datetime.strptime(mask_date, \"%Y%m%d\")\n",
    "    # print(mask_date)\n",
    "    \n",
    "    date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "    # Sort by the date right before the .tif\n",
    "    paths = sorted(s2_paths, key=lambda p: datetime.strptime(p.split('_')[-1].split('.')[0], \"%Y%m%d\"))\n",
    "    # break\n",
    "    paths_after, paths_before = [], []\n",
    "    clouds_nodata_after, clouds_nodata_before = [], []\n",
    "    for path in paths:\n",
    "        img_orig = tifffile.imread(path)\n",
    "        clouds_perc = 100 * (img_orig[:, :, 2] > 3400).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "        nodata_perc = 100 * (img_orig[:, :, 2] <= 1000).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "        date = path.split(\"/\")[-1].split('.')[0].split('_')[-1]\n",
    "        date_str = f\"{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "            \n",
    "        date = datetime.strptime(path.split('_')[-1].split('.')[0], \"%Y%m%d\")\n",
    "        if date > mask_date:\n",
    "            paths_after.append(path)\n",
    "            clouds_nodata_after.append(clouds_perc + nodata_perc)\n",
    "        else:\n",
    "            paths_before.append(path)\n",
    "            clouds_nodata_before.append(clouds_perc + nodata_perc)\n",
    "    print(f\"{event_id}: {len(paths):2} S2 L2A paths, {len(paths_before):2} before, {len(paths_after):2} after, ({mask_date})\")\n",
    "    if len(clouds_nodata_before) > 0 and len(clouds_nodata_after) > 0:\n",
    "        print(f\" --> min nodata/clouds before: {min(clouds_nodata_before):5.1f}%, after: {min(clouds_nodata_after):5.1f}%\")\n",
    "    if len(paths_before) > 0 and len(paths_after) > 0:\n",
    "        label_path = glob.glob(f\"data/{event_id}/*_mask.tif\")[0]\n",
    "        label_paths.append(label_path)\n",
    "        label = tifffile.imread(label_path)\n",
    "        deforest_pxs.append((label == 255).sum())\n",
    "        \n",
    "        s2_valid_data_events.append(event_id)\n",
    "        paths_after_.append(paths_after)\n",
    "        clouds_nodata_after_.append(clouds_nodata_after)\n",
    "        paths_before_.append(paths_before)\n",
    "        clouds_nodata_before_.append(clouds_nodata_before)\n",
    "\n",
    "    if len(paths_before) > 2: # Create a new no deforestation example from the before images\n",
    "        before_indices = np.arange(len(paths_before))\n",
    "        np.random.seed(seed + seed_count)\n",
    "        seed_count += 1\n",
    "        np.random.shuffle(before_indices)\n",
    "        sorted_indices_new_example = sorted(before_indices[:2])\n",
    "        sorted_paths_new_example = [path for k, path in enumerate(paths_before) if k in sorted_indices_new_example]\n",
    "        \n",
    "        s2_valid_data_events.append(f\"{event_id}_no_change_before\")\n",
    "        label_paths.append(\"empty\")\n",
    "        deforest_pxs.append(0)\n",
    "        paths_after_.append([sorted_paths_new_example[1]])\n",
    "        clouds_nodata_after_.append([clouds_nodata_before[sorted_indices_new_example[1]]])\n",
    "        paths_before_.append([sorted_paths_new_example[0]])\n",
    "        clouds_nodata_before_.append([clouds_nodata_before[sorted_indices_new_example[0]]])\n",
    "        \n",
    "    if len(paths_after) > 2: # Create a new no deforestation example from the before images\n",
    "        after_indices = np.arange(len(paths_after))\n",
    "        np.random.seed(seed + seed_count)\n",
    "        seed_count += 1\n",
    "        np.random.shuffle(after_indices)\n",
    "        sorted_indices_new_example = sorted(after_indices[:2])\n",
    "        sorted_paths_new_example = [path for k, path in enumerate(paths_after) if k in sorted_indices_new_example]\n",
    "        \n",
    "        s2_valid_data_events.append(f\"{event_id}_no_change_after\")\n",
    "        label_paths.append(\"empty\")\n",
    "        deforest_pxs.append(0)\n",
    "        paths_after_.append([sorted_paths_new_example[1]])\n",
    "        clouds_nodata_after_.append([clouds_nodata_after[sorted_indices_new_example[1]]])\n",
    "        paths_before_.append([sorted_paths_new_example[0]])\n",
    "        clouds_nodata_before_.append([clouds_nodata_after[sorted_indices_new_example[0]]])\n",
    "len(s2_valid_data_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372e464b-74d2-41c3-ae73-b4e150ca2896",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"event_id\": s2_valid_data_events, \n",
    "                   \"deforest_pxs\": deforest_pxs, \"label_path\": label_paths,\n",
    "                  \"paths_before\": paths_before_, \"clouds_nodata_before\": clouds_nodata_before_,\n",
    "                \"paths_after\": paths_after_, \"clouds_nodata_after\": clouds_nodata_after_})\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ff2c89-0aba-4c07-a405-c3214802da77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_before_path(row):\n",
    "    # Take the closest deforestation date with clouds < 10%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"][::-1], row[\"paths_before\"][::-1]):\n",
    "        if clouds_nodata < 10:\n",
    "            return path, clouds_nodata\n",
    "    # Take the closest deforestation date with clouds < 30%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"][::-1], row[\"paths_before\"][::-1]):\n",
    "        if clouds_nodata < 30:\n",
    "            return path, clouds_nodata\n",
    "    # Take the closest deforestation date with clouds < 100%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"][::-1], row[\"paths_before\"][::-1]):\n",
    "        if clouds_nodata < 100:\n",
    "            return path, clouds_nodata\n",
    "    # Else return the closest path\n",
    "    return row[\"paths_before\"][-1], row[\"clouds_nodata_before\"][-1]\n",
    "\n",
    "def best_after_path(row):\n",
    "    # Take the closest deforestation date with clouds < 10%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"], row[\"paths_after\"]):\n",
    "        if clouds_nodata < 10:\n",
    "            return path, clouds_nodata\n",
    "    # Take the closest deforestation  date with clouds < 30%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"], row[\"paths_after\"]):\n",
    "        if clouds_nodata < 30:\n",
    "            return path, clouds_nodata\n",
    "    # Take the closest deforestation date with clouds < 100%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"], row[\"paths_after\"]):\n",
    "        if clouds_nodata < 100:\n",
    "            return path, clouds_nodata\n",
    "    # Else return the closest path\n",
    "    return row[\"paths_after\"][0], row[\"clouds_nodata_after\"][0]\n",
    "    \n",
    "df[\"path_best_before\"] = df[[\"paths_before\", \"clouds_nodata_before\"]].apply(lambda row: best_before_path(row)[0], axis=1)\n",
    "df[\"path_best_after\"] = df[[\"paths_after\", \"clouds_nodata_after\"]].apply(lambda row: best_after_path(row)[0], axis=1)\n",
    "\n",
    "df[\"clouds_nodata_best_before\"] = df[[\"paths_before\", \"clouds_nodata_before\"]].apply(lambda row: best_before_path(row)[1], axis=1)\n",
    "df[\"clouds_nodata_best_after\"] = df[[\"paths_after\", \"clouds_nodata_after\"]].apply(lambda row: best_after_path(row)[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ba6cc-70f0-49fb-97d7-15b424121342",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clouds_nodata_best_before\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0071c73f-9fc9-46b5-91f9-6224bfe361e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clouds_nodata_best_after\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a7881-22b7-4271-ad03-7afcba2cea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df = df[(df[\"label_path\"]==\"empty\") | ((df[\"clouds_nodata_best_before\"] < 50) & (df[\"clouds_nodata_best_after\"] < 50))]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe615dfe-b37b-46b2-b50c-bfb83d7a1765",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c93318-6ac9-4034-9261-a51919cf556b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed = 222221\n",
    "while True:\n",
    "    np.random.seed(seed)\n",
    "    folds = [0, 1, 2, 3, 4] * (len(df) // 5 + 1) #np.random.choice([0, 1, 2, 3, 4], size=len(df))\n",
    "    np.random.shuffle(folds)\n",
    "    df[\"fold\"] = folds[:len(df)]\n",
    "    # print(df[\"fold\"].value_counts())\n",
    "    std_counts = df.groupby(\"fold\")[\"deforest_pxs\"].sum().std().item()\n",
    "    # print(f'Seed {seed}: {df.groupby(\"fold\")[\"deforest_pxs\"].sum()}, Std = {std_counts:.0f}')\n",
    "    if std_counts < 1250:\n",
    "        break\n",
    "    seed += 1\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5a4a62-b8bf-4727-8f17-f70171882e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    output_path = row[\"path_best_before\"].replace(\".tif\", \"_uncompressed.tif\")\n",
    "    if not os.path.exists(output_path):\n",
    "        with rasterio.open(row[\"path_best_before\"]) as src:\n",
    "            profile = src.profile\n",
    "            profile[\"compress\"] = None\n",
    "            with rasterio.open(output_path, \"w\", **profile) as dst:\n",
    "                dst.write(src.read())\n",
    "\n",
    "    output_path = row[\"path_best_after\"].replace(\".tif\", \"_uncompressed.tif\")\n",
    "    if not os.path.exists(output_path):\n",
    "        with rasterio.open(row[\"path_best_after\"]) as src:\n",
    "            profile = src.profile\n",
    "            profile[\"compress\"] = None\n",
    "            with rasterio.open(output_path, \"w\", **profile) as dst:\n",
    "                dst.write(src.read())\n",
    "\n",
    "df[\"path_best_before\"] = df[\"path_best_before\"].apply(lambda x: x.replace(\".tif\", \"_uncompressed.tif\"))\n",
    "df[\"path_best_after\"] = df[\"path_best_after\"].apply(lambda x: x.replace(\".tif\", \"_uncompressed.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7422c6-cdce-4c60-bd96-c92182a0859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"catalogues\", exist_ok=True)\n",
    "df.to_pickle(\"catalogues/2025_08_19_s2_single.pkl\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4195f91-a82b-4519-bba4-99617f91e3cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(\"fold\")[\"deforest_pxs\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8460766b-4d40-4d34-a0d7-641bd4092854",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0774cf-a3a8-4dfb-aae4-9e0a92386ff8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_nb = \"S2_single_3-0\"\n",
    "path = \"catalogues/2025_08_19_s2_single.pkl\"\n",
    "hps_dict = {\n",
    "    ############\n",
    "    # Data\n",
    "    ############\n",
    "    \"df_path\": path,\n",
    "\n",
    "    ############\n",
    "    # Training\n",
    "    ############\n",
    "\n",
    "    ## Experiment Setup\n",
    "    \"name\": f\"Exp{exp_nb}\",\n",
    "\n",
    "    ## Model\n",
    "    \"num_classes\": 1,\n",
    "    \"input_channel\": 15,\n",
    "    \"backbone\": \"timm_efficientnet_b1\",\n",
    "    \"pretrained\": 1,\n",
    "    \"model\": \"unetplusplus\",\n",
    "\n",
    "    # Training Setup\n",
    "#     \"resume\": \"trained_models/Exp7-4/fold_0/2022-02-05_23-13-25/best_metric_18_0.9768.pt\",\n",
    "    \"print_freq\": 500,\n",
    "    \"use_fp16\": 0,\n",
    "    \"patience\": 8, #5,\n",
    "    \"epoch_start_scheduler\": 10,\n",
    "\n",
    "    # Optimizer\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 0.0,\n",
    "\n",
    "    ## Data Augmentation on CPU\n",
    "    \"train_crop_size\": 256 + 32 + 32,\n",
    "    \"train_batch_size\": 16, #32\n",
    "    \"cutmix_alpha\": 0,\n",
    "    \"da_brightness_magnitude\": 0.0,\n",
    "    \"da_contrast_magnitude\": 0.0,\n",
    "\n",
    "    # Data Augmentation on GPU\n",
    "    \"gpu_da_params\": [0.25],\n",
    "    \n",
    "    ### Loss, Metric\n",
    "    \"alpha\": 0.25,\n",
    "#     \"loss\": \"lovasz\",\n",
    "           }\n",
    "\n",
    "for fold_nb in range(0, 5):\n",
    "    hps = HyperParams(**hps_dict)\n",
    "    hps.fold_nb = fold_nb\n",
    "    num_batches = 250 // (hps.train_batch_size * torch.cuda.device_count())\n",
    "    # num_batches = 10\n",
    "    hps.num_batches = num_batches\n",
    "    train_dataset, train_loader, val_dataset, val_loader = get_dataloaders(hps)\n",
    "    continue\n",
    "\n",
    "    best_metric, best_metric_epoch = train(hps, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a53af26-d0ac-4d6b-8786-278bb88174da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"event_id\"]==1387993]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697af033-72ee-4d15-bab0-2d05b4b5e8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.visualize(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cb4d52-5132-4b04-95dd-345025a4b81a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in range(100):\n",
    "    print(np.unique(train_dataset[k][\"mask\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb2a2f0-88dd-435b-aa90-0bba36c8296f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_nb = \"S2_single_1-1\"\n",
    "path = \"catalogues/2025_08_18_s2_single.pkl\"\n",
    "hps_dict = {\n",
    "    ############\n",
    "    # Data\n",
    "    ############\n",
    "    \"df_path\": path,\n",
    "\n",
    "    ############\n",
    "    # Training\n",
    "    ############\n",
    "\n",
    "    ## Experiment Setup\n",
    "    \"name\": f\"Exp{exp_nb}\",\n",
    "\n",
    "    ## Model\n",
    "    \"num_classes\": 1,\n",
    "    \"input_channel\": 15,\n",
    "    \"backbone\": \"timm_efficientnet_b1\",\n",
    "    \"pretrained\": 1,\n",
    "    \"model\": \"unetplusplus\",\n",
    "\n",
    "    # Training Setup\n",
    "#     \"resume\": \"trained_models/Exp7-4/fold_0/2022-02-05_23-13-25/best_metric_18_0.9768.pt\",\n",
    "    \"print_freq\": 500,\n",
    "    \"use_fp16\": 0,\n",
    "    \"patience\": 6, #5,\n",
    "    \"epoch_start_scheduler\": 6,\n",
    "\n",
    "    # Optimizer\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 0.0,\n",
    "\n",
    "    ## Data Augmentation on CPU\n",
    "    \"train_crop_size\": 400,\n",
    "    \"train_batch_size\": 16, #32\n",
    "    \"cutmix_alpha\": 0,\n",
    "    \"da_brightness_magnitude\": 0.0,\n",
    "    \"da_contrast_magnitude\": 0.0,\n",
    "\n",
    "    # Data Augmentation on GPU\n",
    "    \"gpu_da_params\": [0.25],\n",
    "    \n",
    "    ### Loss, Metric\n",
    "    \"alpha\": 0.5,\n",
    "#     \"loss\": \"lovasz\",\n",
    "           }\n",
    "\n",
    "for fold_nb in range(0, 5):\n",
    "    hps = HyperParams(**hps_dict)\n",
    "    hps.fold_nb = fold_nb\n",
    "    num_batches = 250 // (hps.train_batch_size * torch.cuda.device_count())\n",
    "    # num_batches = 10\n",
    "    hps.num_batches = num_batches\n",
    "    train_dataset, train_loader, val_dataset, val_loader = get_dataloaders(hps)\n",
    "    # continue\n",
    "\n",
    "    best_metric, best_metric_epoch = train(hps, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce6c00-2ffc-4bb6-abb3-fdc6f7aefe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"trained_models/ExpS2_single_1-2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b857b6bb-c91d-4754-8c8e-a7a3de228f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_nb = \"S2_single_1-2\"\n",
    "path = \"catalogues/2025_08_18_s2_single.pkl\"\n",
    "hps_dict = {\n",
    "    ############\n",
    "    # Data\n",
    "    ############\n",
    "    \"df_path\": path,\n",
    "\n",
    "    ############\n",
    "    # Training\n",
    "    ############\n",
    "\n",
    "    ## Experiment Setup\n",
    "    \"name\": f\"Exp{exp_nb}\",\n",
    "\n",
    "    ## Model\n",
    "    \"num_classes\": 1,\n",
    "    \"input_channel\": 15,\n",
    "    \"backbone\": \"timm_efficientnet_b1\",\n",
    "    \"pretrained\": 1,\n",
    "    \"model\": \"unetplusplus\",\n",
    "\n",
    "    # Training Setup\n",
    "#     \"resume\": \"trained_models/Exp7-4/fold_0/2022-02-05_23-13-25/best_metric_18_0.9768.pt\",\n",
    "    \"print_freq\": 500,\n",
    "    \"use_fp16\": 0,\n",
    "    \"patience\": 6, #5,\n",
    "    \"epoch_start_scheduler\": 10,\n",
    "\n",
    "    # Optimizer\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 0.0,\n",
    "\n",
    "    ## Data Augmentation on CPU\n",
    "    \"train_crop_size\": 256 + 32 + 32,\n",
    "    \"train_batch_size\": 16, #32\n",
    "    \"cutmix_alpha\": 0,\n",
    "    \"da_brightness_magnitude\": 0.0,\n",
    "    \"da_contrast_magnitude\": 0.0,\n",
    "\n",
    "    # Data Augmentation on GPU\n",
    "    \"gpu_da_params\": [0.25],\n",
    "    \n",
    "    ### Loss, Metric\n",
    "    \"alpha\": 0.5,\n",
    "#     \"loss\": \"lovasz\",\n",
    "           }\n",
    "\n",
    "for fold_nb in range(0, 5):\n",
    "    hps = HyperParams(**hps_dict)\n",
    "    hps.fold_nb = fold_nb\n",
    "    num_batches = 250 // (hps.train_batch_size * torch.cuda.device_count())\n",
    "    # num_batches = 10\n",
    "    hps.num_batches = num_batches\n",
    "    train_dataset, train_loader, val_dataset, val_loader = get_dataloaders(hps)\n",
    "    # continue\n",
    "\n",
    "    best_metric, best_metric_epoch = train(hps, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd215298-572a-4a3a-b7cb-8619778903ad",
   "metadata": {},
   "source": [
    "## Create cross validation dataset with using the last after image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bbe897-377e-410f-9c27-3f7f8aa00dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_file(\"data/mapbiomas_alerts.geojson\")\n",
    "events_sorted_by_area = df.sort_values(\"areaHa\", ascending=False)[\"alertCode\"].tolist()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0def0e-3fa3-497c-8fc3-3c1062385cd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paths_before_, paths_after_ = [], []\n",
    "clouds_nodata_before_, clouds_nodata_after_ = [], []\n",
    "s2_valid_data_events = []\n",
    "for event_id in events_sorted_by_area[:]:\n",
    "    s2_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel2/*L2A*.tif\") if not \"ndvi\" in k and not \"ndwi\" in k and not \"uncompressed\" in k])\n",
    "    if len(s2_paths) == 0:\n",
    "        continue\n",
    "    mask_path = glob.glob(f\"data/{event_id}/*_mask.tif\")[0]\n",
    "    mask_date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "    mask_date_str = f\"{mask_date[:4]}-{mask_date[4:6]}-{mask_date[6:8]}\"\n",
    "    mask_date = datetime.strptime(mask_date, \"%Y%m%d\")\n",
    "    # print(mask_date)\n",
    "    \n",
    "    date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "    # Sort by the date right before the .tif\n",
    "    paths = sorted(s2_paths, key=lambda p: datetime.strptime(p.split('_')[-1].split('.')[0], \"%Y%m%d\"))\n",
    "    # break\n",
    "    paths_after, paths_before = [], []\n",
    "    clouds_nodata_after, clouds_nodata_before = [], []\n",
    "    for path in paths:\n",
    "        # what_to_do_string = \"SKIP\"\n",
    "        img_orig = tifffile.imread(path)\n",
    "        # img_orig = img_orig.astype(np.float32)\n",
    "        # img_orig = img_orig - 1000\n",
    "        # img_orig[img_orig < 0] = 0\n",
    "        clouds_perc = 100 * (img_orig[:, :, 2] > 3400).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "        nodata_perc = 100 * (img_orig[:, :, 2] <= 1000).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "        date = path.split(\"/\")[-1].split('.')[0].split('_')[-1]\n",
    "        date_str = f\"{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "            \n",
    "        date = datetime.strptime(path.split('_')[-1].split('.')[0], \"%Y%m%d\")\n",
    "        if date > mask_date:\n",
    "            paths_after.append(path)\n",
    "            clouds_nodata_after.append(clouds_perc + nodata_perc)\n",
    "        else:\n",
    "            paths_before.append(path)\n",
    "            clouds_nodata_before.append(clouds_perc + nodata_perc)\n",
    "    print(f\"{event_id}: {len(paths):2} S2 L2A paths, {len(paths_before):2} before, {len(paths_after):2} after, ({mask_date})\")\n",
    "    if len(clouds_nodata_before) > 0 and len(clouds_nodata_after) > 0:\n",
    "        print(f\" --> min nodata/clouds before: {min(clouds_nodata_before):5.1f}%, after: {min(clouds_nodata_after):5.1f}%\")\n",
    "    if len(paths_before) > 0 and len(paths_after) > 0:\n",
    "        s2_valid_data_events.append(event_id)\n",
    "        paths_after_.append(paths_after)\n",
    "        clouds_nodata_after_.append(clouds_nodata_after)\n",
    "        paths_before_.append(paths_before)\n",
    "        clouds_nodata_before_.append(clouds_nodata_before)\n",
    "len(s2_valid_data_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0ea729-1840-4d0c-8768-3d9fa8ec30fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"event_id\": s2_valid_data_events, \n",
    "              \"paths_before\": paths_before_, \"clouds_nodata_before\": clouds_nodata_before_,\n",
    "                  \"paths_after\": paths_after_, \"clouds_nodata_after\": clouds_nodata_after_})\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0c31b8-bc8a-4211-aaae-000a2e9897a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def best_before_path(row):\n",
    "    # Take the earliest date with clouds < 10%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"], row[\"paths_before\"]):\n",
    "        if clouds_nodata < 10:\n",
    "            return path, clouds_nodata\n",
    "    # Take the earliest date with clouds < 30%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"], row[\"paths_before\"]):\n",
    "        if clouds_nodata < 30:\n",
    "            return path, clouds_nodata\n",
    "    # Take the earliest date with clouds < 100%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"], row[\"paths_before\"]):\n",
    "        if clouds_nodata < 100:\n",
    "            return path, clouds_nodata\n",
    "    # Else return the oldest path\n",
    "    return row[\"paths_before\"][0], row[\"clouds_nodata_before\"][0]\n",
    "\n",
    "def best_after_path(row):\n",
    "    # Take the latest date with clouds < 10%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"][::-1], row[\"paths_after\"][::-1]):\n",
    "        if clouds_nodata < 10:\n",
    "            return path, clouds_nodata\n",
    "    # Take the latest date with clouds < 30%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"][::-1], row[\"paths_after\"][::-1]):\n",
    "        if clouds_nodata < 30:\n",
    "            return path, clouds_nodata\n",
    "    # Take the latest date with clouds < 100%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"][::-1], row[\"paths_after\"][::-1]):\n",
    "        if clouds_nodata < 100:\n",
    "            return path, clouds_nodata\n",
    "    # Else return the newest path\n",
    "    return row[\"paths_after\"][-1], row[\"clouds_nodata_after\"][-1]\n",
    "    \n",
    "df[\"path_best_before\"] = df[[\"paths_before\", \"clouds_nodata_before\"]].apply(lambda row: best_before_path(row)[0], axis=1)\n",
    "df[\"path_best_after\"] = df[[\"paths_after\", \"clouds_nodata_after\"]].apply(lambda row: best_after_path(row)[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c371fb80-c4e4-474e-b4f3-92d7cd4e1c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clouds_nodata_best_before\"] = df[[\"paths_before\", \"clouds_nodata_before\"]].apply(lambda row: best_before_path(row)[1], axis=1)\n",
    "df[\"clouds_nodata_best_after\"] = df[[\"paths_after\", \"clouds_nodata_after\"]].apply(lambda row: best_after_path(row)[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0659d8f8-bac0-4954-ad2a-db09fbc12ecc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_paths = []\n",
    "deforest_pxs = []\n",
    "for event_id in df[\"event_id\"].tolist():\n",
    "    label_path = glob.glob(f\"data/{event_id}/*_mask.tif\")[0]\n",
    "    label_paths.append(label_path)\n",
    "    label = tifffile.imread(label_path)\n",
    "    deforest_pxs.append((label == 255).sum())\n",
    "df[\"deforest_pxs\"] = deforest_pxs\n",
    "df[\"label_path\"] = label_paths\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e751faa-fc2e-4bee-b0f0-9792bdf7d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39e592d-b61d-45db-8772-67027771fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 222221\n",
    "while True:\n",
    "    np.random.seed(seed)\n",
    "    folds = [0, 1, 2, 3, 4] * (len(df) // 5 + 1) #np.random.choice([0, 1, 2, 3, 4], size=len(df))\n",
    "    np.random.shuffle(folds)\n",
    "    df[\"fold\"] = folds[:len(df)]\n",
    "    # print(df[\"fold\"].value_counts())\n",
    "    std_counts = df.groupby(\"fold\")[\"deforest_pxs\"].sum().std().item()\n",
    "    # print(f'Seed {seed}: {df.groupby(\"fold\")[\"deforest_pxs\"].sum()}, Std = {std_counts:.0f}')\n",
    "    if std_counts < 2000:\n",
    "        break\n",
    "    seed += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b7c19b-3b11-4d07-9f4c-1e9e210cfda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    with rasterio.open(row[\"path_best_before\"]) as src:\n",
    "        output_path = row[\"path_best_before\"].replace(\".tif\", \"_uncompressed.tif\")\n",
    "        if not os.path.exists(output_path):\n",
    "            profile = src.profile\n",
    "            profile[\"compress\"] = None\n",
    "            with rasterio.open(output_path, \"w\", **profile) as dst:\n",
    "                dst.write(src.read())\n",
    "\n",
    "    with rasterio.open(row[\"path_best_after\"]) as src:\n",
    "        output_path = row[\"path_best_after\"].replace(\".tif\", \"_uncompressed.tif\")\n",
    "        if not os.path.exists(output_path):\n",
    "            profile = src.profile\n",
    "            profile[\"compress\"] = None\n",
    "            with rasterio.open(output_path, \"w\", **profile) as dst:\n",
    "                dst.write(src.read())\n",
    "\n",
    "df[\"path_best_before\"] = df[\"path_best_before\"].apply(lambda x: x.replace(\".tif\", \"_uncompressed.tif\"))\n",
    "df[\"path_best_after\"] = df[\"path_best_after\"].apply(lambda x: x.replace(\".tif\", \"_uncompressed.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e17c3da-201d-4bae-bf1a-65f0c938e5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"catalogues\", exist_ok=True)\n",
    "df.to_pickle(\"catalogues/2025_08_18_s2_single_last_after_img.pkl\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d31198c-4d6b-4805-ba38-92daa8a9e612",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(\"fold\")[\"deforest_pxs\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322c5b5d-84d4-42c3-8273-2c7e87d09650",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2121827-b2d5-4e0d-bd69-2b937c4f5dde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_nb = \"S2_single_2-0\"\n",
    "path = \"catalogues/2025_08_18_s2_single_last_after_img.pkl\"\n",
    "hps_dict = {\n",
    "    ############\n",
    "    # Data\n",
    "    ############\n",
    "    \"df_path\": path,\n",
    "\n",
    "    ############\n",
    "    # Training\n",
    "    ############\n",
    "\n",
    "    ## Experiment Setup\n",
    "    \"name\": f\"Exp{exp_nb}\",\n",
    "\n",
    "    ## Model\n",
    "    \"num_classes\": 1,\n",
    "    \"input_channel\": 15,\n",
    "    \"backbone\": \"timm_efficientnet_b1\",\n",
    "    \"pretrained\": 1,\n",
    "    \"model\": \"unetplusplus\",\n",
    "\n",
    "    # Training Setup\n",
    "#     \"resume\": \"trained_models/Exp7-4/fold_0/2022-02-05_23-13-25/best_metric_18_0.9768.pt\",\n",
    "    \"print_freq\": 500,\n",
    "    \"use_fp16\": 0,\n",
    "    \"patience\": 8, #5,\n",
    "    \"epoch_start_scheduler\": 10,\n",
    "\n",
    "    # Optimizer\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 0.0,\n",
    "\n",
    "    ## Data Augmentation on CPU\n",
    "    \"train_crop_size\": 256 + 32 + 32,\n",
    "    \"train_batch_size\": 16, #32\n",
    "    \"cutmix_alpha\": 0,\n",
    "    \"da_brightness_magnitude\": 0.0,\n",
    "    \"da_contrast_magnitude\": 0.0,\n",
    "\n",
    "    # Data Augmentation on GPU\n",
    "    \"gpu_da_params\": [0.25],\n",
    "    \n",
    "    ### Loss, Metric\n",
    "    \"alpha\": 0.25,\n",
    "#     \"loss\": \"lovasz\",\n",
    "           }\n",
    "\n",
    "for fold_nb in range(0, 5):\n",
    "    hps = HyperParams(**hps_dict)\n",
    "    hps.fold_nb = fold_nb\n",
    "    num_batches = 250 // (hps.train_batch_size * torch.cuda.device_count())\n",
    "    # num_batches = 10\n",
    "    hps.num_batches = num_batches\n",
    "    train_dataset, train_loader, val_dataset, val_loader = get_dataloaders(hps)\n",
    "    # continue\n",
    "\n",
    "    best_metric, best_metric_epoch = train(hps, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe0226b-0bfa-4601-9c28-317165de8d93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
