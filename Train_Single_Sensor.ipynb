{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47382a04-3378-458b-875c-4ce62a6e4b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tifffile\n",
      "  Downloading tifffile-2025.6.11-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from tifffile) (2.3.1)\n",
      "Downloading tifffile-2025.6.11-py3-none-any.whl (230 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.8/230.8 kB\u001b[0m \u001b[31m283.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tifffile\n",
      "Successfully installed tifffile-2025.6.11\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6031383c-6318-4d2e-86f9-6ace15e99fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tifffile\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 250)\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import shape\n",
    "import glob\n",
    "from shapely.geometry import box\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "import json\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "red_to_green = LinearSegmentedColormap.from_list(\"RedGreen\", [\"red\", \"green\"])\n",
    "\n",
    "from src.train import train\n",
    "from src.hyperparams import HyperParams, open_from_yaml\n",
    "from src.data import *\n",
    "# from src.validate import return_metrics_all_folds, visualize_single_model, cross_validation, visualize_s2_concat_bands_path\n",
    "from utils import visualize_s2_concat_bands_path, visualize_s1_path, scale_img, visualize_ls_concat_bands_path, visualize_cb_mux_path, visualize_cb_wpm_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e7410b0-57cb-427b-b184-76ea8bf0c176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_cloud_nodata = 75\n",
    "# s2_valid_data_events = []\n",
    "# for event_id in events_sorted_by_area[:]:\n",
    "#     # rand_int = np.random.randint(len(events_sorted_by_area))\n",
    "#     # rand_int = k\n",
    "#     # event_id = events_sorted_by_area[rand_int]\n",
    "#     # s2_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel2/*L2A*.tif\") if not \"ndvi\" in k and not \"ndwi\" in k])\n",
    "#     s1_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel1/*.tif\") if not \"_nrpb\" in k and not \"_rfdi\" in k and not \"_rvi\" in k and not \"_vv_vh_ratio\" in k])\n",
    "#     if len(s1_paths) == 0:\n",
    "#         continue\n",
    "#     # s2_16d_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel2/*16D*.tif\") if not \"ndvi\" in k and not \"ndwi\" in k])\n",
    "#     # ls_paths = sorted([k for k in glob.glob(f\"data/{event_id}/landsat/*_L2SP_*.tif\") if not \"_nbr\" in k and not \"_ndwi\" in k and not \"_ndvi\" in k])\n",
    "#     # cb_wpm_paths = sorted([k for k in glob.glob(f\"data/{event_id}/cbers4a/*_WPM_*.tif\") if not \"_ndvi\" in k])\n",
    "#     # cb_mux_paths = sorted([k for k in glob.glob(f\"data/{event_id}/cbers4a/*_MUX_*.tif\") if not \"_ndvi\" in k])\n",
    "#     # print(f\"{event_id}: {len(s2_paths):2} S2 L2A, {len(s2_16d_paths):2} S2 16D,{len(s1_paths):2} S1, {len(ls_paths):2} LS L2SP, {len(cb_wpm_paths):2} CB WPM, {len(cb_mux_paths):2} CB MUX\")\n",
    "#     # paths = s2_paths + s1_paths + ls_paths + cb_wpm_paths + cb_mux_paths + s2_16d_paths\n",
    "#     # if len(paths) == 0:\n",
    "#     #     continue\n",
    "#     # continue\n",
    "#     mask_path = glob.glob(f\"data/{event_id}/*_mask.tif\")[0]\n",
    "#     mask_date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "#     mask_date_str = f\"{mask_date[:4]}-{mask_date[4:6]}-{mask_date[6:8]}\"\n",
    "#     mask_date = datetime.strptime(mask_date, \"%Y%m%d\")\n",
    "#     # print(mask_date)\n",
    "    \n",
    "#     date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "#     # Sort by the date right before the .tif\n",
    "#     paths = sorted(s1_paths, key=lambda p: datetime.strptime(p.split('_')[-1].split('.')[0], \"%Y%m%d\"))\n",
    "#     # break\n",
    "#     paths_after, paths_before = [], []\n",
    "#     for path in paths:\n",
    "#         date = datetime.strptime(path.split('_')[-1].split('.')[0], \"%Y%m%d\")\n",
    "#         if date > mask_date:\n",
    "#             paths_after.append(path)\n",
    "#         else:\n",
    "#             paths_before.append(path)\n",
    "#     print(f\"{event_id}: {len(paths):2} S1 paths, {len(paths_before):2} before, {len(paths_after):2} after ({mask_date})\")\n",
    "#     if len(paths_before) > 0 and len(paths_after) > 0:\n",
    "#         s2_valid_data_events.append(event_id)\n",
    "# len(s2_valid_data_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f2051-5469-44da-b857-2438fb8bdd4a",
   "metadata": {},
   "source": [
    "# S2 single sensor, single before/after image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a3ae77-1192-43da-bf7f-30ee9bef3476",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create cross validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "084c5c82-c50c-4292-9d11-ecdbf75ec1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = gpd.read_file(\"data/mapbiomas_alerts.geojson\")\n",
    "events_sorted_by_area = df.sort_values(\"areaHa\", ascending=False)[\"alertCode\"].tolist()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17e98212-b27f-4a21-b5db-5b22a0f8fcd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1389549: 16 S2 L2A paths, 10 before,  6 after, (2025-05-01 00:00:00)\n",
      " --> min nodata/clouds before:   8.5%, after:   0.0%\n",
      "1389572: 15 S2 L2A paths, 10 before,  5 after, (2025-05-19 00:00:00)\n",
      " --> min nodata/clouds before: 100.0%, after: 100.0%\n",
      "1389158:  1 S2 L2A paths,  1 before,  0 after, (2025-05-08 00:00:00)\n",
      "1389503:  8 S2 L2A paths,  5 before,  3 after, (2025-05-16 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1389968: 13 S2 L2A paths, 10 before,  3 after, (2025-05-15 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1389833:  2 S2 L2A paths,  2 before,  0 after, (2025-05-01 00:00:00)\n",
      "1387972: 18 S2 L2A paths, 12 before,  6 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1388042: 13 S2 L2A paths, 11 before,  2 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1389517: 17 S2 L2A paths,  9 before,  8 after, (2025-05-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1387954:  9 S2 L2A paths,  5 before,  4 after, (2025-03-01 00:00:00)\n",
      " --> min nodata/clouds before: 100.0%, after: 100.0%\n",
      "1388065: 13 S2 L2A paths, 11 before,  2 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before:   2.7%, after: 100.0%\n",
      "1389891: 21 S2 L2A paths, 12 before,  9 after, (2025-05-09 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1388607: 10 S2 L2A paths,  7 before,  3 after, (2025-05-01 00:00:00)\n",
      " --> min nodata/clouds before:  85.4%, after:  85.5%\n",
      "1389526: 11 S2 L2A paths,  6 before,  5 after, (2025-05-08 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.1%\n",
      "1388066: 11 S2 L2A paths,  7 before,  4 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before: 100.0%, after:  80.3%\n",
      "1388048: 16 S2 L2A paths, 10 before,  6 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.2%, after:   1.4%\n",
      "1388078: 14 S2 L2A paths, 12 before,  2 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after: 100.0%\n",
      "1388022:  6 S2 L2A paths,  3 before,  3 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before: 100.0%, after:   0.3%\n",
      "1389165: 12 S2 L2A paths,  6 before,  6 after, (2025-05-21 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1388023:  5 S2 L2A paths,  0 before,  5 after, (2025-02-01 00:00:00)\n",
      "1389786: 17 S2 L2A paths, 13 before,  4 after, (2025-05-21 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1388039: 12 S2 L2A paths,  9 before,  3 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1387927: 13 S2 L2A paths,  9 before,  4 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after: 100.0%\n",
      "1389494: 22 S2 L2A paths, 14 before,  8 after, (2025-05-26 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1390023: 11 S2 L2A paths,  8 before,  3 after, (2025-05-27 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1387942:  2 S2 L2A paths,  2 before,  0 after, (2025-01-01 00:00:00)\n",
      "1387997: 13 S2 L2A paths, 10 before,  3 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after: 100.0%\n",
      "1388043: 10 S2 L2A paths,  6 before,  4 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before: 100.0%, after: 100.0%\n",
      "1388015:  7 S2 L2A paths,  4 before,  3 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.3%, after:   0.0%\n",
      "1387993: 18 S2 L2A paths, 10 before,  8 after, (2025-03-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after: 100.0%\n",
      "1388031:  4 S2 L2A paths,  4 before,  0 after, (2025-01-01 00:00:00)\n",
      "1388045: 13 S2 L2A paths, 11 before,  2 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after: 100.0%\n",
      "1387989:  9 S2 L2A paths,  5 before,  4 after, (2025-03-01 00:00:00)\n",
      " --> min nodata/clouds before:   1.0%, after: 100.0%\n",
      "1387981: 17 S2 L2A paths, 12 before,  5 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before: 100.0%, after: 100.0%\n",
      "1388071: 17 S2 L2A paths, 11 before,  6 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1387944: 12 S2 L2A paths,  9 before,  3 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1387970:  4 S2 L2A paths,  3 before,  1 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after: 100.0%\n",
      "1388038:  3 S2 L2A paths,  1 before,  2 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.8%, after:   0.9%\n",
      "1388074: 13 S2 L2A paths, 11 before,  2 after, (2025-03-01 00:00:00)\n",
      " --> min nodata/clouds before:  11.4%, after: 100.0%\n",
      "1388007: 11 S2 L2A paths,  6 before,  5 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1388001: 13 S2 L2A paths, 11 before,  2 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.2%\n",
      "1387923:  3 S2 L2A paths,  2 before,  1 after, (2025-03-01 00:00:00)\n",
      " --> min nodata/clouds before: 100.0%, after: 100.0%\n",
      "1388059: 26 S2 L2A paths, 22 before,  4 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1388037: 11 S2 L2A paths,  9 before,  2 after, (2025-03-01 00:00:00)\n",
      " --> min nodata/clouds before: 100.0%, after: 100.0%\n",
      "1388051: 12 S2 L2A paths,  8 before,  4 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1387986: 12 S2 L2A paths, 11 before,  1 after, (2025-03-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after: 100.0%\n",
      "1387926: 13 S2 L2A paths, 11 before,  2 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after: 100.0%\n",
      "1388046: 15 S2 L2A paths, 10 before,  5 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.4%\n",
      "1388076: 12 S2 L2A paths, 10 before,  2 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after: 100.0%\n",
      "1388077: 11 S2 L2A paths,  7 before,  4 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1387951: 12 S2 L2A paths, 12 before,  0 after, (2025-01-01 00:00:00)\n",
      "1388016: 13 S2 L2A paths, 11 before,  2 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.1%\n",
      "1388067: 15 S2 L2A paths, 11 before,  4 after, (2025-03-01 00:00:00)\n",
      " --> min nodata/clouds before:  99.9%, after: 100.0%\n",
      "1388061: 17 S2 L2A paths, 11 before,  6 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1387971:  1 S2 L2A paths,  1 before,  0 after, (2025-01-01 00:00:00)\n",
      "1388035:  5 S2 L2A paths,  0 before,  5 after, (2025-02-01 00:00:00)\n",
      "1387928:  1 S2 L2A paths,  0 before,  1 after, (2025-02-01 00:00:00)\n",
      "1388079: 14 S2 L2A paths, 11 before,  3 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   4.7%, after: 100.0%\n",
      "1388028:  8 S2 L2A paths,  7 before,  1 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1387950:  2 S2 L2A paths,  0 before,  2 after, (2025-02-01 00:00:00)\n",
      "1389780: 14 S2 L2A paths,  9 before,  5 after, (2025-05-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.1%, after:   0.0%\n",
      "1388049: 14 S2 L2A paths, 11 before,  3 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after: 100.0%\n",
      "1389443: 13 S2 L2A paths,  7 before,  6 after, (2025-05-28 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1388080: 14 S2 L2A paths, 10 before,  4 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.0%\n",
      "1387925:  9 S2 L2A paths,  6 before,  3 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.9%, after:   0.5%\n",
      "1388026:  6 S2 L2A paths,  2 before,  4 after, (2025-02-01 00:00:00)\n",
      " --> min nodata/clouds before: 100.0%, after:   0.0%\n",
      "1388057: 14 S2 L2A paths, 12 before,  2 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after: 100.0%\n",
      "1388047: 23 S2 L2A paths, 17 before,  6 after, (2025-01-01 00:00:00)\n",
      " --> min nodata/clouds before:   0.0%, after:   0.4%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_before_, paths_after_ = [], []\n",
    "clouds_nodata_before_, clouds_nodata_after_ = [], []\n",
    "s2_valid_data_events = []\n",
    "for event_id in events_sorted_by_area[:]:\n",
    "    s2_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel2/*L2A*.tif\") if not \"ndvi\" in k and not \"ndwi\" in k])\n",
    "    if len(s2_paths) == 0:\n",
    "        continue\n",
    "    mask_path = glob.glob(f\"data/{event_id}/*_mask.tif\")[0]\n",
    "    mask_date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "    mask_date_str = f\"{mask_date[:4]}-{mask_date[4:6]}-{mask_date[6:8]}\"\n",
    "    mask_date = datetime.strptime(mask_date, \"%Y%m%d\")\n",
    "    # print(mask_date)\n",
    "    \n",
    "    date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "    # Sort by the date right before the .tif\n",
    "    paths = sorted(s2_paths, key=lambda p: datetime.strptime(p.split('_')[-1].split('.')[0], \"%Y%m%d\"))\n",
    "    # break\n",
    "    paths_after, paths_before = [], []\n",
    "    clouds_nodata_after, clouds_nodata_before = [], []\n",
    "    for path in paths:\n",
    "        # what_to_do_string = \"SKIP\"\n",
    "        img_orig = tifffile.imread(path)\n",
    "        # img_orig = img_orig.astype(np.float32)\n",
    "        # img_orig = img_orig - 1000\n",
    "        # img_orig[img_orig < 0] = 0\n",
    "        clouds_perc = 100 * (img_orig[:, :, 2] > 3400).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "        nodata_perc = 100 * (img_orig[:, :, 2] <= 1000).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "        date = path.split(\"/\")[-1].split('.')[0].split('_')[-1]\n",
    "        date_str = f\"{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "            \n",
    "        date = datetime.strptime(path.split('_')[-1].split('.')[0], \"%Y%m%d\")\n",
    "        if date > mask_date:\n",
    "            paths_after.append(path)\n",
    "            clouds_nodata_after.append(clouds_perc + nodata_perc)\n",
    "        else:\n",
    "            paths_before.append(path)\n",
    "            clouds_nodata_before.append(clouds_perc + nodata_perc)\n",
    "    print(f\"{event_id}: {len(paths):2} S2 L2A paths, {len(paths_before):2} before, {len(paths_after):2} after, ({mask_date})\")\n",
    "    if len(clouds_nodata_before) > 0 and len(clouds_nodata_after) > 0:\n",
    "        print(f\" --> min nodata/clouds before: {min(clouds_nodata_before):5.1f}%, after: {min(clouds_nodata_after):5.1f}%\")\n",
    "    if len(paths_before) > 0 and len(paths_after) > 0:\n",
    "        s2_valid_data_events.append(event_id)\n",
    "        paths_after_.append(paths_after)\n",
    "        clouds_nodata_after_.append(clouds_nodata_after)\n",
    "        paths_before_.append(paths_before)\n",
    "        clouds_nodata_before_.append(clouds_nodata_before)\n",
    "len(s2_valid_data_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372e464b-74d2-41c3-ae73-b4e150ca2896",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"event_id\": s2_valid_data_events, \n",
    "              \"paths_before\": paths_before_, \"clouds_nodata_before\": clouds_nodata_before_,\n",
    "                  \"paths_after\": paths_after_, \"clouds_nodata_after\": clouds_nodata_after_})\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c635b52-cbd9-488f-a7fa-9695178f2cc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def best_before_path(row):\n",
    "    # Take the earliest date with clouds < 10%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"], row[\"paths_before\"]):\n",
    "        if clouds_nodata < 10:\n",
    "            return path\n",
    "    # Take the earliest date with clouds < 30%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"], row[\"paths_before\"]):\n",
    "        if clouds_nodata < 30:\n",
    "            return path\n",
    "    # Take the earliest date with clouds < 100%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"], row[\"paths_before\"]):\n",
    "        if clouds_nodata < 100:\n",
    "            return path\n",
    "    # Else return the oldest path\n",
    "    return row[\"paths_before\"][0]\n",
    "\n",
    "def best_after_path(row):\n",
    "    # Take the latest date with clouds < 10%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"], row[\"paths_after\"]):\n",
    "        if clouds_nodata < 10:\n",
    "            return path\n",
    "    # Take the latest date with clouds < 30%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"], row[\"paths_after\"]):\n",
    "        if clouds_nodata < 30:\n",
    "            return path\n",
    "    # Take the latest date with clouds < 100%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"], row[\"paths_after\"]):\n",
    "        if clouds_nodata < 100:\n",
    "            return path\n",
    "    # Else return the newest path\n",
    "    return row[\"paths_after\"][-1]\n",
    "    \n",
    "df[\"path_best_before\"] = df[[\"paths_before\", \"clouds_nodata_before\"]].apply(lambda row: best_before_path(row), axis=1)\n",
    "df[\"path_best_after\"] = df[[\"paths_after\", \"clouds_nodata_after\"]].apply(lambda row: best_after_path(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a09e2d1-2074-4a98-87b2-425a9c78e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_paths = []\n",
    "deforest_pxs = []\n",
    "for event_id in df[\"event_id\"].tolist():\n",
    "    label_path = glob.glob(f\"data/{event_id}/*_mask.tif\")[0]\n",
    "    label_paths.append(label_path)\n",
    "    label = tifffile.imread(mask_path)\n",
    "    deforest_pxs.append((label == 255).sum())\n",
    "df[\"deforest_pxs\"] = deforest_pxs\n",
    "df[\"label_path\"] = label_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe615dfe-b37b-46b2-b50c-bfb83d7a1765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['event_id', 'paths_before', 'clouds_nodata_before', 'paths_after',\n",
       "       'clouds_nodata_after', 'path_best_before', 'path_best_after',\n",
       "       'deforest_pxs', 'label_path'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7c93318-6ac9-4034-9261-a51919cf556b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 10)\n"
     ]
    }
   ],
   "source": [
    "seed = 22222\n",
    "while True:\n",
    "    np.random.seed(seed)\n",
    "    folds = [0, 1, 2, 3, 4] * (len(df) // 5 + 1) #np.random.choice([0, 1, 2, 3, 4], size=len(df))\n",
    "    np.random.shuffle(folds)\n",
    "    df[\"fold\"] = folds[:len(df)]\n",
    "    # print(df[\"fold\"].value_counts())\n",
    "    std_counts = df.groupby(\"fold\")[\"deforest_pxs\"].sum().std().item()\n",
    "    # print(f'Seed {seed}: {df.groupby(\"fold\")[\"deforest_pxs\"].sum()}, Std = {std_counts:.0f}')\n",
    "    if std_counts < 1500:\n",
    "        break\n",
    "    seed += 1\n",
    "os.makedirs(\"catalogues\", exist_ok=True)\n",
    "df.to_pickle(\"catalogues/2025_08_18_s2_single.pkl\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4195f91-a82b-4519-bba4-99617f91e3cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8460766b-4d40-4d34-a0d7-641bd4092854",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a7a6c7-5621-41b6-a33a-e641d547c262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18-Aug-25 13:46:46 - Training with 1 GPUS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Augmentations: [RandomCrop(p=1.0, border_mode=0, fill=0.0, fill_mask=0.0, height=288, pad_if_needed=False, pad_position='center', width=288)]\n",
      "Train Image Data Augmentations: []\n",
      "####################################################################################################\n",
      "####################################################################################################\n",
      "Fold 0 --> Train: 46, Val: 12\n",
      "####################################################################################################\n",
      "We want to train 7 batches in each epoch, but our current dataset is only 3 batches long. Doubling dataset size\n",
      "We want to train 7 batches in each epoch, but our current dataset is only 6 batches long. Doubling dataset size\n",
      "Last 5 sample weights: [1, 1, 1, 1, 1]\n",
      "Dice for class 1: 0.984\n",
      "XE: 0.705\n",
      "Dice for class 1: 0.961\n",
      "XE: 0.634\n",
      "Dice for class 1: 0.974\n",
      "XE: 0.628\n",
      "Dice for class 1: 0.972\n",
      "XE: 0.578\n",
      "Dice for class 1: 0.957\n",
      "XE: 0.525\n",
      "Dice for class 1: 0.980\n",
      "XE: 0.520\n",
      "Dice for class 1: 0.964\n",
      "XE: 0.476\n",
      "Dice for class 1: 0.980\n",
      "XE: 0.417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18-Aug-25 13:46:59 - Ep: [1] TotalT: 0.2 min, BatchT: 1.281s, DataT: 0.299s, GpuDaT: 0.010s, Loss: 0.7659\n",
      "18-Aug-25 13:47:04 - Ep: [1]  ValT: 0.09 min, BatchT: 5.376s, DataT: 3.059s, Loss: 0.8394, IoU: 0.0048 (val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice for class 1: 0.993\n",
      "XE: 0.686\n",
      "Dice for class 1: 0.962\n",
      "XE: 0.438\n",
      "Dice for class 1: 0.986\n",
      "XE: 0.359\n",
      "Dice for class 1: 0.944\n",
      "XE: 0.416\n",
      "Dice for class 1: 0.978\n",
      "XE: 0.405\n",
      "Dice for class 1: 0.950\n",
      "XE: 0.429\n",
      "Dice for class 1: 0.983\n",
      "XE: 0.374\n",
      "Dice for class 1: 0.981\n",
      "XE: 0.356\n",
      "Dice for class 1: 0.981\n",
      "XE: 0.325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18-Aug-25 13:47:11 - Ep: [2] TotalT: 0.4 min, BatchT: 0.701s, DataT: 0.452s, GpuDaT: 0.006s, Loss: 0.6792\n",
      "18-Aug-25 13:47:14 - Ep: [2]  ValT: 0.05 min, BatchT: 2.971s, DataT: 2.816s, Loss: 0.7220, IoU: 0.0046 (val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice for class 1: 0.993\n",
      "XE: 0.451\n",
      "Dice for class 1: 0.946\n",
      "XE: 0.358\n",
      "Dice for class 1: 0.961\n",
      "XE: 0.347\n",
      "Dice for class 1: 0.987\n",
      "XE: 0.300\n",
      "Dice for class 1: 0.986\n",
      "XE: 0.314\n",
      "Dice for class 1: 0.979\n",
      "XE: 0.308\n",
      "Dice for class 1: 0.962\n",
      "XE: 0.283\n",
      "Dice for class 1: 0.973\n",
      "XE: 0.284\n",
      "Dice for class 1: 0.971\n",
      "XE: 0.287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18-Aug-25 13:47:21 - Ep: [3] TotalT: 0.6 min, BatchT: 0.693s, DataT: 0.444s, GpuDaT: 0.005s, Loss: 0.6403\n",
      "18-Aug-25 13:47:24 - Ep: [3]  ValT: 0.05 min, BatchT: 2.952s, DataT: 2.803s, Loss: 1.0922, IoU: 0.0055 (val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice for class 1: 0.992\n",
      "XE: 1.192\n",
      "Dice for class 1: 0.968\n",
      "XE: 0.276\n",
      "Dice for class 1: 0.973\n",
      "XE: 0.282\n",
      "Dice for class 1: 0.979\n",
      "XE: 0.251\n",
      "Dice for class 1: 0.985\n",
      "XE: 0.261\n",
      "Dice for class 1: 0.969\n",
      "XE: 0.248\n",
      "Dice for class 1: 0.989\n",
      "XE: 0.220\n",
      "Dice for class 1: 0.964\n",
      "XE: 0.256\n",
      "Dice for class 1: 0.948\n",
      "XE: 0.269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18-Aug-25 13:47:30 - Ep: [4] TotalT: 0.7 min, BatchT: 0.672s, DataT: 0.423s, GpuDaT: 0.005s, Loss: 0.6149\n"
     ]
    }
   ],
   "source": [
    "exp_nb = \"S2_single_0-1\"\n",
    "path = \"catalogues/2025_08_18_s2_single.pkl\"\n",
    "hps_dict = {\n",
    "    ############\n",
    "    # Data\n",
    "    ############\n",
    "    \"df_path\": path,\n",
    "\n",
    "    ############\n",
    "    # Training\n",
    "    ############\n",
    "\n",
    "    ## Experiment Setup\n",
    "    \"name\": f\"Exp{exp_nb}\",\n",
    "\n",
    "    ## Model\n",
    "    \"num_classes\": 1,\n",
    "    \"input_channel\": 13,\n",
    "    \"backbone\": \"timm_efficientnet_b1\",\n",
    "    \"pretrained\": 1,\n",
    "    \"model\": \"unetplusplus\",\n",
    "\n",
    "    # Training Setup\n",
    "#     \"resume\": \"trained_models/Exp7-4/fold_0/2022-02-05_23-13-25/best_metric_18_0.9768.pt\",\n",
    "    \"print_freq\": 10,\n",
    "    \"use_fp16\": 0,\n",
    "    \"patience\": 5, #5,\n",
    "    \"epoch_start_scheduler\": 5,\n",
    "\n",
    "    # Optimizer\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 0.0,\n",
    "\n",
    "    ## Data Augmentation on CPU\n",
    "    \"train_crop_size\": 256 + 32,\n",
    "    \"train_batch_size\": 14, #32\n",
    "    \"cutmix_alpha\": 0,\n",
    "    \"da_brightness_magnitude\": 0.0,\n",
    "    \"da_contrast_magnitude\": 0.0,\n",
    "\n",
    "    # Data Augmentation on GPU\n",
    "    \"gpu_da_params\": [0.25],\n",
    "    \n",
    "    ### Loss, Metric\n",
    "    \"alpha\": 0.5,\n",
    "#     \"loss\": \"lovasz\",\n",
    "           }\n",
    "\n",
    "for fold_nb in range(5):\n",
    "    hps = HyperParams(**hps_dict)\n",
    "    hps.fold_nb = fold_nb\n",
    "    num_batches = 100 // (hps.train_batch_size * torch.cuda.device_count())\n",
    "    # num_batches = 10\n",
    "    hps.num_batches = num_batches\n",
    "    train_dataset, train_loader, val_dataset, val_loader = get_dataloaders(hps)\n",
    "    # continue\n",
    "\n",
    "    best_metric, best_metric_epoch = train(hps, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cb4d52-5132-4b04-95dd-345025a4b81a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in range(100):\n",
    "    print(np.unique(train_dataset[k][\"mask\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa53266-8ef5-4105-8743-5b615dc360e1",
   "metadata": {},
   "source": [
    "# S1 single sensor, single before/after image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910ba1cd-4c2d-4328-ba05-e8f574a0133a",
   "metadata": {},
   "source": [
    "## Create cross validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa5b11-5b52-4802-a501-3dea8b501b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_file(\"data/mapbiomas_alerts.geojson\")\n",
    "events_sorted_by_area = df.sort_values(\"areaHa\", ascending=False)[\"alertCode\"].tolist()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb62d90-d70d-4586-ac99-a8fce6ee27ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paths_before_, paths_after_ = [], []\n",
    "nodata_before_, nodata_after_ = [], []\n",
    "s1_valid_data_events = []\n",
    "for event_id in events_sorted_by_area[:]:\n",
    "    s1_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel1/*.tif\") if not \"_nrpb\" in k and not \"_rfdi\" in k and not \"_rvi\" in k and not \"_vv_vh_ratio\" in k])\n",
    "    if len(s1_paths) == 0:\n",
    "        continue\n",
    "    mask_path = glob.glob(f\"data/{event_id}/*_mask.tif\")[0]\n",
    "    mask_date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "    mask_date_str = f\"{mask_date[:4]}-{mask_date[4:6]}-{mask_date[6:8]}\"\n",
    "    mask_date = datetime.strptime(mask_date, \"%Y%m%d\")\n",
    "    # print(mask_date)\n",
    "    \n",
    "    date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "    # Sort by the date right before the .tif\n",
    "    paths = sorted(s1_paths, key=lambda p: datetime.strptime(p.split('_')[-1].split('.')[0], \"%Y%m%d\"))\n",
    "    # break\n",
    "    paths_after, paths_before = [], []\n",
    "    nodata_after, nodata_before = [], []\n",
    "    for path in paths:\n",
    "        # what_to_do_string = \"SKIP\"\n",
    "        img_orig = tifffile.imread(path)\n",
    "        nodata_perc = 100 * (img_orig[:, :, 0] == 0).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "        date = path.split(\"/\")[-1].split('.')[0].split('_')[-1]\n",
    "        date_str = f\"{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "            \n",
    "        date = datetime.strptime(path.split('_')[-1].split('.')[0], \"%Y%m%d\")\n",
    "        if date > mask_date:\n",
    "            paths_after.append(path)\n",
    "            nodata_after.append(nodata_perc)\n",
    "        else:\n",
    "            paths_before.append(path)\n",
    "            nodata_before.append(nodata_perc)\n",
    "    print(f\"{event_id}: {len(paths):2} S1 paths, {len(paths_before):2} before, {len(paths_after):2} after, ({mask_date})\")\n",
    "    if len(nodata_before) > 0 and len(nodata_after) > 0:\n",
    "        print(f\" --> min nodata before: {min(nodata_before):5.1f}%, after: {min(nodata_after):5.1f}%\")\n",
    "    if len(paths_before) > 0 and len(paths_after) > 0:\n",
    "        s1_valid_data_events.append(event_id)\n",
    "        paths_after_.append(paths_after)\n",
    "        nodata_after_.append(nodata_after)\n",
    "        paths_before_.append(paths_before)\n",
    "        nodata_before_.append(nodata_before)\n",
    "len(s1_valid_data_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caa8b12-767e-4dc2-81e8-9d8fcda99338",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"event_id\": s1_valid_data_events, \n",
    "              \"paths_before\": paths_before_, \"nodata_before\": nodata_before_,\n",
    "                  \"paths_after\": paths_after_, \"nodata_after\": nodata_after_})\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a219863-2dcd-4f67-89f8-d36f3c09a89f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def best_before_path(row):\n",
    "    # Take the earliest date with clouds < 10%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"], row[\"paths_before\"]):\n",
    "        if clouds_nodata < 10:\n",
    "            return path\n",
    "    # Take the earliest date with clouds < 30%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"], row[\"paths_before\"]):\n",
    "        if clouds_nodata < 30:\n",
    "            return path\n",
    "    # Take the earliest date with clouds < 100%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_before\"], row[\"paths_before\"]):\n",
    "        if clouds_nodata < 100:\n",
    "            return path\n",
    "    # Else return the oldest path\n",
    "    return row[\"paths_before\"][0]\n",
    "\n",
    "def best_after_path(row):\n",
    "    # Take the latest date with clouds < 10%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"], row[\"paths_after\"]):\n",
    "        if clouds_nodata < 10:\n",
    "            return path\n",
    "    # Take the latest date with clouds < 30%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"], row[\"paths_after\"]):\n",
    "        if clouds_nodata < 30:\n",
    "            return path\n",
    "    # Take the latest date with clouds < 100%\n",
    "    for clouds_nodata, path in zip(row[\"clouds_nodata_after\"], row[\"paths_after\"]):\n",
    "        if clouds_nodata < 100:\n",
    "            return path\n",
    "    # Else return the newest path\n",
    "    return row[\"paths_after\"][-1]\n",
    "    \n",
    "df[\"path_best_before\"] = df[[\"paths_before\", \"clouds_nodata_before\"]].apply(lambda row: best_before_path(row), axis=1)\n",
    "df[\"path_best_after\"] = df[[\"paths_after\", \"clouds_nodata_after\"]].apply(lambda row: best_after_path(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a4fa86-6d7f-4dde-90b7-eb80a5684103",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_paths = []\n",
    "deforest_pxs = []\n",
    "for event_id in df[\"event_id\"].tolist():\n",
    "    label_path = glob.glob(f\"data/{event_id}/*_mask.tif\")[0]\n",
    "    label_paths.append(label_path)\n",
    "    label = tifffile.imread(mask_path)\n",
    "    deforest_pxs.append((label == 255).sum())\n",
    "df[\"deforest_pxs\"] = deforest_pxs\n",
    "df[\"label_path\"] = label_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51eea01-448d-4241-8d5e-9e890d868e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c6a7bd-d94d-4b21-a293-2dcbc690bc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 22222\n",
    "while True:\n",
    "    np.random.seed(seed)\n",
    "    folds = [0, 1, 2, 3, 4] * (len(df) // 5 + 1) #np.random.choice([0, 1, 2, 3, 4], size=len(df))\n",
    "    np.random.shuffle(folds)\n",
    "    df[\"fold\"] = folds[:len(df)]\n",
    "    # print(df[\"fold\"].value_counts())\n",
    "    std_counts = df.groupby(\"fold\")[\"deforest_pxs\"].sum().std().item()\n",
    "    # print(f'Seed {seed}: {df.groupby(\"fold\")[\"deforest_pxs\"].sum()}, Std = {std_counts:.0f}')\n",
    "    if std_counts < 1500:\n",
    "        break\n",
    "    seed += 1\n",
    "os.makedirs(\"catalogues\", exist_ok=True)\n",
    "df.to_pickle(\"catalogues/2025_08_18_s2_single.pkl\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c94dc7-aacd-4b56-a5a4-156529433f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499636dc-3fff-490b-85f6-208c56be3b49",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dcdbd8-54a7-404a-bc69-aacd6737e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_nb = \"S2_single_0-2\"\n",
    "path = \"catalogues/2025_08_18_s2_single.pkl\"\n",
    "hps_dict = {\n",
    "    ############\n",
    "    # Data\n",
    "    ############\n",
    "    \"df_path\": path,\n",
    "\n",
    "    ############\n",
    "    # Training\n",
    "    ############\n",
    "\n",
    "    ## Experiment Setup\n",
    "    \"name\": f\"Exp{exp_nb}\",\n",
    "\n",
    "    ## Model\n",
    "    \"num_classes\": 1,\n",
    "    \"input_channel\": 13,\n",
    "    \"backbone\": \"timm_efficientnet_b1\",\n",
    "    \"pretrained\": 1,\n",
    "    \"model\": \"unetplusplus\",\n",
    "\n",
    "    # Training Setup\n",
    "#     \"resume\": \"trained_models/Exp7-4/fold_0/2022-02-05_23-13-25/best_metric_18_0.9768.pt\",\n",
    "    \"print_freq\": 1,\n",
    "    \"use_fp16\": 0,\n",
    "    \"patience\": 2, #5,\n",
    "    \"epoch_start_scheduler\": 5,\n",
    "\n",
    "    # Optimizer\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 0.0,\n",
    "\n",
    "    ## Data Augmentation on CPU\n",
    "    \"train_crop_size\": 400,\n",
    "    \"train_batch_size\": 4, #32\n",
    "    \"cutmix_alpha\": 0,\n",
    "    \"da_brightness_magnitude\": 0.0,\n",
    "    \"da_contrast_magnitude\": 0.0,\n",
    "\n",
    "    # Data Augmentation on GPU\n",
    "    \"gpu_da_params\": [0.25],\n",
    "    \n",
    "    ### Loss, Metric\n",
    "    \"alpha\": 0.5,\n",
    "#     \"loss\": \"lovasz\",\n",
    "           }\n",
    "\n",
    "for fold_nb in range(5):\n",
    "    hps = HyperParams(**hps_dict)\n",
    "    hps.fold_nb = fold_nb\n",
    "    # num_batches = 50 // (hps.train_batch_size * torch.cuda.device_count())\n",
    "    num_batches = 10\n",
    "    hps.num_batches = num_batches\n",
    "    train_dataset, train_loader, val_dataset, val_loader = get_dataloaders(hps)\n",
    "    # continue\n",
    "\n",
    "    best_metric, best_metric_epoch = train(hps, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715840f6-c2b7-4661-a4ee-b939f4a96843",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in range(100):\n",
    "    print(np.unique(train_dataset[k][\"mask\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3b10e1-552f-4661-95c5-2eb0daebe408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
