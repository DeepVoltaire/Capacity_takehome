{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33426877-b84c-4e2b-bc76-d68bbd76d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tifffile pandas scikit-learn matplotlib geopandas rasterio albumentations segmentation_models_pytorch#opencv-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c43a140-a617-489b-ae20-3818d5ee541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "import tifffile\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 250)\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import shape\n",
    "import glob\n",
    "from shapely.geometry import box\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "import json\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "red_to_green = LinearSegmentedColormap.from_list(\"RedGreen\", [\"red\", \"green\"])\n",
    "\n",
    "from src.visualize import visualize_s2_concat_bands_path, visualize_s1_path, scale_img, visualize_ls_concat_bands_path\n",
    "from src.visualize import visualize_cb_mux_path, visualize_cb_wpm_path\n",
    "# from src.visualize import visualize_s2_concat_bands_path_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79964fc1-db1e-4bcc-82be-c2db8cb805d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# from pathlib import Path\n",
    "\n",
    "# for src_folder in glob.glob(\"1*\"):\n",
    "#     event_id = src_folder.split(\"/\")[-1]\n",
    "#     dst_folder = Path(f\"data/{event_id}\")  # will be created if it doesn't exist\n",
    "\n",
    "#     # copies entire directory tree; preserves metadata (copy2)\n",
    "#     shutil.copytree(\n",
    "#         src_folder,\n",
    "#         dst_folder,\n",
    "#         dirs_exist_ok=True,                     # requires Python 3.8+\n",
    "#         copy_function=shutil.copy2,             # preserve mtime/permissions\n",
    "#         symlinks=False,                         # set True to keep symlinks as symlinks\n",
    "#         ignore=shutil.ignore_patterns('__pycache__', '*.pyc', '.DS_Store')\n",
    "#     )\n",
    "\n",
    "# for src_folder in glob.glob(\"1*\"):\n",
    "#     shutil.rmtree(src_folder)\n",
    "\n",
    "# for path in glob.glob(\"README_*\"):\n",
    "#     shutil.move(path, f\"data/{path}\")\n",
    "# shutil.move(\"DATASET_OVERVIEW.md\", f\"data/DATASET_OVERVIEW.md\")\n",
    "# shutil.move(\"dataset_analysis.json\", \"data/dataset_analysis.json\")\n",
    "# shutil.move(\"mapbiomas_alerts.geojson\", \"data/mapbiomas_alerts.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f580886c-1612-40ce-9c25-25bce8c505f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/dataset_analysis.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    dataset_analysis = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2db593-3844-469c-852c-755aa65d91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_file(\"data/mapbiomas_alerts.geojson\")\n",
    "events_sorted_by_area = df.sort_values(\"areaHa\", ascending=False)[\"alertCode\"].tolist()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2672ce-d8de-4a60-b310-406396a42989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# json.loads(open(\"data/1389384/sentinel2/1389384_sentinel2_S2-16D_V2_018015_20250218_20250218_bands.json\").read())\n",
    "\n",
    "# alertCode = \"1389384\"\n",
    "# with rasterio.open(f\"data/1389384/sentinel2/1389384_sentinel2_S2-16D_V2_018015_20250218_20250218.tif\") as src:\n",
    "#     data = src.read()  # Shape: (bands, height, width)\n",
    "# data.shape\n",
    "\n",
    "# event_ids = [k.split(\"/\")[-1] for k in sorted(glob.glob(\"data/1*\"))]\n",
    "# len(event_ids)#, event_ids\n",
    "\n",
    "# nothing_count = 0\n",
    "# for event_id in events_sorted_by_area:\n",
    "# # for event_id in [k.split(\"/\")[-1] for k in glob.glob(f\"data/*\")]:\n",
    "#     if len(glob.glob(f\"data/{event_id}/*/*.tif\")) == 0:\n",
    "#         print(f\"data/{event_id}/*/*.tif\")\n",
    "#         nothing_count += 1\n",
    "#     if len(glob.glob(f\"data/{event_id}/*/*.tif\")) < 10:\n",
    "#         print(glob.glob(f\"data/{event_id}/*/*.tif\"))\n",
    "#     # else:\n",
    "#     #     print(glob.glob(f\"data/{event_id}/*/*.tif\"))\n",
    "# nothing_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19dd285-57b7-4e1b-a019-c7a4a8c5c35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event_id in events_sorted_by_area[2:]:\n",
    "    s1_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel1/*.tif\") if not \"_nrpb\" in k and not \"_rfdi\" in k and not \"_rvi\" in k and not \"_vv_vh_ratio\" in k and not \"uncompressed\" in k])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36215d8c-25c2-41a0-a27e-e14bc7375c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c95fbe4-fb16-4788-9536-860761fe32e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.min(), img.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c15679-4a6f-486b-9eeb-5d1ff15299cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/1389572/sentinel1/1389572_sentinel1_S1A_IW_GRDH_1SDV_20250509T090650_20250509T090715_059115_0755A7_20250509.tif'\n",
    "img = rasterio.open(path.replace(\".tif\", \"_rvi.tif\")).read(1)\n",
    "plt.imshow(img, vmin=10000, vmax=20000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b2b19e-1da2-4692-a0d5-4413acde4610",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.min(), img.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca85d87-45fd-4cd0-a894-7e619b6acecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = rasterio.open(path.replace(\".tif\", \"_rfdi.tif\")).read(1)\n",
    "plt.imshow(img, vmin=0, vmax=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e89095-e507-4c1b-8d7f-40f5d7605a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = rasterio.open('data/1389384/sentinel1/1389384_sentinel1_S1A_IW_GRDH_1SDV_20250406T093308_20250406T093333_058634_07424A_20250406_nrpb.tif').read(1)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d0342e-17fd-43bf-af3a-5d2b31e50dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ax_empty(ax):\n",
    "    \"\"\"Check if a matplotlib axis is empty.\"\"\"\n",
    "    return not (ax.lines or ax.collections or ax.patches or ax.images)\n",
    "\n",
    "for event_id in events_sorted_by_area[:]:\n",
    "    s2_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel2/*L2A*.tif\") if not \"ndvi\" in k and not \"ndwi\" in k and not \"uncompressed\" in k])\n",
    "    s2_16d_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel2/*16D*.tif\") if not \"ndvi\" in k and not \"ndwi\" in k and not \"uncompressed\" in k])\n",
    "    s1_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel1/*.tif\") if not \"_nrpb\" in k and not \"_rfdi\" in k and not \"_rvi\" in k and not \"_vv_vh_ratio\" in k and not \"uncompressed\" in k])\n",
    "    ls_paths = sorted([k for k in glob.glob(f\"data/{event_id}/landsat/*_L2SP_*.tif\") if not \"_nbr\" in k and not \"_ndwi\" in k and not \"_ndvi\" in k])\n",
    "    cb_wpm_paths = sorted([k for k in glob.glob(f\"data/{event_id}/cbers4a/*_WPM_*.tif\") if not \"_ndvi\" in k])\n",
    "    cb_mux_paths = sorted([k for k in glob.glob(f\"data/{event_id}/cbers4a/*_MUX_*.tif\") if not \"_ndvi\" in k])\n",
    "    print(f\"{event_id}: {len(s2_paths):2} S2 L2A, {len(s2_16d_paths):2} S2 16D,{len(s1_paths):2} S1, {len(ls_paths):2} LS L2SP, {len(cb_wpm_paths):2} CB WPM, {len(cb_mux_paths):2} CB MUX\")\n",
    "    paths = s2_paths + s1_paths + ls_paths + cb_wpm_paths + cb_mux_paths + s2_16d_paths\n",
    "    if len(paths) == 0:\n",
    "        continue\n",
    "    mask_path = glob.glob(f\"data/{event_id}/*_mask.tif\")[0]\n",
    "    \n",
    "    # Sort by the date right before the .tif\n",
    "    paths = sorted(paths, key=lambda p: datetime.strptime(p.split('_')[-1].split('.')[0], \"%Y%m%d\"))\n",
    "    imgs_to_show, cloud_masks_to_show = [], []\n",
    "    titles = []\n",
    "    for path in paths:\n",
    "        img_orig = tifffile.imread(path)\n",
    "        date = path.split(\"/\")[-1].split('.')[0].split('_')[-1]\n",
    "        date_str = f\"{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "        if \"L2A\" in path:\n",
    "            scl = img_orig[:, :, -2].copy()\n",
    "            img_orig = img_orig[:, :, [11, 8, 4]]\n",
    "            img_orig = img_orig.astype(np.float32)\n",
    "            if scl.max() > 11:\n",
    "                if scl.max() > 50:\n",
    "                    factor = 6\n",
    "                elif scl.max() > 45:\n",
    "                    factor = 5\n",
    "                elif scl.max() >= 40:\n",
    "                    factor = 4.5\n",
    "                else:\n",
    "                    factor = 4\n",
    "                img_orig = (img_orig / factor).round(0)\n",
    "                scl_clean = (scl / factor).round(0).astype('uint8')   # back to 0..11\n",
    "            else:\n",
    "                factor = 1\n",
    "                scl_clean = scl.copy()\n",
    "        \n",
    "            img_orig[img_orig == 0] = np.nan\n",
    "\n",
    "            # low values, but all minimum above 1000 ==> Subtract by 1000  (undo the PB 04.X radiometric offset)\n",
    "            if (np.nanmin(img_orig, axis=(0,1)) > 1000).sum() == 3:\n",
    "                img_orig = img_orig - 1000\n",
    "                img_orig[img_orig < 0] = np.nan\n",
    "            \n",
    "            # if scl.max() > 11:\n",
    "            #     f, ax = plt.subplots(1, 3, figsize=(25, 8))\n",
    "            #     img = visualize_s2_concat_bands_path(path)\n",
    "            #     ax[0].imshow(img)\n",
    "            #     ax[1].imshow(scl, vmin=0, vmax=11)\n",
    "            #     ax[1].set_title(f\"Tampered values: {np.unique(scl)}\", fontsize=16)\n",
    "            #     ax[2].imshow(scl_clean, vmin=0, vmax=11)\n",
    "            #     ax[2].set_title(f\"Dividing by {factor}: {np.unique(scl_clean)}\", fontsize=16)\n",
    "            #     plt.tight_layout()\n",
    "            #     plt.show()\n",
    "\n",
    "            min_ = np.nanmin(img_orig, axis=(0,1)).round(0)\n",
    "            max_ = np.nanmax(img_orig, axis=(0,1)).round(0)\n",
    "            mean_ = np.nanmean(img_orig, axis=(0,1)).round(0)\n",
    "            \n",
    "            cloudmask = np.isin(scl_clean, [8, 9, 10]) # clouds only\n",
    "\n",
    "            clouds_perc = 100 * cloudmask.sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "            nodata_perc = 100 * (img_orig[:, :, 2] == 0).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "            s2_sat = path.split(\"/\")[-1].split(\"_\")[2]\n",
    "            print(f\"S2 {s2_sat} {date_str}: {clouds_perc:5.1f}% Clouds, {nodata_perc:5.1f}% Nodata\")\n",
    "            img = visualize_s2_concat_bands_path(path)\n",
    "            titles.append(f\"S2 {s2_sat}: {date_str} - factor {factor}\\n{clouds_perc:.1f}% Clouds, {nodata_perc:.1f}% Nodata\\nMin: {min_}, Max: {max_}\\nMean: {mean_}\")\n",
    "        elif \"16D\" in path:\n",
    "            scl = img_orig[:, :, -1].copy()\n",
    "            img_orig = img_orig[:, :, [10, 7, 3]]\n",
    "            img_orig = img_orig.astype(np.float32)\n",
    "            if scl.max() > 11:\n",
    "                if scl.max() > 50:\n",
    "                    factor = 6\n",
    "                elif scl.max() > 45:\n",
    "                    factor = 5\n",
    "                elif scl.max() >= 40:\n",
    "                    factor = 4.5\n",
    "                else:\n",
    "                    factor = 4\n",
    "                img_orig = (img_orig / factor).round(0)\n",
    "                scl_clean = (scl / factor).round(0).astype('uint8')   # back to 0..11\n",
    "            else:\n",
    "                factor = 1\n",
    "                scl_clean = scl.copy()\n",
    "                \n",
    "            img_orig[img_orig == 0] = np.nan\n",
    "            # low values, but all minimum above 1000 ==> Subtract by 1000 (undo the PB 04.X radiometric offset)\n",
    "            if (np.nanmin(img_orig, axis=(0,1)) > 1000).sum() == 3:\n",
    "                img_orig = img_orig - 1000\n",
    "                img_orig[img_orig < 0] = np.nan\n",
    "            min_ = np.nanmin(img_orig, axis=(0,1)).round(0)\n",
    "            max_ = np.nanmax(img_orig, axis=(0,1)).round(0)\n",
    "            mean_ = np.nanmean(img_orig, axis=(0,1)).round(0)\n",
    "            \n",
    "            # if scl.max() > 11:\n",
    "            #     f, ax = plt.subplots(1, 3, figsize=(25, 8))\n",
    "            #     img = visualize_s2_concat_bands_path(path)\n",
    "            #     ax[0].imshow(img)\n",
    "            #     ax[1].imshow(scl, vmin=0, vmax=11)\n",
    "            #     ax[1].set_title(f\"Tampered values: {np.unique(scl)}\", fontsize=16)\n",
    "            #     ax[2].imshow(scl_clean, vmin=0, vmax=11)\n",
    "            #     ax[2].set_title(f\"Dividing by {factor}: {np.unique(scl_clean)}\", fontsize=16)\n",
    "            #     plt.tight_layout()\n",
    "            #     plt.show()\n",
    "            cloudmask = np.isin(scl_clean, [8, 9, 10]) # clouds only\n",
    "            clouds_perc = 100 * cloudmask.sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "            nodata_perc = 100 * (img_orig[:, :, 1] == 0).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "            print(f\"S2 16D {date_str}: {clouds_perc:5.1f}% Clouds, {nodata_perc:5.1f}% Nodata\")\n",
    "            img = visualize_s2_concat_bands_path(path)\n",
    "            titles.append(f\"S2 16D: {date_str} - factor {factor}\\n{clouds_perc:.1f}% Clouds, {nodata_perc:.1f}% Nodata\\nMin: {min_}, Max: {max_}\\nMean: {mean_}\")\n",
    "        elif \"_L2SP\" in path:\n",
    "            min_ = img_orig[:, :, [7, 3, 0]].min(axis=(0,1)).round(0)\n",
    "            max_ = img_orig[:, :, [7, 3, 0]].max(axis=(0,1)).round(0)\n",
    "            mean_ = img_orig[:, :, [7, 3, 0]].mean(axis=(0,1)).round(0)\n",
    "            \n",
    "            qa = img_orig[:, :, -5].copy()\n",
    "            # bit positions for dilated, cirrus, cloud\n",
    "            CLOUD_BITS = (1, 2, 3)\n",
    "            cloudmask = np.zeros_like(qa, dtype=bool)\n",
    "            for b in CLOUD_BITS:\n",
    "                cloudmask |= np.bitwise_and(qa, 1 << b) != 0\n",
    "            cloudmask = cloudmask.astype(np.uint8)\n",
    "\n",
    "            clouds_perc = 100 * (cloudmask.sum() / (img_orig.shape[0] * img_orig.shape[1]))\n",
    "            nodata_perc = 100 * (img_orig[:, :, 2] == 0).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "            print(f\"LS     {date_str}: {clouds_perc:5.1f}% Clouds, {nodata_perc:5.1f}% Nodata\")\n",
    "            img = visualize_ls_concat_bands_path(path)\n",
    "            titles.append(f\"LS: {date_str}\\n{clouds_perc:.1f}% Clouds, {nodata_perc:.1f}% Nodata\\nMin: {min_}, Max: {max_}\\nMean: {mean_}\")\n",
    "        elif \"_WPM_\" in path:\n",
    "            min_ = img_orig.min(axis=(0,1)).round(0)\n",
    "            max_ = img_orig.max(axis=(0,1)).round(0)\n",
    "            mean_ = img_orig.mean(axis=(0,1)).round(0)\n",
    "\n",
    "            cloudmask = (img_orig[:, :, 2] > 35000).astype(np.uint8)\n",
    "            clouds_perc = 100 * cloudmask.sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "            nodata_perc = 100 * (img_orig[:, :, 0] == 0).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "            print(f\"WPM    {date_str}: {clouds_perc:5.1f}% Clouds, {nodata_perc:5.1f}% Nodata\")\n",
    "            img = visualize_cb_wpm_path(path)\n",
    "            titles.append(f\"WPM: {date_str}\\n{clouds_perc:.1f}% Clouds, {nodata_perc:.1f}% Nodata\\nMin: {min_}, Max: {max_}\\nMean: {mean_}\")\n",
    "        elif \"_MUX_\" in path:\n",
    "            min_ = img_orig[:, :, [3, 2, 1]].min(axis=(0,1)).round(0)\n",
    "            max_ = img_orig[:, :, [3, 2, 1]].max(axis=(0,1)).round(0)\n",
    "            mean_ = img_orig[:, :, [3, 2, 1]].mean(axis=(0,1)).round(0)\n",
    "\n",
    "            cloudmask = (img_orig[:, :, 0] > 60).astype(np.uint8)\n",
    "            clouds_perc = 100 * cloudmask.sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "            nodata_perc = 100 * (img_orig[:, :, 0] == 0).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "            print(f\"MUX    {date_str}: {clouds_perc:5.1f}% Clouds, {nodata_perc:5.1f}% Nodata\")\n",
    "            img = visualize_cb_mux_path(path)\n",
    "            titles.append(f\"MUX: {date_str}\\n{clouds_perc:.1f}% Clouds, {nodata_perc:.1f}% Nodata\\nMin: {min_}, Max: {max_}\\nMean: {mean_}\")\n",
    "        else:\n",
    "            min_ = img_orig.min(axis=(0,1)).round(0)\n",
    "            max_ = img_orig.max(axis=(0,1)).round(0)\n",
    "            mean_ = img_orig.mean(axis=(0,1)).round(0)\n",
    "            cloudmask = np.zeros_like(img_orig[:, :, 0], dtype=np.uint8)\n",
    "            nodata_perc = 100 * (img_orig[:, :, 0] == 0).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "            print(f\"S1     {date_str}: {nodata_perc:5.1f}% Nodata\")\n",
    "            img = visualize_s1_path(path)\n",
    "            titles.append(f\"S1: {date_str}\\n{nodata_perc:.1f}% Nodata\\nMin: {min_}, Max: {max_}\\nMean: {mean_}\")\n",
    "        imgs_to_show.append(img)\n",
    "        cloud_masks_to_show.append(cloudmask)\n",
    "\n",
    "    if len(imgs_to_show) > 0:\n",
    "        rows = (len(imgs_to_show) // 5) + 1\n",
    "        cols =  (len(imgs_to_show) // rows) + 1\n",
    "        f, ax = plt.subplots(rows, max(2, cols), figsize=(30, 7 * rows))\n",
    "        ax = ax.flatten()\n",
    "        for j in range(len(imgs_to_show)):\n",
    "            ax[j].imshow(imgs_to_show[j])\n",
    "            ax[j].imshow(np.ma.masked_where(cloud_masks_to_show[j] != 1, cloud_masks_to_show[j]), cmap=\"spring\", alpha=0.5)\n",
    "            ax[j].set_title(titles[j], fontsize=15)\n",
    "\n",
    "        mask = rasterio.open(mask_path).read(1)\n",
    "        date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "        date_str = f\"{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "        if mask.shape[0] != img.shape[1]:\n",
    "            mask = cv2.resize(mask, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "        ax[j+1].imshow(img)\n",
    "        ax[j+1].imshow(np.ma.masked_where(mask != 255, mask), cmap=\"cool\" if \"_MUX_\" in path else \"autumn\", alpha=0.5)\n",
    "        ax[j+1].set_title(f\"Deforestation date: {date_str}\", fontsize=15)\n",
    "        # Hide empty axes\n",
    "        for ax_idx in range(len(ax)):\n",
    "            if is_ax_empty(ax[ax_idx]):\n",
    "                ax[ax_idx].set_visible(False)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d87e8a-526d-4d80-94e7-e6b7efb21cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # S2 16D IS SUPER MESSY, DONT USE\n",
    "# def is_ax_empty(ax):\n",
    "#     \"\"\"Check if a matplotlib axis is empty.\"\"\"\n",
    "#     return not (ax.lines or ax.collections or ax.patches or ax.images)\n",
    "\n",
    "# max_cloud_nodata = 100\n",
    "# for event_id in events_sorted_by_area[:]:\n",
    "#     # rand_int = np.random.randint(len(events_sorted_by_area))\n",
    "#     # rand_int = k\n",
    "#     # event_id = events_sorted_by_area[rand_int]\n",
    "#     s2_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel2/*L2A*_ndvi.tif\")])\n",
    "#     s2_16d_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel2/*16D*_ndvi.tif\")])\n",
    "#     s1_paths = sorted([k for k in glob.glob(f\"data/{event_id}/sentinel1/*.tif\") if not \"_nrpb\" in k and not \"_rfdi\" in k and not \"_rvi\" in k and not \"_vv_vh_ratio\" in k])\n",
    "#     ls_paths = sorted([k for k in glob.glob(f\"data/{event_id}/landsat/*_L2SP_*ndvi.tif\")])\n",
    "#     cb_wpm_paths = sorted([k for k in glob.glob(f\"data/{event_id}/cbers4a/*_WPM_*_ndvi.tif\")])\n",
    "#     cb_mux_paths = sorted([k for k in glob.glob(f\"data/{event_id}/cbers4a/*_MUX_*_ndvi.tif\")])\n",
    "#     print(f\"{event_id}: {len(s2_paths)} S2 L2A, {len(s1_paths)} S1, {len(ls_paths)} LS L2SP, {len(cb_wpm_paths)} CB WPM, {len(cb_mux_paths)} CB MUX\")\n",
    "#     paths = s2_paths + s1_paths + ls_paths + cb_wpm_paths + cb_mux_paths + s2_16d_paths\n",
    "#     if len(paths) == 0:\n",
    "#         continue\n",
    "#     mask_path = glob.glob(f\"data/{event_id}/*_mask.tif\")[0]\n",
    "    \n",
    "#     # Sort by the date right before the .tif\n",
    "#     paths = sorted(paths, key=lambda p: datetime.strptime(p.replace(\"_ndvi\", \"\").split('_')[-1].split('.')[0], \"%Y%m%d\"))\n",
    "#     imgs_to_show = []\n",
    "#     titles = []\n",
    "#     for path in paths:\n",
    "#         img_orig = rasterio.open(path).read()\n",
    "#         date = path.replace(\"_ndvi\", \"\").split(\"/\")[-1].split('.')[0].split('_')[-1]\n",
    "#         date_str = f\"{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "#         if \"L2A\" in path:\n",
    "#             img_orig = img_orig[0]\n",
    "#             # print(img_orig[:, :, 1:-2].min(axis=(0, 1)), img_orig[:, :, 1:-2].max(axis=(0, 1)))\n",
    "#             nodata_perc = 100 * (img_orig == 0).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "#             what_to_do_string = \"SKIP\"\n",
    "#             if nodata_perc < max_cloud_nodata:\n",
    "#                 what_to_do_string = \"NO SKIP\"\n",
    "#             print(f\"S2 L2A {date_str}: {nodata_perc:5.1f}% Nodata --> {what_to_do_string}\")\n",
    "#             if nodata_perc > max_cloud_nodata:\n",
    "#                 continue\n",
    "#             # img = visualize_s2_concat_bands_path(path)\n",
    "#             ending = path.split(\"/\")[-1].split('.')[0].split('_')[-1]\n",
    "#             titles.append(f\"S2 L2A: {date_str}\\n{nodata_perc:.1f}% Nodata\")\n",
    "#         elif \"16D\" in path:\n",
    "#             img_orig = img_orig[0]\n",
    "#             # clouds_perc = 100 * (img_orig[:, :, 1] > 2400).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "#             nodata_perc = 100 * (img_orig == 0).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "#             what_to_do_string = \"SKIP\"\n",
    "#             if nodata_perc < max_cloud_nodata:\n",
    "#                 what_to_do_string = \"NO SKIP\"\n",
    "#             print(f\"S2 16D {date_str}: {nodata_perc:5.1f}% Nodata --> {what_to_do_string}\")\n",
    "#             if nodata_perc > max_cloud_nodata:\n",
    "#                 continue\n",
    "#             # img = visualize_s2_concat_bands_path(path)\n",
    "#             ending = path.split(\"/\")[-1].split('.')[0].split('_')[-1]\n",
    "#             titles.append(f\"S2 16D: {date_str}\\n{nodata_perc:.1f}% Nodata\")\n",
    "#         elif \"_L2SP\" in path:\n",
    "#             img_orig = img_orig[0]\n",
    "#             nodata_perc = 100 * (img_orig == 0).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "#             what_to_do_string = \"SKIP\"\n",
    "#             if clouds_perc + nodata_perc < max_cloud_nodata:\n",
    "#                 what_to_do_string = \"NO SKIP\"\n",
    "#             print(f\"LS     {date_str}: {nodata_perc:5.1f}% Nodata --> {what_to_do_string}\")\n",
    "#             if nodata_perc > max_cloud_nodata:\n",
    "#                 continue\n",
    "#             # img = visualize_ls_concat_bands_path(path)\n",
    "#             ending = path.split(\"/\")[-1].split('.')[0].split('_')[-1]\n",
    "#             titles.append(f\"LS: {date_str}\\n{nodata_perc:.1f}% Nodata\")\n",
    "#         elif \"_WPM_\" in path:\n",
    "#             img_orig = img_orig[0]\n",
    "#             # clouds_perc = 100 * (img_orig[:, :, 2] > 35000).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "#             nodata_perc = 100 * (img_orig == 0).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "#             what_to_do_string = \"SKIP\"\n",
    "#             if nodata_perc < max_cloud_nodata:\n",
    "#                 what_to_do_string = \"NO SKIP\"\n",
    "#             print(f\"WPM    {date_str}: {nodata_perc:5.1f}% Nodata --> {what_to_do_string}\")\n",
    "#             if nodata_perc > max_cloud_nodata:\n",
    "#                 continue\n",
    "#             # img = visualize_cb_wpm_path(path)\n",
    "#             ending = path.split(\"/\")[-1].split('.')[0].split('_')[-1]\n",
    "#             titles.append(f\"WPM: {date_str}\\n{nodata_perc:.1f}% Nodata\")\n",
    "#         elif \"_MUX_\" in path:\n",
    "#             img_orig = img_orig[0]\n",
    "#             # clouds_perc = 100 * (img_orig[:, :, 0] > 55).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "#             nodata_perc = 100 * (img_orig == 0).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "#             what_to_do_string = \"SKIP\"\n",
    "#             if nodata_perc < max_cloud_nodata:\n",
    "#                 what_to_do_string = \"NO SKIP\"\n",
    "#             print(f\"MUX    {date_str}: {nodata_perc:5.1f}% Nodata --> {what_to_do_string}\")\n",
    "#             if nodata_perc > max_cloud_nodata:\n",
    "#                 continue\n",
    "#             # img = visualize_cb_mux_path(path)\n",
    "#             ending = path.split(\"/\")[-1].split('.')[0].split('_')[-1]\n",
    "#             titles.append(f\"MUX: {date_str}\\n{nodata_perc:.1f}% Nodata\")\n",
    "#         else:\n",
    "#             if img_orig.shape[0] < 15:\n",
    "#                 img_orig = np.transpose(img_orig, (1, 2, 0))\n",
    "#             nodata_perc = 100 * (img_orig[:, :, 0] == 0).sum() / (img_orig.shape[0] * img_orig.shape[1])\n",
    "#             what_to_do_string = \"SKIP\"\n",
    "#             if nodata_perc < max_cloud_nodata:\n",
    "#                 what_to_do_string = \"NO SKIP\"\n",
    "#             date = path.split(\"/\")[-1].split('.')[0].split('_')[-1]\n",
    "#             date_str = f\"{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "#             print(f\"S1     {date_str}: {nodata_perc:5.1f}% Nodata --> {what_to_do_string}\")\n",
    "#             if nodata_perc > max_cloud_nodata:\n",
    "#                 continue\n",
    "#             img = visualize_s1_path(path)\n",
    "#             ending = path.split(\"/\")[-1].split('.')[0].split('_')[-1]\n",
    "#             titles.append(f\"S1: {date_str}\\n{nodata_perc:.1f}% Nodata\")\n",
    "#             imgs_to_show.append(img)\n",
    "#             continue\n",
    "#         print(img_orig.min(), img_orig.max(), img_orig.mean())\n",
    "#         imgs_to_show.append(img_orig)\n",
    "\n",
    "#     if len(imgs_to_show) > 0:\n",
    "#         rows = (len(imgs_to_show) // 5) + 1\n",
    "#         cols =  (len(imgs_to_show) // rows) + 1\n",
    "#         f, ax = plt.subplots(rows, max(2, cols), figsize=(30, 7 * rows))\n",
    "#         ax = ax.flatten()\n",
    "#         for j in range(len(imgs_to_show)):\n",
    "#             if \"S1\" in titles[j]:\n",
    "#                 ax[j].imshow(imgs_to_show[j])\n",
    "#             else:\n",
    "#                 ax[j].imshow(imgs_to_show[j], vmin=-2000, vmax=6000, cmap=red_to_green)\n",
    "#             ax[j].set_title(titles[j], fontsize=15)\n",
    "\n",
    "#         mask = rasterio.open(mask_path).read(1)\n",
    "#         date = mask_path.split(\"/\")[-1].split('_')[1]\n",
    "#         date_str = f\"{date[:4]}-{date[4:6]}-{date[6:8]}\"\n",
    "#         if mask.shape[0] != img.shape[1]:\n",
    "#             import cv2\n",
    "#             mask = cv2.resize(mask, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "#         ax[j+1].imshow(img)\n",
    "#         # plt.imshow(np.ma.masked_where(mask != 0, mask), cmap=\"winter\", alpha=0.5)\n",
    "#         ax[j+1].imshow(np.ma.masked_where(mask != 255, mask), cmap=\"cool\" if \"_MUX_\" in path else \"autumn\", alpha=0.5)\n",
    "#         ax[j+1].set_title(f\"{date_str}\", fontsize=15)\n",
    "#         # Hide empty axes\n",
    "#         for ax_idx in range(len(ax)):\n",
    "#             if is_ax_empty(ax[ax_idx]):  # type: ignore\n",
    "#                 ax[ax_idx].set_visible(False)  # type: ignore\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852a7386-472a-473f-89d9-01ae0ab50831",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mask_path in glob.glob(f\"data/*/*_mask.tif\"):\n",
    "    mask = tifffile.imread(mask_path)\n",
    "    print(mask_path, np.unique(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d797ad-7721-4ebe-91eb-7016e5760814",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdc1566-7f96-4b8e-b886-7d2f4bb92ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27a745b-8328-4bfc-b582-ed8f1fc203c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [\n",
    "    \"AOT\",\n",
    "    \"B01\",\n",
    "    \"B02\",\n",
    "    \"B03\",\n",
    "    \"B04\",\n",
    "    \"B05\",\n",
    "    \"B06\",\n",
    "    \"B07\",\n",
    "    \"B08\",\n",
    "    \"B09\",\n",
    "    \"B11\",\n",
    "    \"B12\",\n",
    "    \"B8A\",\n",
    "    \"SCL\",\n",
    "    \"WVP\"\n",
    "]\n",
    "bands.index(\"B12\"), bands.index(\"B08\"), bands.index(\"B04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbc6184-9fc1-419c-a62c-60f0c2fc320e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts = {}\n",
    "for count_type in [\"s2_l2a\", \"s2_l2a_ndvi\", \"s2_l2a_gndvi\", \"s2_l2a_ndwi\", \"s2_16d\", \"s2_16d_ndvi\", \"s2_16d_gndvi\", \"s2_16d_ndwi\"]:\n",
    "    counts[count_type] = 0\n",
    "no_s1 = 0\n",
    "for event_id in event_ids:\n",
    "    print(f\"{event_id}\")\n",
    "    s2_paths = glob.glob(f\"data/{event_id}/sentinel2/*\")\n",
    "    tif_paths = glob.glob(f\"data/{event_id}/sentinel2/*.tif\")\n",
    "    if len([k for k in tif_paths if \"_MSIL2A_\" in k]) > 0:\n",
    "        counts[\"s2_l2a\"] += 1\n",
    "    # not \"ndvi\" in k and not \"ndwi\" in k\n",
    "    print(f\"   S2: {len(s2_paths):2} paths\")\n",
    "    s1_paths = glob.glob(f\"data/{event_id}/sentinel1/*.tif\")\n",
    "    if len(s1_paths) == 0:\n",
    "        no_s1 += 1\n",
    "    print(f\"   S1: {len(s1_paths):2} paths\")\n",
    "    # print(s1_paths)\n",
    "    # ls_paths = glob.glob(f\"data/{event_id}/landsat/*\")\n",
    "    # print(f\"   LS: {len(ls_paths):2} paths\")\n",
    "    # cb_paths = glob.glob(f\"data/{event_id}/cbers4a/*\")\n",
    "    # print(f\"   CB: {len(cb_paths):2} paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b59b42-3a51-4725-a65d-82da4f312d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[\"s2_l2a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0922a9b3-e167-4e64-b289-3a729feb288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Iterable, Union, Tuple\n",
    "\n",
    "# ---------------------- Config ----------------------\n",
    "BASE_DIR = Path(\"data\")  # change if you want to scan the filesystem\n",
    "SENSORS = [\"sentinel2\", \"sentinel1\", \"landsat\", \"cbers4a\"]\n",
    "\n",
    "# Indices to detect in filenames (lowercase match)\n",
    "INDEX_KEYS = [\"ndvi\", \"gndvi\", \"ndwi\", \"nbr\", \"rvi\", \"rfdi\", \"nrpb\", \"vv_vh_ratio\"]\n",
    "\n",
    "# Optional: pretty labels for combinations\n",
    "SENSOR_ALIASES = {\n",
    "    \"sentinel1\": \"s1\",\n",
    "    \"sentinel2\": \"s2\",\n",
    "    \"landsat\": \"ls\",\n",
    "    \"cbers4a\": \"cbers\",\n",
    "}\n",
    "\n",
    "# ------------------- Helpers ------------------------\n",
    "def is_raster(path: Union[str, Path]) -> bool:\n",
    "    p = str(path).lower()\n",
    "    return p.endswith(\".tif\") and not p.endswith(\".tif.aux.xml\")\n",
    "\n",
    "def get_event_id(path: Union[str, Path]) -> str:\n",
    "    parts = Path(path).parts\n",
    "    try:\n",
    "        i = parts.index(\"data\")\n",
    "        return parts[i + 1]\n",
    "    except (ValueError, IndexError):\n",
    "        m = re.search(r\"data[/\\\\]([^/\\\\]+)[/\\\\]\", str(path))\n",
    "        return m.group(1) if m else \"UNKNOWN\"\n",
    "\n",
    "def get_sensor(path: Union[str, Path]) -> str:\n",
    "    parts = Path(path).parts\n",
    "    try:\n",
    "        i = parts.index(\"data\")\n",
    "        return parts[i + 2]\n",
    "    except (ValueError, IndexError):\n",
    "        m = re.search(r\"data[/\\\\][^/\\\\]+[/\\\\]([^/\\\\]+)[/\\\\]\", str(path))\n",
    "        return m.group(1) if m else \"UNKNOWN\"\n",
    "\n",
    "def detect_index(filename: str) -> Union[str, None]:\n",
    "    base = os.path.basename(filename).lower()\n",
    "    for key in INDEX_KEYS:\n",
    "        if re.search(rf\"(^|[^a-z]){re.escape(key)}($|[^a-z])\", base):\n",
    "            return key\n",
    "    return None\n",
    "\n",
    "def detect_raw_type(sensor: str, filename: str) -> Union[str, None]:\n",
    "    \"\"\"\n",
    "    Heuristics for \"raw\" product-type buckets per sensor family.\n",
    "    Extend/adjust to your naming conventions if needed.\n",
    "    \"\"\"\n",
    "    base = os.path.basename(filename)\n",
    "\n",
    "    if sensor == \"sentinel2\":\n",
    "        if \"S2-16D\" in base:\n",
    "            return \"16D\"\n",
    "        if \"MSIL2A\" in base:\n",
    "            return \"L2A\"\n",
    "        return None\n",
    "\n",
    "    if sensor == \"landsat\":\n",
    "        if \"LANDSAT-16D\" in base:\n",
    "            return \"16D\"\n",
    "        if re.search(r\"L[CT]0[0-9]_L2SP\", base):  # LC08/LC09 L2SP\n",
    "            return \"L2SP\"\n",
    "        return None\n",
    "\n",
    "    if sensor == \"sentinel1\":\n",
    "        if re.search(r\"S1[AB]_IW_GRDH\", base):\n",
    "            return \"GRDH\"\n",
    "        return None\n",
    "\n",
    "    if sensor == \"cbers4a\":\n",
    "        if \"MUX_CBERS_4A_MUX_RAW\" in base:\n",
    "            return \"MUX_RAW\"\n",
    "        if \"WPM_CBERS4A_WPM_PCA\" in base:\n",
    "            return \"WPM_PCA\"\n",
    "        return None\n",
    "\n",
    "    return None\n",
    "\n",
    "def iter_paths(root: Union[str, Path]) -> Iterable[Path]:\n",
    "    root = Path(root)\n",
    "    if root.is_dir():\n",
    "        yield from root.rglob(\"*\")\n",
    "    else:\n",
    "        # If 'root' is a file containing paths, adapt here to read lines\n",
    "        yield root\n",
    "\n",
    "# ------------------- Core logic ---------------------\n",
    "def analyze(paths: Iterable[Union[str, Path]]):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      per_sensor_index_counts: {sensor: Counter({index: count, ...}), ...}\n",
    "      per_sensor_raw_counts:   {sensor: Counter({rawType: count, ...}), ...}\n",
    "      missing_counts:          Counter({sensor: num_events_with_no_data, ...})\n",
    "      missing_event_ids:       {sensor: set(event_ids_with_no_data)}\n",
    "      combo_counts:            Counter({combo_label: num_events})\n",
    "    \"\"\"\n",
    "    per_sensor_index_counts = defaultdict(Counter)\n",
    "    per_sensor_raw_counts = defaultdict(Counter)\n",
    "\n",
    "    # Track whether each (event_id, sensor) has ANY raster at all\n",
    "    has_data = defaultdict(lambda: {s: False for s in SENSORS})\n",
    "    all_event_ids = set()\n",
    "\n",
    "    for p in paths:\n",
    "        p = Path(p)\n",
    "        if not p.is_file():\n",
    "            continue\n",
    "\n",
    "        sensor = get_sensor(p)\n",
    "        event_id = get_event_id(p)\n",
    "        all_event_ids.add(event_id)\n",
    "\n",
    "        if sensor not in SENSORS:\n",
    "            continue\n",
    "\n",
    "        if is_raster(p):\n",
    "            # presence flag\n",
    "            has_data[event_id][sensor] = True\n",
    "\n",
    "            # indices per sensor\n",
    "            idx = detect_index(p.name)\n",
    "            if idx:\n",
    "                per_sensor_index_counts[sensor][idx] += 1\n",
    "\n",
    "            # raw/product type per sensor\n",
    "            raw_type = detect_raw_type(sensor, p.name)\n",
    "            if raw_type:\n",
    "                per_sensor_raw_counts[sensor][raw_type] += 1\n",
    "\n",
    "    # Compute missing events per sensor\n",
    "    missing_event_ids = {s: set() for s in SENSORS}\n",
    "    for ev in all_event_ids:\n",
    "        row = has_data.get(ev, {s: False for s in SENSORS})\n",
    "        for s in SENSORS:\n",
    "            if not row.get(s, False):\n",
    "                missing_event_ids[s].add(ev)\n",
    "\n",
    "    missing_counts = Counter({s: len(missing_event_ids[s]) for s in SENSORS})\n",
    "\n",
    "    # Build combination labels per event\n",
    "    combo_counts = Counter()\n",
    "    for ev in all_event_ids:\n",
    "        present = [s for s in SENSORS if has_data[ev].get(s, False)]\n",
    "        if len(present) == 0:\n",
    "            combo_label = \"none\"\n",
    "        elif len(present) == 1:\n",
    "            # \"only s2\", etc.\n",
    "            pl = SENSOR_ALIASES.get(present[0], present[0])\n",
    "            combo_label = f\"only {pl}\"\n",
    "        else:\n",
    "            # \"s1+s2\", \"s2+ls\", etc. (use aliases, keep SENSORS order)\n",
    "            parts = [SENSOR_ALIASES.get(s, s) for s in present]\n",
    "            combo_label = \"+\".join(parts)\n",
    "        combo_counts[combo_label] += 1\n",
    "\n",
    "    return (per_sensor_index_counts,\n",
    "            per_sensor_raw_counts,\n",
    "            missing_counts,\n",
    "            missing_event_ids,\n",
    "            combo_counts)\n",
    "\n",
    "# ------------------- Pretty printing ----------------\n",
    "def print_summary(per_sensor_index_counts,\n",
    "                  per_sensor_raw_counts,\n",
    "                  missing_counts,\n",
    "                  missing_event_ids,\n",
    "                  combo_counts,\n",
    "                  show_missing_lists=False):\n",
    "    print(\"\\n=== INDEX COUNTS PER SENSOR ===\")\n",
    "    for sensor in SENSORS:\n",
    "        cnt = per_sensor_index_counts.get(sensor, Counter())\n",
    "        print(f\"\\n[{sensor}]\")\n",
    "        if cnt:\n",
    "            for k, v in cnt.most_common():\n",
    "                print(f\"  {k:12s} {v:6d}\")\n",
    "        else:\n",
    "            print(\"  (no index rasters found)\")\n",
    "\n",
    "    print(\"\\n=== RAW/PRODUCT TYPE COUNTS PER SENSOR ===\")\n",
    "    for sensor in SENSORS:\n",
    "        cnt = per_sensor_raw_counts.get(sensor, Counter())\n",
    "        print(f\"\\n[{sensor}]\")\n",
    "        if cnt:\n",
    "            for k, v in cnt.most_common():\n",
    "                print(f\"  {k:12s} {v:6d}\")\n",
    "        else:\n",
    "            print(\"  (no raw/product-type rasters found)\")\n",
    "\n",
    "    print(\"\\n=== EVENTS WITH NO DATA BY SENSOR ===\")\n",
    "    for sensor in SENSORS:\n",
    "        print(f\"  {sensor:10s}: {missing_counts[sensor]:6d} events with no rasters\")\n",
    "\n",
    "    print(\"\\n=== SENSOR PRESENCE COMBINATIONS (per event) ===\")\n",
    "    for label, n in combo_counts.most_common():\n",
    "        print(f\"  {label:15s} {n:6d}\")\n",
    "\n",
    "    if show_missing_lists:\n",
    "        print(\"\\n=== LIST OF EVENT IDS WITH NO DATA (per sensor) ===\")\n",
    "        for sensor in SENSORS:\n",
    "            ids = sorted(missing_event_ids[sensor])\n",
    "            print(f\"\\n[{sensor}] ({len(ids)}):\")\n",
    "            if ids:\n",
    "                print(\", \".join(ids))\n",
    "            else:\n",
    "                print(\"  (none)\")\n",
    "\n",
    "# ------------------- Example usage ------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Option A: Walk your on-disk tree under ./data\n",
    "    results = analyze(iter_paths(BASE_DIR))\n",
    "    print_summary(*results, show_missing_lists=False)\n",
    "\n",
    "    # Option B: Use prebuilt lists instead of walking the FS (uncomment):\n",
    "    # sample_all = s2_list + s1_list + landsat_list + cbers_list\n",
    "    # results = analyze(sample_all)\n",
    "    # print_summary(*results, show_missing_lists=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d156da-28b2-45c1-8792-f4d81f3ebcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_with = sorted([k for k in glob.glob(\"data/*/sentinel2/*.tif\") if not \"ndvi\" in k and not \"ndwi\" in k])\n",
    "len(s2_band_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f817c3-9119-4aa3-a6f8-997349d78593",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_band_paths = sorted([k for k in glob.glob(\"data/*/sentinel2/*.tif\") if not \"ndvi\" in k and not \"ndwi\" in k])\n",
    "len(s2_band_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b2368c-69c8-4706-a799-be5c1a361ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cfbb62-0ef8-4e03-bc82-15ecd40fda99",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique([k.split(\"/\")[1] for k in s2_band_paths if \"_S2-16D_V2_\" in k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c41a0d2-fd57-44dc-a81e-38dd62a05687",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([k for k in s2_band_paths if \"_S2-16D_V2_\" in k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4dcf84-c6d5-4782-8031-538dbd396519",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique([k.split(\"/\")[1] for k in s2_band_paths if \"L2A\" in k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c988984-1b43-4f6e-800b-2eb124322ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([k for k in s2_band_paths if \"L2A\" in k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7091501f-b2f2-4ac1-a9a6-87fbb916d798",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_band_paths = sorted([k for k in glob.glob(\"data/*/sentinel2/*.tif\") if \"_ndvi\" in k])\n",
    "len(s2_band_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddd0a89-c36d-4d79-98d1-8e921ff51a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_orig = rasterio.open(s2_band_paths[rand_int]).read()\n",
    "img_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175f5ad7-a1d6-42ce-94a3-1fd8dfb503f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_orig.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ae251-9bd6-42ad-b779-2db8b562a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_orig.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4e016d-4ca4-4116-9af5-f765d62fa65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "red_to_green = LinearSegmentedColormap.from_list(\"RedGreen\", [\"red\", \"green\"])\n",
    "\n",
    "for k in range(10):\n",
    "    rand_int = np.random.randint(len(s2_band_paths))\n",
    "    ndvi = rasterio.open(s2_band_paths[rand_int]).read(1)\n",
    "    print(ndvi.min(), ndvi.max())\n",
    "    # img_orig = tifffile.imread(s2_band_paths[rand_int])\n",
    "    # img = visualize_s2_concat_bands_path(s2_band_paths[rand_int])\n",
    "    ending = s2_band_paths[rand_int].split(\"/\")[-1]\n",
    "\n",
    "    f, ax = plt.subplots(1, 2, figsize=(30, 10))\n",
    "    ax[0].imshow(ndvi, vmin=-2000, vmax=6000, cmap=red_to_green)\n",
    "    ax[0].set_title(f\"{ending[:30]}\", fontsize=15)\n",
    "    # ax[0].set_title(f\"{ending[:30]}: {img_orig.shape}\", fontsize=15)\n",
    "    # pred = ((preds[i].astype(np.float32) / 255.0) > threshold) * 1\n",
    "    ax[1].imshow(ndvi, vmin=-2000, vmax=6000, cmap=red_to_green)\n",
    "    # axarr[1].imshow(np.ma.masked_where(pred == 0, pred), cmap=\"spring\", alpha=alpha)\n",
    "    # title_string = f\"FPs: {fps[i]:.0f} = {100 * fps[i] / fp_sum:.1f}%, FNs: {fns[i]:.0f} = {100 * fns[i] / fn_sum:.1f}%, TPs: {tps[i]:.0f}\"\n",
    "    # axarr[1].set_title(f\"(Pred) {title_string}\", fontsize=fontsize)\n",
    "    \n",
    "    # label = tifffile.imread(GT_paths[i])\n",
    "    # nan_mask = label == 255\n",
    "    # label = ((label >= 2) & (label <= 4)).astype(np.uint8)\n",
    "    # label[nan_mask] = 0\n",
    "    # ax[1].imshow(img)\n",
    "    # axarr[2].imshow(np.ma.masked_where(label != 1, label), cmap=\"spring\", alpha=alpha)\n",
    "    ax[1].set_title(f\"image with GT mask\", fontsize=15)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db4c18e-fec3-4215-bb96-ab9b5789bcd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
